{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTune_ConvNeXt_on_CIFAR10_Wandb.ipynb",
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyNoiIZoH3aWZ2dSOrQadDTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghvender1205/WandbAI-Examples/blob/master/Projects/FineTune_ConvNeXt_on_CIFAR10_Wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Modules\n",
        "We need some modules to be installed for this.\n",
        "1. `wandb`\n",
        "2. `timm` (pytorch_image_models)"
      ],
      "metadata": {
        "id": "EcGUQity2lzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuH4sHV92Xjw",
        "outputId": "60778135-c3c1-4155-dd38-2aa3f3a96344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |█████████████▌                  | 834.1 MB 1.4 MB/s eta 0:13:51tcmalloc: large alloc 1147494400 bytes == 0x3aace000 @  0x7ffb37532615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.2 MB/s eta 0:13:06tcmalloc: large alloc 1434370048 bytes == 0x7f124000 @  0x7ffb37532615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 1.3 MB/s eta 0:08:37tcmalloc: large alloc 1792966656 bytes == 0x3f56000 @  0x7ffb37532615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.2 MB/s eta 0:04:11tcmalloc: large alloc 2241208320 bytes == 0x6ed3e000 @  0x7ffb37532615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0xf46a0000 @  0x7ffb375311e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 2477817856 bytes == 0x16a90c000 @  0x7ffb37532615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 5.3 kB/s \n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 28 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -qq wandb timm==0.3.2 six tensorboardX2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone ConvNeXt Official Repository\n",
        "!git clone https://github.com/facebookresearch/ConvNeXt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgQ6Hf4v26t8",
        "outputId": "de25d4a0-6884-4097-b012-93b273acaaa5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ConvNeXt' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download CIFAR10 Dataset"
      ],
      "metadata": {
        "id": "IzPtyyyw3Fx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YoongiKim/CIFAR-10-images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4EqvCf_3EmF",
        "outputId": "07f697bf-4b2b-4fe4-dae2-2b23416a8d0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CIFAR-10-images' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained Weights\n",
        "Download and load pretrained weights of `ConvNeXt` on the `ImageNet1k` dataset."
      ],
      "metadata": {
        "id": "eE2rX95a3Myl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ConvNeXt/\n",
        "!wget https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrxKugBq3KWK",
        "outputId": "a51a6c3b-21de-4b24-f8f8-85bf8bebf06e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ConvNeXt\n",
            "--2022-06-08 10:39:35--  https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 114414741 (109M) [binary/octet-stream]\n",
            "Saving to: ‘convnext_tiny_1k_224_ema.pth.1’\n",
            "\n",
            "convnext_tiny_1k_22 100%[===================>] 109.11M  36.4MB/s    in 3.0s    \n",
            "\n",
            "2022-06-08 10:39:39 (36.4 MB/s) - ‘convnext_tiny_1k_224_ema.pth.1’ saved [114414741/114414741]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with WandbAI"
      ],
      "metadata": {
        "id": "mWmF80N44RMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --epochs 10 \\\n",
        "                --model convnext_tiny \\\n",
        "                --data_set image_folder \\\n",
        "                --data_path ../CIFAR-10-images/train \\\n",
        "                --eval_data_path ../CIFAR-10-images/test \\\n",
        "                --nb_classes 10 \\\n",
        "                --num_workers 8 \\\n",
        "                --warmup_epochs 0 \\\n",
        "                --save_ckpt true \\\n",
        "                --output_dir model_ckpt \\\n",
        "                --finetune convnext_tiny_1k_224_ema.pth \\\n",
        "                --cutmix 0 \\\n",
        "                --mixup 0 --lr 4e-4 \\\n",
        "                --enable_wandb true --wandb_ckpt true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOPbWiq_3aAy",
        "outputId": "bb1f4c3b-f0dc-4127-a27e-e75b04f9a9be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=0.0, cutmix_minmax=None, data_path='../CIFAR-10-images/train', data_set='image_folder', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0, enable_wandb=True, epochs=10, eval=False, eval_data_path='../CIFAR-10-images/test', finetune='convnext_tiny_1k_224_ema.pth', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.0004, min_lr=1e-06, mixup=0.0, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='convnext_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=10, num_workers=8, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='model_ckpt', pin_mem=True, project='convnext', recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', save_ckpt=True, save_ckpt_freq=1, save_ckpt_num=3, seed=0, smoothing=0.1, start_epoch=0, train_interpolation='bicubic', update_freq=1, use_amp=False, wandb_ckpt=True, warmup_epochs=0, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=1)\n",
            "Transform = \n",
            "RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)\n",
            "RandomHorizontalFlip(p=0.5)\n",
            "<timm.data.auto_augment.RandAugment object at 0x7ff55d2e0490>\n",
            "ToTensor()\n",
            "Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
            "<timm.data.random_erasing.RandomErasing object at 0x7ff55d2e0750>\n",
            "---------------------------\n",
            "Number of the class = 10\n",
            "Transform = \n",
            "Resize(size=256, interpolation=bicubic)\n",
            "CenterCrop(size=(224, 224))\n",
            "ToTensor()\n",
            "Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "---------------------------\n",
            "Number of the class = 10\n",
            "Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7ff55d2e0150>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/ConvNeXt/wandb/run-20220608_104657-xkrxh4el\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtreasured-plant-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/raghvender/convnext\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/raghvender/convnext/runs/xkrxh4el\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Load ckpt from convnext_tiny_1k_224_ema.pth\n",
            "Load state_dict by model_key = model\n",
            "Removing key head.weight from pretrained checkpoint\n",
            "Removing key head.bias from pretrained checkpoint\n",
            "Weights of ConvNeXt not initialized from pretrained model: ['head.weight', 'head.bias']\n",
            "Model = ConvNeXt(\n",
            "  (downsample_layers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
            "      (1): LayerNorm()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): LayerNorm()\n",
            "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): LayerNorm()\n",
            "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): LayerNorm()\n",
            "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "  )\n",
            "  (stages): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Block(\n",
            "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Block(\n",
            "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (3): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (4): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (5): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (6): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (7): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (8): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Block(\n",
            "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
            ")\n",
            "number of params: 27827818\n",
            "LR = 0.00040000\n",
            "Batch size = 64\n",
            "Update frequent = 1\n",
            "Number of training examples = 50000\n",
            "Number of training training per epoch = 781\n",
            "Param groups = {\n",
            "  \"decay\": {\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"params\": [\n",
            "      \"downsample_layers.0.0.weight\",\n",
            "      \"downsample_layers.1.1.weight\",\n",
            "      \"downsample_layers.2.1.weight\",\n",
            "      \"downsample_layers.3.1.weight\",\n",
            "      \"stages.0.0.dwconv.weight\",\n",
            "      \"stages.0.0.pwconv1.weight\",\n",
            "      \"stages.0.0.pwconv2.weight\",\n",
            "      \"stages.0.1.dwconv.weight\",\n",
            "      \"stages.0.1.pwconv1.weight\",\n",
            "      \"stages.0.1.pwconv2.weight\",\n",
            "      \"stages.0.2.dwconv.weight\",\n",
            "      \"stages.0.2.pwconv1.weight\",\n",
            "      \"stages.0.2.pwconv2.weight\",\n",
            "      \"stages.1.0.dwconv.weight\",\n",
            "      \"stages.1.0.pwconv1.weight\",\n",
            "      \"stages.1.0.pwconv2.weight\",\n",
            "      \"stages.1.1.dwconv.weight\",\n",
            "      \"stages.1.1.pwconv1.weight\",\n",
            "      \"stages.1.1.pwconv2.weight\",\n",
            "      \"stages.1.2.dwconv.weight\",\n",
            "      \"stages.1.2.pwconv1.weight\",\n",
            "      \"stages.1.2.pwconv2.weight\",\n",
            "      \"stages.2.0.dwconv.weight\",\n",
            "      \"stages.2.0.pwconv1.weight\",\n",
            "      \"stages.2.0.pwconv2.weight\",\n",
            "      \"stages.2.1.dwconv.weight\",\n",
            "      \"stages.2.1.pwconv1.weight\",\n",
            "      \"stages.2.1.pwconv2.weight\",\n",
            "      \"stages.2.2.dwconv.weight\",\n",
            "      \"stages.2.2.pwconv1.weight\",\n",
            "      \"stages.2.2.pwconv2.weight\",\n",
            "      \"stages.2.3.dwconv.weight\",\n",
            "      \"stages.2.3.pwconv1.weight\",\n",
            "      \"stages.2.3.pwconv2.weight\",\n",
            "      \"stages.2.4.dwconv.weight\",\n",
            "      \"stages.2.4.pwconv1.weight\",\n",
            "      \"stages.2.4.pwconv2.weight\",\n",
            "      \"stages.2.5.dwconv.weight\",\n",
            "      \"stages.2.5.pwconv1.weight\",\n",
            "      \"stages.2.5.pwconv2.weight\",\n",
            "      \"stages.2.6.dwconv.weight\",\n",
            "      \"stages.2.6.pwconv1.weight\",\n",
            "      \"stages.2.6.pwconv2.weight\",\n",
            "      \"stages.2.7.dwconv.weight\",\n",
            "      \"stages.2.7.pwconv1.weight\",\n",
            "      \"stages.2.7.pwconv2.weight\",\n",
            "      \"stages.2.8.dwconv.weight\",\n",
            "      \"stages.2.8.pwconv1.weight\",\n",
            "      \"stages.2.8.pwconv2.weight\",\n",
            "      \"stages.3.0.dwconv.weight\",\n",
            "      \"stages.3.0.pwconv1.weight\",\n",
            "      \"stages.3.0.pwconv2.weight\",\n",
            "      \"stages.3.1.dwconv.weight\",\n",
            "      \"stages.3.1.pwconv1.weight\",\n",
            "      \"stages.3.1.pwconv2.weight\",\n",
            "      \"stages.3.2.dwconv.weight\",\n",
            "      \"stages.3.2.pwconv1.weight\",\n",
            "      \"stages.3.2.pwconv2.weight\",\n",
            "      \"head.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  },\n",
            "  \"no_decay\": {\n",
            "    \"weight_decay\": 0.0,\n",
            "    \"params\": [\n",
            "      \"downsample_layers.0.0.bias\",\n",
            "      \"downsample_layers.0.1.weight\",\n",
            "      \"downsample_layers.0.1.bias\",\n",
            "      \"downsample_layers.1.0.weight\",\n",
            "      \"downsample_layers.1.0.bias\",\n",
            "      \"downsample_layers.1.1.bias\",\n",
            "      \"downsample_layers.2.0.weight\",\n",
            "      \"downsample_layers.2.0.bias\",\n",
            "      \"downsample_layers.2.1.bias\",\n",
            "      \"downsample_layers.3.0.weight\",\n",
            "      \"downsample_layers.3.0.bias\",\n",
            "      \"downsample_layers.3.1.bias\",\n",
            "      \"stages.0.0.gamma\",\n",
            "      \"stages.0.0.dwconv.bias\",\n",
            "      \"stages.0.0.norm.weight\",\n",
            "      \"stages.0.0.norm.bias\",\n",
            "      \"stages.0.0.pwconv1.bias\",\n",
            "      \"stages.0.0.pwconv2.bias\",\n",
            "      \"stages.0.1.gamma\",\n",
            "      \"stages.0.1.dwconv.bias\",\n",
            "      \"stages.0.1.norm.weight\",\n",
            "      \"stages.0.1.norm.bias\",\n",
            "      \"stages.0.1.pwconv1.bias\",\n",
            "      \"stages.0.1.pwconv2.bias\",\n",
            "      \"stages.0.2.gamma\",\n",
            "      \"stages.0.2.dwconv.bias\",\n",
            "      \"stages.0.2.norm.weight\",\n",
            "      \"stages.0.2.norm.bias\",\n",
            "      \"stages.0.2.pwconv1.bias\",\n",
            "      \"stages.0.2.pwconv2.bias\",\n",
            "      \"stages.1.0.gamma\",\n",
            "      \"stages.1.0.dwconv.bias\",\n",
            "      \"stages.1.0.norm.weight\",\n",
            "      \"stages.1.0.norm.bias\",\n",
            "      \"stages.1.0.pwconv1.bias\",\n",
            "      \"stages.1.0.pwconv2.bias\",\n",
            "      \"stages.1.1.gamma\",\n",
            "      \"stages.1.1.dwconv.bias\",\n",
            "      \"stages.1.1.norm.weight\",\n",
            "      \"stages.1.1.norm.bias\",\n",
            "      \"stages.1.1.pwconv1.bias\",\n",
            "      \"stages.1.1.pwconv2.bias\",\n",
            "      \"stages.1.2.gamma\",\n",
            "      \"stages.1.2.dwconv.bias\",\n",
            "      \"stages.1.2.norm.weight\",\n",
            "      \"stages.1.2.norm.bias\",\n",
            "      \"stages.1.2.pwconv1.bias\",\n",
            "      \"stages.1.2.pwconv2.bias\",\n",
            "      \"stages.2.0.gamma\",\n",
            "      \"stages.2.0.dwconv.bias\",\n",
            "      \"stages.2.0.norm.weight\",\n",
            "      \"stages.2.0.norm.bias\",\n",
            "      \"stages.2.0.pwconv1.bias\",\n",
            "      \"stages.2.0.pwconv2.bias\",\n",
            "      \"stages.2.1.gamma\",\n",
            "      \"stages.2.1.dwconv.bias\",\n",
            "      \"stages.2.1.norm.weight\",\n",
            "      \"stages.2.1.norm.bias\",\n",
            "      \"stages.2.1.pwconv1.bias\",\n",
            "      \"stages.2.1.pwconv2.bias\",\n",
            "      \"stages.2.2.gamma\",\n",
            "      \"stages.2.2.dwconv.bias\",\n",
            "      \"stages.2.2.norm.weight\",\n",
            "      \"stages.2.2.norm.bias\",\n",
            "      \"stages.2.2.pwconv1.bias\",\n",
            "      \"stages.2.2.pwconv2.bias\",\n",
            "      \"stages.2.3.gamma\",\n",
            "      \"stages.2.3.dwconv.bias\",\n",
            "      \"stages.2.3.norm.weight\",\n",
            "      \"stages.2.3.norm.bias\",\n",
            "      \"stages.2.3.pwconv1.bias\",\n",
            "      \"stages.2.3.pwconv2.bias\",\n",
            "      \"stages.2.4.gamma\",\n",
            "      \"stages.2.4.dwconv.bias\",\n",
            "      \"stages.2.4.norm.weight\",\n",
            "      \"stages.2.4.norm.bias\",\n",
            "      \"stages.2.4.pwconv1.bias\",\n",
            "      \"stages.2.4.pwconv2.bias\",\n",
            "      \"stages.2.5.gamma\",\n",
            "      \"stages.2.5.dwconv.bias\",\n",
            "      \"stages.2.5.norm.weight\",\n",
            "      \"stages.2.5.norm.bias\",\n",
            "      \"stages.2.5.pwconv1.bias\",\n",
            "      \"stages.2.5.pwconv2.bias\",\n",
            "      \"stages.2.6.gamma\",\n",
            "      \"stages.2.6.dwconv.bias\",\n",
            "      \"stages.2.6.norm.weight\",\n",
            "      \"stages.2.6.norm.bias\",\n",
            "      \"stages.2.6.pwconv1.bias\",\n",
            "      \"stages.2.6.pwconv2.bias\",\n",
            "      \"stages.2.7.gamma\",\n",
            "      \"stages.2.7.dwconv.bias\",\n",
            "      \"stages.2.7.norm.weight\",\n",
            "      \"stages.2.7.norm.bias\",\n",
            "      \"stages.2.7.pwconv1.bias\",\n",
            "      \"stages.2.7.pwconv2.bias\",\n",
            "      \"stages.2.8.gamma\",\n",
            "      \"stages.2.8.dwconv.bias\",\n",
            "      \"stages.2.8.norm.weight\",\n",
            "      \"stages.2.8.norm.bias\",\n",
            "      \"stages.2.8.pwconv1.bias\",\n",
            "      \"stages.2.8.pwconv2.bias\",\n",
            "      \"stages.3.0.gamma\",\n",
            "      \"stages.3.0.dwconv.bias\",\n",
            "      \"stages.3.0.norm.weight\",\n",
            "      \"stages.3.0.norm.bias\",\n",
            "      \"stages.3.0.pwconv1.bias\",\n",
            "      \"stages.3.0.pwconv2.bias\",\n",
            "      \"stages.3.1.gamma\",\n",
            "      \"stages.3.1.dwconv.bias\",\n",
            "      \"stages.3.1.norm.weight\",\n",
            "      \"stages.3.1.norm.bias\",\n",
            "      \"stages.3.1.pwconv1.bias\",\n",
            "      \"stages.3.1.pwconv2.bias\",\n",
            "      \"stages.3.2.gamma\",\n",
            "      \"stages.3.2.dwconv.bias\",\n",
            "      \"stages.3.2.norm.weight\",\n",
            "      \"stages.3.2.norm.bias\",\n",
            "      \"stages.3.2.pwconv1.bias\",\n",
            "      \"stages.3.2.pwconv2.bias\",\n",
            "      \"norm.weight\",\n",
            "      \"norm.bias\",\n",
            "      \"head.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  }\n",
            "}\n",
            "Use Cosine LR scheduler\n",
            "Set warmup steps = 0\n",
            "Set warmup steps = 0\n",
            "Max WD = 0.0500000, Min WD = 0.0500000\n",
            "criterion = LabelSmoothingCrossEntropy()\n",
            "Auto resume checkpoint: \n",
            "Start training for 10 epochs\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [0]  [  0/781]  eta: 1:23:15  lr: 0.000400  min_lr: 0.000400  loss: 2.2958 (2.2958)  class_acc: 0.1562 (0.1562)  weight_decay: 0.0500 (0.0500)  time: 6.3964  data: 2.3601  max mem: 8477\n",
            "Epoch: [0]  [ 10/781]  eta: 0:17:18  lr: 0.000400  min_lr: 0.000400  loss: 2.1855 (2.1665)  class_acc: 0.2344 (0.2443)  weight_decay: 0.0500 (0.0500)  time: 1.3471  data: 0.2151  max mem: 8477\n",
            "Epoch: [0]  [ 20/781]  eta: 0:14:04  lr: 0.000400  min_lr: 0.000400  loss: 1.9205 (2.0094)  class_acc: 0.3438 (0.3378)  weight_decay: 0.0500 (0.0500)  time: 0.8459  data: 0.0005  max mem: 8477\n",
            "Epoch: [0]  [ 30/781]  eta: 0:12:52  lr: 0.000400  min_lr: 0.000400  loss: 1.7574 (1.8965)  class_acc: 0.4531 (0.3881)  weight_decay: 0.0500 (0.0500)  time: 0.8540  data: 0.0005  max mem: 8477\n",
            "Epoch: [0]  [ 40/781]  eta: 0:12:13  lr: 0.000400  min_lr: 0.000400  loss: 1.5949 (1.8164)  class_acc: 0.5000 (0.4211)  weight_decay: 0.0500 (0.0500)  time: 0.8626  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [ 50/781]  eta: 0:11:46  lr: 0.000400  min_lr: 0.000400  loss: 1.5727 (1.7577)  class_acc: 0.5000 (0.4449)  weight_decay: 0.0500 (0.0500)  time: 0.8712  data: 0.0005  max mem: 8477\n",
            "Epoch: [0]  [ 60/781]  eta: 0:11:27  lr: 0.000400  min_lr: 0.000400  loss: 1.4765 (1.7076)  class_acc: 0.5625 (0.4654)  weight_decay: 0.0500 (0.0500)  time: 0.8806  data: 0.0004  max mem: 8477\n",
            "Epoch: [0]  [ 70/781]  eta: 0:11:12  lr: 0.000400  min_lr: 0.000400  loss: 1.4264 (1.6736)  class_acc: 0.5625 (0.4800)  weight_decay: 0.0500 (0.0500)  time: 0.8931  data: 0.0005  max mem: 8477\n",
            "Epoch: [0]  [ 80/781]  eta: 0:11:00  lr: 0.000400  min_lr: 0.000400  loss: 1.4176 (1.6376)  class_acc: 0.5938 (0.4950)  weight_decay: 0.0500 (0.0500)  time: 0.9067  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [ 90/781]  eta: 0:10:49  lr: 0.000400  min_lr: 0.000400  loss: 1.3510 (1.6025)  class_acc: 0.6250 (0.5113)  weight_decay: 0.0500 (0.0500)  time: 0.9165  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [100/781]  eta: 0:10:39  lr: 0.000400  min_lr: 0.000400  loss: 1.3126 (1.5761)  class_acc: 0.6250 (0.5234)  weight_decay: 0.0500 (0.0500)  time: 0.9268  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [110/781]  eta: 0:10:30  lr: 0.000400  min_lr: 0.000400  loss: 1.3359 (1.5596)  class_acc: 0.6250 (0.5301)  weight_decay: 0.0500 (0.0500)  time: 0.9406  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [120/781]  eta: 0:10:22  lr: 0.000400  min_lr: 0.000400  loss: 1.3420 (1.5405)  class_acc: 0.6250 (0.5395)  weight_decay: 0.0500 (0.0500)  time: 0.9508  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [130/781]  eta: 0:10:13  lr: 0.000400  min_lr: 0.000400  loss: 1.3332 (1.5259)  class_acc: 0.6406 (0.5471)  weight_decay: 0.0500 (0.0500)  time: 0.9528  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [140/781]  eta: 0:10:03  lr: 0.000400  min_lr: 0.000400  loss: 1.3629 (1.5140)  class_acc: 0.6250 (0.5523)  weight_decay: 0.0500 (0.0500)  time: 0.9454  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [150/781]  eta: 0:09:53  lr: 0.000400  min_lr: 0.000400  loss: 1.3219 (1.5008)  class_acc: 0.6250 (0.5571)  weight_decay: 0.0500 (0.0500)  time: 0.9373  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [160/781]  eta: 0:09:44  lr: 0.000400  min_lr: 0.000400  loss: 1.3270 (1.4903)  class_acc: 0.6250 (0.5623)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [170/781]  eta: 0:09:34  lr: 0.000400  min_lr: 0.000400  loss: 1.3270 (1.4802)  class_acc: 0.6406 (0.5671)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0004  max mem: 8477\n",
            "Epoch: [0]  [180/781]  eta: 0:09:24  lr: 0.000399  min_lr: 0.000399  loss: 1.3276 (1.4727)  class_acc: 0.6250 (0.5695)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0005  max mem: 8477\n",
            "Epoch: [0]  [190/781]  eta: 0:09:15  lr: 0.000399  min_lr: 0.000399  loss: 1.3620 (1.4681)  class_acc: 0.5938 (0.5708)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [200/781]  eta: 0:09:05  lr: 0.000399  min_lr: 0.000399  loss: 1.3352 (1.4599)  class_acc: 0.6094 (0.5741)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [210/781]  eta: 0:08:56  lr: 0.000399  min_lr: 0.000399  loss: 1.2645 (1.4504)  class_acc: 0.6406 (0.5784)  weight_decay: 0.0500 (0.0500)  time: 0.9369  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [220/781]  eta: 0:08:46  lr: 0.000399  min_lr: 0.000399  loss: 1.2991 (1.4453)  class_acc: 0.6406 (0.5810)  weight_decay: 0.0500 (0.0500)  time: 0.9365  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [230/781]  eta: 0:08:37  lr: 0.000399  min_lr: 0.000399  loss: 1.3340 (1.4426)  class_acc: 0.6250 (0.5825)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [240/781]  eta: 0:08:27  lr: 0.000399  min_lr: 0.000399  loss: 1.3116 (1.4375)  class_acc: 0.6406 (0.5849)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [250/781]  eta: 0:08:18  lr: 0.000399  min_lr: 0.000399  loss: 1.2614 (1.4310)  class_acc: 0.6562 (0.5876)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [260/781]  eta: 0:08:08  lr: 0.000399  min_lr: 0.000399  loss: 1.2765 (1.4252)  class_acc: 0.6562 (0.5899)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [270/781]  eta: 0:07:59  lr: 0.000399  min_lr: 0.000399  loss: 1.2886 (1.4217)  class_acc: 0.6094 (0.5910)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [280/781]  eta: 0:07:49  lr: 0.000399  min_lr: 0.000399  loss: 1.3266 (1.4198)  class_acc: 0.6094 (0.5916)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [290/781]  eta: 0:07:40  lr: 0.000399  min_lr: 0.000399  loss: 1.2978 (1.4153)  class_acc: 0.6250 (0.5932)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [300/781]  eta: 0:07:31  lr: 0.000399  min_lr: 0.000399  loss: 1.2174 (1.4087)  class_acc: 0.6562 (0.5961)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [310/781]  eta: 0:07:21  lr: 0.000398  min_lr: 0.000398  loss: 1.2174 (1.4045)  class_acc: 0.6719 (0.5983)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0005  max mem: 8477\n",
            "Epoch: [0]  [320/781]  eta: 0:07:12  lr: 0.000398  min_lr: 0.000398  loss: 1.2104 (1.3975)  class_acc: 0.6719 (0.6014)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [330/781]  eta: 0:07:02  lr: 0.000398  min_lr: 0.000398  loss: 1.1963 (1.3930)  class_acc: 0.6719 (0.6036)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [340/781]  eta: 0:06:53  lr: 0.000398  min_lr: 0.000398  loss: 1.2972 (1.3901)  class_acc: 0.6406 (0.6053)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [350/781]  eta: 0:06:43  lr: 0.000398  min_lr: 0.000398  loss: 1.2427 (1.3853)  class_acc: 0.6719 (0.6073)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [360/781]  eta: 0:06:34  lr: 0.000398  min_lr: 0.000398  loss: 1.2494 (1.3831)  class_acc: 0.6719 (0.6083)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [370/781]  eta: 0:06:25  lr: 0.000398  min_lr: 0.000398  loss: 1.2723 (1.3800)  class_acc: 0.6562 (0.6098)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [380/781]  eta: 0:06:15  lr: 0.000398  min_lr: 0.000398  loss: 1.2198 (1.3759)  class_acc: 0.6875 (0.6120)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [390/781]  eta: 0:06:06  lr: 0.000398  min_lr: 0.000398  loss: 1.1995 (1.3711)  class_acc: 0.7031 (0.6139)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [400/781]  eta: 0:05:56  lr: 0.000397  min_lr: 0.000397  loss: 1.1995 (1.3680)  class_acc: 0.6875 (0.6151)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [410/781]  eta: 0:05:47  lr: 0.000397  min_lr: 0.000397  loss: 1.2557 (1.3649)  class_acc: 0.6562 (0.6168)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [0]  [420/781]  eta: 0:05:38  lr: 0.000397  min_lr: 0.000397  loss: 1.1951 (1.3607)  class_acc: 0.7031 (0.6188)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [430/781]  eta: 0:05:28  lr: 0.000397  min_lr: 0.000397  loss: 1.1951 (1.3574)  class_acc: 0.6875 (0.6202)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0012  max mem: 8477\n",
            "Epoch: [0]  [440/781]  eta: 0:05:19  lr: 0.000397  min_lr: 0.000397  loss: 1.1991 (1.3550)  class_acc: 0.6719 (0.6210)  weight_decay: 0.0500 (0.0500)  time: 0.9362  data: 0.0013  max mem: 8477\n",
            "Epoch: [0]  [450/781]  eta: 0:05:09  lr: 0.000397  min_lr: 0.000397  loss: 1.2708 (1.3530)  class_acc: 0.6719 (0.6218)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0010  max mem: 8477\n",
            "Epoch: [0]  [460/781]  eta: 0:05:00  lr: 0.000397  min_lr: 0.000397  loss: 1.2344 (1.3495)  class_acc: 0.6719 (0.6235)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [470/781]  eta: 0:04:51  lr: 0.000396  min_lr: 0.000396  loss: 1.2180 (1.3468)  class_acc: 0.6875 (0.6247)  weight_decay: 0.0500 (0.0500)  time: 0.9327  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [480/781]  eta: 0:04:41  lr: 0.000396  min_lr: 0.000396  loss: 1.2226 (1.3447)  class_acc: 0.6719 (0.6255)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0007  max mem: 8477\n",
            "Epoch: [0]  [490/781]  eta: 0:04:32  lr: 0.000396  min_lr: 0.000396  loss: 1.2082 (1.3425)  class_acc: 0.6562 (0.6262)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [500/781]  eta: 0:04:23  lr: 0.000396  min_lr: 0.000396  loss: 1.2041 (1.3397)  class_acc: 0.6562 (0.6271)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [510/781]  eta: 0:04:13  lr: 0.000396  min_lr: 0.000396  loss: 1.2432 (1.3382)  class_acc: 0.6562 (0.6276)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0004  max mem: 8477\n",
            "Epoch: [0]  [520/781]  eta: 0:04:04  lr: 0.000396  min_lr: 0.000396  loss: 1.2129 (1.3344)  class_acc: 0.6875 (0.6294)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [530/781]  eta: 0:03:54  lr: 0.000395  min_lr: 0.000395  loss: 1.1688 (1.3323)  class_acc: 0.7031 (0.6301)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [540/781]  eta: 0:03:45  lr: 0.000395  min_lr: 0.000395  loss: 1.2189 (1.3298)  class_acc: 0.7031 (0.6316)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [550/781]  eta: 0:03:36  lr: 0.000395  min_lr: 0.000395  loss: 1.1861 (1.3274)  class_acc: 0.7031 (0.6328)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [560/781]  eta: 0:03:26  lr: 0.000395  min_lr: 0.000395  loss: 1.1861 (1.3251)  class_acc: 0.6875 (0.6339)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [570/781]  eta: 0:03:17  lr: 0.000395  min_lr: 0.000395  loss: 1.1918 (1.3233)  class_acc: 0.6719 (0.6347)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0006  max mem: 8477\n",
            "Epoch: [0]  [580/781]  eta: 0:03:08  lr: 0.000395  min_lr: 0.000395  loss: 1.1984 (1.3211)  class_acc: 0.6719 (0.6354)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0010  max mem: 8477\n",
            "Epoch: [0]  [590/781]  eta: 0:02:58  lr: 0.000394  min_lr: 0.000394  loss: 1.1899 (1.3190)  class_acc: 0.6875 (0.6362)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0012  max mem: 8477\n",
            "Epoch: [0]  [600/781]  eta: 0:02:49  lr: 0.000394  min_lr: 0.000394  loss: 1.1899 (1.3171)  class_acc: 0.6875 (0.6370)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [610/781]  eta: 0:02:40  lr: 0.000394  min_lr: 0.000394  loss: 1.2083 (1.3154)  class_acc: 0.6875 (0.6377)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [620/781]  eta: 0:02:30  lr: 0.000394  min_lr: 0.000394  loss: 1.2202 (1.3135)  class_acc: 0.6562 (0.6384)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [630/781]  eta: 0:02:21  lr: 0.000394  min_lr: 0.000394  loss: 1.2010 (1.3132)  class_acc: 0.6406 (0.6383)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [640/781]  eta: 0:02:11  lr: 0.000393  min_lr: 0.000393  loss: 1.2411 (1.3116)  class_acc: 0.6719 (0.6392)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0010  max mem: 8477\n",
            "Epoch: [0]  [650/781]  eta: 0:02:02  lr: 0.000393  min_lr: 0.000393  loss: 1.1709 (1.3089)  class_acc: 0.7031 (0.6404)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0010  max mem: 8477\n",
            "Epoch: [0]  [660/781]  eta: 0:01:53  lr: 0.000393  min_lr: 0.000393  loss: 1.1494 (1.3069)  class_acc: 0.6875 (0.6410)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0008  max mem: 8477\n",
            "Epoch: [0]  [670/781]  eta: 0:01:43  lr: 0.000393  min_lr: 0.000393  loss: 1.2016 (1.3057)  class_acc: 0.6719 (0.6418)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0011  max mem: 8477\n",
            "Epoch: [0]  [680/781]  eta: 0:01:34  lr: 0.000393  min_lr: 0.000393  loss: 1.2093 (1.3049)  class_acc: 0.6719 (0.6423)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0012  max mem: 8477\n",
            "Epoch: [0]  [690/781]  eta: 0:01:25  lr: 0.000392  min_lr: 0.000392  loss: 1.2057 (1.3031)  class_acc: 0.6719 (0.6428)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0012  max mem: 8477\n",
            "Epoch: [0]  [700/781]  eta: 0:01:15  lr: 0.000392  min_lr: 0.000392  loss: 1.1767 (1.3007)  class_acc: 0.6875 (0.6437)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0011  max mem: 8477\n",
            "Epoch: [0]  [710/781]  eta: 0:01:06  lr: 0.000392  min_lr: 0.000392  loss: 1.1665 (1.2990)  class_acc: 0.6875 (0.6444)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0009  max mem: 8477\n",
            "Epoch: [0]  [720/781]  eta: 0:00:57  lr: 0.000392  min_lr: 0.000392  loss: 1.1467 (1.2967)  class_acc: 0.7188 (0.6455)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0012  max mem: 8477\n",
            "Epoch: [0]  [730/781]  eta: 0:00:47  lr: 0.000391  min_lr: 0.000391  loss: 1.1512 (1.2948)  class_acc: 0.7188 (0.6462)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0013  max mem: 8477\n",
            "Epoch: [0]  [740/781]  eta: 0:00:38  lr: 0.000391  min_lr: 0.000391  loss: 1.1512 (1.2933)  class_acc: 0.7031 (0.6469)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0013  max mem: 8477\n",
            "Epoch: [0]  [750/781]  eta: 0:00:28  lr: 0.000391  min_lr: 0.000391  loss: 1.1220 (1.2916)  class_acc: 0.7031 (0.6474)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0013  max mem: 8477\n",
            "Epoch: [0]  [760/781]  eta: 0:00:19  lr: 0.000391  min_lr: 0.000391  loss: 1.1221 (1.2898)  class_acc: 0.7031 (0.6481)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0010  max mem: 8477\n",
            "Epoch: [0]  [770/781]  eta: 0:00:10  lr: 0.000391  min_lr: 0.000391  loss: 1.1221 (1.2883)  class_acc: 0.7031 (0.6488)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0005  max mem: 8477\n",
            "Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000390  min_lr: 0.000390  loss: 1.1431 (1.2866)  class_acc: 0.7031 (0.6496)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0006  max mem: 8477\n",
            "Epoch: [0] Total time: 0:12:10 (0.9359 s / it)\n",
            "Averaged stats: lr: 0.000390  min_lr: 0.000390  loss: 1.1431 (1.2866)  class_acc: 0.7031 (0.6496)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:08:25  loss: 0.5671 (0.5671)  acc1: 81.2500 (81.2500)  acc5: 97.9167 (97.9167)  time: 4.8160  data: 3.8060  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:20  loss: 0.3672 (0.3667)  acc1: 89.5833 (89.5833)  acc5: 98.9583 (99.1477)  time: 0.8502  data: 0.3470  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:56  loss: 0.2714 (0.2908)  acc1: 92.7083 (92.3611)  acc5: 100.0000 (99.5536)  time: 0.4551  data: 0.0008  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:44  loss: 0.3292 (0.3355)  acc1: 90.6250 (90.6586)  acc5: 100.0000 (99.6976)  time: 0.4578  data: 0.0007  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:36  loss: 0.4873 (0.3794)  acc1: 85.4167 (89.2276)  acc5: 100.0000 (99.7459)  time: 0.4615  data: 0.0013  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:29  loss: 0.4980 (0.3998)  acc1: 85.4167 (88.5825)  acc5: 100.0000 (99.7549)  time: 0.4651  data: 0.0015  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:23  loss: 0.4824 (0.4127)  acc1: 85.4167 (87.9098)  acc5: 100.0000 (99.7780)  time: 0.4656  data: 0.0006  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:18  loss: 0.3669 (0.3910)  acc1: 90.6250 (88.7471)  acc5: 100.0000 (99.7506)  time: 0.4664  data: 0.0007  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.2934 (0.3853)  acc1: 92.7083 (89.1204)  acc5: 100.0000 (99.7042)  time: 0.4670  data: 0.0014  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.2704 (0.3707)  acc1: 92.7083 (89.7665)  acc5: 100.0000 (99.7138)  time: 0.4647  data: 0.0011  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.2283 (0.3629)  acc1: 93.7500 (90.0990)  acc5: 100.0000 (99.6906)  time: 0.4623  data: 0.0003  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.2535 (0.3602)  acc1: 93.7500 (90.1600)  acc5: 100.0000 (99.6900)  time: 0.4450  data: 0.0003  max mem: 8477\n",
            "Test: Total time: 0:00:52 (0.5042 s / it)\n",
            "* Acc@1 90.160 Acc@5 99.690 loss 0.360\n",
            "Accuracy of the model on the 10000 test images: 90.2%\n",
            "Max accuracy: 90.16%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [1]  [  0/781]  eta: 0:53:30  lr: 0.000390  min_lr: 0.000390  loss: 1.2693 (1.2693)  class_acc: 0.6562 (0.6562)  weight_decay: 0.0500 (0.0500)  time: 4.1110  data: 3.1008  max mem: 8477\n",
            "Epoch: [1]  [ 10/781]  eta: 0:15:43  lr: 0.000390  min_lr: 0.000390  loss: 1.2184 (1.2005)  class_acc: 0.6875 (0.6861)  weight_decay: 0.0500 (0.0500)  time: 1.2236  data: 0.2836  max mem: 8477\n",
            "Epoch: [1]  [ 20/781]  eta: 0:13:46  lr: 0.000390  min_lr: 0.000390  loss: 1.1416 (1.1669)  class_acc: 0.7188 (0.6994)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [ 30/781]  eta: 0:12:58  lr: 0.000389  min_lr: 0.000389  loss: 1.1416 (1.1615)  class_acc: 0.7188 (0.6996)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0008  max mem: 8477\n",
            "Epoch: [1]  [ 40/781]  eta: 0:12:31  lr: 0.000389  min_lr: 0.000389  loss: 1.1839 (1.1752)  class_acc: 0.7031 (0.6917)  weight_decay: 0.0500 (0.0500)  time: 0.9384  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [ 50/781]  eta: 0:12:10  lr: 0.000389  min_lr: 0.000389  loss: 1.1693 (1.1716)  class_acc: 0.7031 (0.6927)  weight_decay: 0.0500 (0.0500)  time: 0.9414  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [ 60/781]  eta: 0:11:53  lr: 0.000389  min_lr: 0.000389  loss: 1.1262 (1.1708)  class_acc: 0.7031 (0.6942)  weight_decay: 0.0500 (0.0500)  time: 0.9403  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [ 70/781]  eta: 0:11:38  lr: 0.000388  min_lr: 0.000388  loss: 1.1903 (1.1747)  class_acc: 0.6875 (0.6930)  weight_decay: 0.0500 (0.0500)  time: 0.9371  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [ 80/781]  eta: 0:11:24  lr: 0.000388  min_lr: 0.000388  loss: 1.1769 (1.1705)  class_acc: 0.7031 (0.6950)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [ 90/781]  eta: 0:11:11  lr: 0.000388  min_lr: 0.000388  loss: 1.1147 (1.1673)  class_acc: 0.7188 (0.6957)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [100/781]  eta: 0:10:59  lr: 0.000388  min_lr: 0.000388  loss: 1.1148 (1.1658)  class_acc: 0.7188 (0.6979)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [110/781]  eta: 0:10:47  lr: 0.000387  min_lr: 0.000387  loss: 1.1299 (1.1668)  class_acc: 0.7031 (0.6961)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0009  max mem: 8477\n",
            "Epoch: [1]  [120/781]  eta: 0:10:35  lr: 0.000387  min_lr: 0.000387  loss: 1.1572 (1.1643)  class_acc: 0.6875 (0.6980)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0008  max mem: 8477\n",
            "Epoch: [1]  [130/781]  eta: 0:10:24  lr: 0.000387  min_lr: 0.000387  loss: 1.1720 (1.1698)  class_acc: 0.6719 (0.6964)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [140/781]  eta: 0:10:14  lr: 0.000386  min_lr: 0.000386  loss: 1.1709 (1.1699)  class_acc: 0.6875 (0.6964)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [150/781]  eta: 0:10:03  lr: 0.000386  min_lr: 0.000386  loss: 1.1462 (1.1677)  class_acc: 0.7031 (0.6971)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [160/781]  eta: 0:09:53  lr: 0.000386  min_lr: 0.000386  loss: 1.1031 (1.1676)  class_acc: 0.7031 (0.6981)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [170/781]  eta: 0:09:42  lr: 0.000386  min_lr: 0.000386  loss: 1.1933 (1.1689)  class_acc: 0.6875 (0.6977)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [180/781]  eta: 0:09:32  lr: 0.000385  min_lr: 0.000385  loss: 1.1933 (1.1707)  class_acc: 0.7188 (0.6972)  weight_decay: 0.0500 (0.0500)  time: 0.9361  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [190/781]  eta: 0:09:22  lr: 0.000385  min_lr: 0.000385  loss: 1.1575 (1.1683)  class_acc: 0.7188 (0.6987)  weight_decay: 0.0500 (0.0500)  time: 0.9369  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [200/781]  eta: 0:09:12  lr: 0.000385  min_lr: 0.000385  loss: 1.1480 (1.1681)  class_acc: 0.7031 (0.6991)  weight_decay: 0.0500 (0.0500)  time: 0.9375  data: 0.0009  max mem: 8477\n",
            "Epoch: [1]  [210/781]  eta: 0:09:02  lr: 0.000384  min_lr: 0.000384  loss: 1.1480 (1.1690)  class_acc: 0.7031 (0.6992)  weight_decay: 0.0500 (0.0500)  time: 0.9360  data: 0.0007  max mem: 8477\n",
            "Epoch: [1]  [220/781]  eta: 0:08:52  lr: 0.000384  min_lr: 0.000384  loss: 1.1709 (1.1680)  class_acc: 0.7031 (0.7005)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0009  max mem: 8477\n",
            "Epoch: [1]  [230/781]  eta: 0:08:43  lr: 0.000384  min_lr: 0.000384  loss: 1.1709 (1.1689)  class_acc: 0.7188 (0.7004)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [240/781]  eta: 0:08:33  lr: 0.000383  min_lr: 0.000383  loss: 1.1563 (1.1673)  class_acc: 0.7188 (0.7018)  weight_decay: 0.0500 (0.0500)  time: 0.9371  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [250/781]  eta: 0:08:23  lr: 0.000383  min_lr: 0.000383  loss: 1.1103 (1.1669)  class_acc: 0.7344 (0.7021)  weight_decay: 0.0500 (0.0500)  time: 0.9376  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [260/781]  eta: 0:08:13  lr: 0.000383  min_lr: 0.000383  loss: 1.1004 (1.1637)  class_acc: 0.7500 (0.7039)  weight_decay: 0.0500 (0.0500)  time: 0.9380  data: 0.0015  max mem: 8477\n",
            "Epoch: [1]  [270/781]  eta: 0:08:04  lr: 0.000382  min_lr: 0.000382  loss: 1.1175 (1.1638)  class_acc: 0.7344 (0.7038)  weight_decay: 0.0500 (0.0500)  time: 0.9369  data: 0.0014  max mem: 8477\n",
            "Epoch: [1]  [280/781]  eta: 0:07:54  lr: 0.000382  min_lr: 0.000382  loss: 1.1358 (1.1623)  class_acc: 0.7031 (0.7046)  weight_decay: 0.0500 (0.0500)  time: 0.9373  data: 0.0015  max mem: 8477\n",
            "Epoch: [1]  [290/781]  eta: 0:07:44  lr: 0.000382  min_lr: 0.000382  loss: 1.1575 (1.1627)  class_acc: 0.7031 (0.7044)  weight_decay: 0.0500 (0.0500)  time: 0.9377  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [300/781]  eta: 0:07:35  lr: 0.000381  min_lr: 0.000381  loss: 1.2070 (1.1630)  class_acc: 0.7031 (0.7046)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0008  max mem: 8477\n",
            "Epoch: [1]  [310/781]  eta: 0:07:25  lr: 0.000381  min_lr: 0.000381  loss: 1.1452 (1.1613)  class_acc: 0.7031 (0.7052)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0009  max mem: 8477\n",
            "Epoch: [1]  [320/781]  eta: 0:07:16  lr: 0.000381  min_lr: 0.000381  loss: 1.0920 (1.1591)  class_acc: 0.7188 (0.7060)  weight_decay: 0.0500 (0.0500)  time: 0.9373  data: 0.0015  max mem: 8477\n",
            "Epoch: [1]  [330/781]  eta: 0:07:06  lr: 0.000380  min_lr: 0.000380  loss: 1.0900 (1.1576)  class_acc: 0.7188 (0.7067)  weight_decay: 0.0500 (0.0500)  time: 0.9367  data: 0.0016  max mem: 8477\n",
            "Epoch: [1]  [340/781]  eta: 0:06:56  lr: 0.000380  min_lr: 0.000380  loss: 1.1238 (1.1567)  class_acc: 0.7344 (0.7074)  weight_decay: 0.0500 (0.0500)  time: 0.9367  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [350/781]  eta: 0:06:47  lr: 0.000380  min_lr: 0.000380  loss: 1.1391 (1.1571)  class_acc: 0.6875 (0.7071)  weight_decay: 0.0500 (0.0500)  time: 0.9368  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [360/781]  eta: 0:06:37  lr: 0.000379  min_lr: 0.000379  loss: 1.1565 (1.1566)  class_acc: 0.6719 (0.7072)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [370/781]  eta: 0:06:28  lr: 0.000379  min_lr: 0.000379  loss: 1.1430 (1.1566)  class_acc: 0.6875 (0.7067)  weight_decay: 0.0500 (0.0500)  time: 0.9355  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [380/781]  eta: 0:06:18  lr: 0.000379  min_lr: 0.000379  loss: 1.1043 (1.1550)  class_acc: 0.7188 (0.7076)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0015  max mem: 8477\n",
            "Epoch: [1]  [390/781]  eta: 0:06:09  lr: 0.000378  min_lr: 0.000378  loss: 1.0752 (1.1537)  class_acc: 0.7344 (0.7082)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0017  max mem: 8477\n",
            "Epoch: [1]  [400/781]  eta: 0:05:59  lr: 0.000378  min_lr: 0.000378  loss: 1.0747 (1.1525)  class_acc: 0.7344 (0.7088)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0015  max mem: 8477\n",
            "Epoch: [1]  [410/781]  eta: 0:05:50  lr: 0.000378  min_lr: 0.000378  loss: 1.1236 (1.1526)  class_acc: 0.7188 (0.7088)  weight_decay: 0.0500 (0.0500)  time: 0.9368  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [420/781]  eta: 0:05:40  lr: 0.000377  min_lr: 0.000377  loss: 1.1236 (1.1517)  class_acc: 0.7188 (0.7090)  weight_decay: 0.0500 (0.0500)  time: 0.9373  data: 0.0014  max mem: 8477\n",
            "Epoch: [1]  [430/781]  eta: 0:05:31  lr: 0.000377  min_lr: 0.000377  loss: 1.1164 (1.1517)  class_acc: 0.7188 (0.7092)  weight_decay: 0.0500 (0.0500)  time: 0.9373  data: 0.0017  max mem: 8477\n",
            "Epoch: [1]  [440/781]  eta: 0:05:21  lr: 0.000376  min_lr: 0.000376  loss: 1.1451 (1.1520)  class_acc: 0.7031 (0.7092)  weight_decay: 0.0500 (0.0500)  time: 0.9370  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [450/781]  eta: 0:05:12  lr: 0.000376  min_lr: 0.000376  loss: 1.1253 (1.1506)  class_acc: 0.7188 (0.7100)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [460/781]  eta: 0:05:02  lr: 0.000376  min_lr: 0.000376  loss: 1.1105 (1.1497)  class_acc: 0.7188 (0.7101)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [470/781]  eta: 0:04:53  lr: 0.000375  min_lr: 0.000375  loss: 1.1156 (1.1484)  class_acc: 0.7031 (0.7107)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0009  max mem: 8477\n",
            "Epoch: [1]  [480/781]  eta: 0:04:43  lr: 0.000375  min_lr: 0.000375  loss: 1.1309 (1.1481)  class_acc: 0.7031 (0.7105)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0014  max mem: 8477\n",
            "Epoch: [1]  [490/781]  eta: 0:04:34  lr: 0.000374  min_lr: 0.000374  loss: 1.1498 (1.1488)  class_acc: 0.6875 (0.7100)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [500/781]  eta: 0:04:24  lr: 0.000374  min_lr: 0.000374  loss: 1.1404 (1.1491)  class_acc: 0.6875 (0.7101)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0008  max mem: 8477\n",
            "Epoch: [1]  [510/781]  eta: 0:04:15  lr: 0.000374  min_lr: 0.000374  loss: 1.0995 (1.1486)  class_acc: 0.7188 (0.7102)  weight_decay: 0.0500 (0.0500)  time: 0.9355  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [520/781]  eta: 0:04:05  lr: 0.000373  min_lr: 0.000373  loss: 1.1087 (1.1480)  class_acc: 0.7344 (0.7107)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [530/781]  eta: 0:03:56  lr: 0.000373  min_lr: 0.000373  loss: 1.1436 (1.1483)  class_acc: 0.7188 (0.7104)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [540/781]  eta: 0:03:46  lr: 0.000372  min_lr: 0.000372  loss: 1.1552 (1.1476)  class_acc: 0.7031 (0.7108)  weight_decay: 0.0500 (0.0500)  time: 0.9377  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [550/781]  eta: 0:03:37  lr: 0.000372  min_lr: 0.000372  loss: 1.0904 (1.1463)  class_acc: 0.7344 (0.7114)  weight_decay: 0.0500 (0.0500)  time: 0.9364  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [560/781]  eta: 0:03:28  lr: 0.000372  min_lr: 0.000372  loss: 1.0960 (1.1458)  class_acc: 0.7344 (0.7118)  weight_decay: 0.0500 (0.0500)  time: 0.9360  data: 0.0015  max mem: 8477\n",
            "Epoch: [1]  [570/781]  eta: 0:03:18  lr: 0.000371  min_lr: 0.000371  loss: 1.1447 (1.1462)  class_acc: 0.7031 (0.7113)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0015  max mem: 8477\n",
            "Epoch: [1]  [580/781]  eta: 0:03:09  lr: 0.000371  min_lr: 0.000371  loss: 1.1564 (1.1458)  class_acc: 0.7031 (0.7114)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [590/781]  eta: 0:02:59  lr: 0.000370  min_lr: 0.000370  loss: 1.0366 (1.1442)  class_acc: 0.7344 (0.7122)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [600/781]  eta: 0:02:50  lr: 0.000370  min_lr: 0.000370  loss: 1.0499 (1.1433)  class_acc: 0.7344 (0.7124)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [610/781]  eta: 0:02:40  lr: 0.000370  min_lr: 0.000370  loss: 1.1134 (1.1426)  class_acc: 0.7188 (0.7125)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0008  max mem: 8477\n",
            "Epoch: [1]  [620/781]  eta: 0:02:31  lr: 0.000369  min_lr: 0.000369  loss: 1.1070 (1.1422)  class_acc: 0.7031 (0.7126)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0009  max mem: 8477\n",
            "Epoch: [1]  [630/781]  eta: 0:02:22  lr: 0.000369  min_lr: 0.000369  loss: 1.1120 (1.1423)  class_acc: 0.7031 (0.7125)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [640/781]  eta: 0:02:12  lr: 0.000368  min_lr: 0.000368  loss: 1.1154 (1.1416)  class_acc: 0.7188 (0.7132)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0010  max mem: 8477\n",
            "Epoch: [1]  [650/781]  eta: 0:02:03  lr: 0.000368  min_lr: 0.000368  loss: 1.0759 (1.1404)  class_acc: 0.7500 (0.7135)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0020  max mem: 8477\n",
            "Epoch: [1]  [660/781]  eta: 0:01:53  lr: 0.000367  min_lr: 0.000367  loss: 1.1002 (1.1403)  class_acc: 0.7188 (0.7134)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0018  max mem: 8477\n",
            "Epoch: [1]  [670/781]  eta: 0:01:44  lr: 0.000367  min_lr: 0.000367  loss: 1.1036 (1.1392)  class_acc: 0.7188 (0.7139)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0007  max mem: 8477\n",
            "Epoch: [1]  [680/781]  eta: 0:01:34  lr: 0.000367  min_lr: 0.000367  loss: 1.1256 (1.1392)  class_acc: 0.7188 (0.7142)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [690/781]  eta: 0:01:25  lr: 0.000366  min_lr: 0.000366  loss: 1.1256 (1.1387)  class_acc: 0.7188 (0.7146)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [700/781]  eta: 0:01:16  lr: 0.000366  min_lr: 0.000366  loss: 1.0918 (1.1378)  class_acc: 0.7500 (0.7150)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0014  max mem: 8477\n",
            "Epoch: [1]  [710/781]  eta: 0:01:06  lr: 0.000365  min_lr: 0.000365  loss: 1.0859 (1.1369)  class_acc: 0.7500 (0.7156)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [720/781]  eta: 0:00:57  lr: 0.000365  min_lr: 0.000365  loss: 1.1130 (1.1368)  class_acc: 0.7188 (0.7155)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [730/781]  eta: 0:00:47  lr: 0.000364  min_lr: 0.000364  loss: 1.1308 (1.1366)  class_acc: 0.7188 (0.7156)  weight_decay: 0.0500 (0.0500)  time: 0.9327  data: 0.0011  max mem: 8477\n",
            "Epoch: [1]  [740/781]  eta: 0:00:38  lr: 0.000364  min_lr: 0.000364  loss: 1.1481 (1.1367)  class_acc: 0.7188 (0.7156)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [750/781]  eta: 0:00:29  lr: 0.000363  min_lr: 0.000363  loss: 1.1102 (1.1360)  class_acc: 0.7344 (0.7161)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0013  max mem: 8477\n",
            "Epoch: [1]  [760/781]  eta: 0:00:19  lr: 0.000363  min_lr: 0.000363  loss: 1.0761 (1.1356)  class_acc: 0.7344 (0.7164)  weight_decay: 0.0500 (0.0500)  time: 0.9360  data: 0.0015  max mem: 8477\n",
            "Epoch: [1]  [770/781]  eta: 0:00:10  lr: 0.000362  min_lr: 0.000362  loss: 1.0518 (1.1351)  class_acc: 0.7344 (0.7167)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0012  max mem: 8477\n",
            "Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000362  min_lr: 0.000362  loss: 1.0455 (1.1345)  class_acc: 0.7344 (0.7169)  weight_decay: 0.0500 (0.0500)  time: 0.9327  data: 0.0005  max mem: 8477\n",
            "Epoch: [1] Total time: 0:12:14 (0.9400 s / it)\n",
            "Averaged stats: lr: 0.000362  min_lr: 0.000362  loss: 1.0455 (1.1345)  class_acc: 0.7344 (0.7169)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:05:20  loss: 0.3998 (0.3998)  acc1: 89.5833 (89.5833)  acc5: 100.0000 (100.0000)  time: 3.0550  data: 2.5292  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:06  loss: 0.3036 (0.3167)  acc1: 91.6667 (91.7614)  acc5: 100.0000 (99.9053)  time: 0.7010  data: 0.2389  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:49  loss: 0.3002 (0.3032)  acc1: 92.7083 (92.4107)  acc5: 100.0000 (99.8016)  time: 0.4614  data: 0.0061  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:40  loss: 0.3544 (0.3411)  acc1: 90.6250 (91.0954)  acc5: 100.0000 (99.7648)  time: 0.4576  data: 0.0013  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:34  loss: 0.4916 (0.3967)  acc1: 85.4167 (88.9482)  acc5: 100.0000 (99.7713)  time: 0.4594  data: 0.0003  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:28  loss: 0.4139 (0.3691)  acc1: 87.5000 (90.0735)  acc5: 100.0000 (99.8162)  time: 0.4613  data: 0.0006  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.3144 (0.3850)  acc1: 89.5833 (89.3955)  acc5: 100.0000 (99.7609)  time: 0.4620  data: 0.0014  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.2833 (0.3623)  acc1: 89.5833 (90.3022)  acc5: 100.0000 (99.7653)  time: 0.4632  data: 0.0014  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.2672 (0.3614)  acc1: 92.7083 (90.3807)  acc5: 100.0000 (99.7428)  time: 0.4634  data: 0.0013  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.3186 (0.3531)  acc1: 92.7083 (90.6708)  acc5: 100.0000 (99.7482)  time: 0.4623  data: 0.0010  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.2051 (0.3368)  acc1: 96.8750 (91.3057)  acc5: 100.0000 (99.7628)  time: 0.4601  data: 0.0003  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.1901 (0.3301)  acc1: 96.8750 (91.5000)  acc5: 100.0000 (99.7700)  time: 0.4404  data: 0.0002  max mem: 8477\n",
            "Test: Total time: 0:00:50 (0.4849 s / it)\n",
            "* Acc@1 91.500 Acc@5 99.770 loss 0.330\n",
            "Accuracy of the model on the 10000 test images: 91.5%\n",
            "Max accuracy: 91.50%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [2]  [  0/781]  eta: 0:38:51  lr: 0.000362  min_lr: 0.000362  loss: 1.2637 (1.2637)  class_acc: 0.6406 (0.6406)  weight_decay: 0.0500 (0.0500)  time: 2.9849  data: 1.9526  max mem: 8477\n",
            "Epoch: [2]  [ 10/781]  eta: 0:14:26  lr: 0.000361  min_lr: 0.000361  loss: 1.1237 (1.1363)  class_acc: 0.7188 (0.7159)  weight_decay: 0.0500 (0.0500)  time: 1.1244  data: 0.1785  max mem: 8477\n",
            "Epoch: [2]  [ 20/781]  eta: 0:13:06  lr: 0.000361  min_lr: 0.000361  loss: 1.0857 (1.1035)  class_acc: 0.7344 (0.7403)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [ 30/781]  eta: 0:12:33  lr: 0.000360  min_lr: 0.000360  loss: 1.0590 (1.0970)  class_acc: 0.7656 (0.7409)  weight_decay: 0.0500 (0.0500)  time: 0.9361  data: 0.0013  max mem: 8477\n",
            "Epoch: [2]  [ 40/781]  eta: 0:12:11  lr: 0.000360  min_lr: 0.000360  loss: 1.0888 (1.1132)  class_acc: 0.7188 (0.7317)  weight_decay: 0.0500 (0.0500)  time: 0.9397  data: 0.0016  max mem: 8477\n",
            "Epoch: [2]  [ 50/781]  eta: 0:11:54  lr: 0.000360  min_lr: 0.000360  loss: 1.1134 (1.1209)  class_acc: 0.7188 (0.7255)  weight_decay: 0.0500 (0.0500)  time: 0.9392  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [ 60/781]  eta: 0:11:40  lr: 0.000359  min_lr: 0.000359  loss: 1.0926 (1.1156)  class_acc: 0.7344 (0.7254)  weight_decay: 0.0500 (0.0500)  time: 0.9368  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [ 70/781]  eta: 0:11:26  lr: 0.000359  min_lr: 0.000359  loss: 1.0649 (1.1122)  class_acc: 0.7500 (0.7269)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0015  max mem: 8477\n",
            "Epoch: [2]  [ 80/781]  eta: 0:11:14  lr: 0.000358  min_lr: 0.000358  loss: 1.0880 (1.1100)  class_acc: 0.7500 (0.7274)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0013  max mem: 8477\n",
            "Epoch: [2]  [ 90/781]  eta: 0:11:02  lr: 0.000358  min_lr: 0.000358  loss: 1.0860 (1.1100)  class_acc: 0.7500 (0.7277)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0018  max mem: 8477\n",
            "Epoch: [2]  [100/781]  eta: 0:10:51  lr: 0.000357  min_lr: 0.000357  loss: 1.0860 (1.1051)  class_acc: 0.7656 (0.7311)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0018  max mem: 8477\n",
            "Epoch: [2]  [110/781]  eta: 0:10:40  lr: 0.000357  min_lr: 0.000357  loss: 1.0919 (1.1033)  class_acc: 0.7500 (0.7307)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [120/781]  eta: 0:10:29  lr: 0.000356  min_lr: 0.000356  loss: 1.0858 (1.1001)  class_acc: 0.7344 (0.7330)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0013  max mem: 8477\n",
            "Epoch: [2]  [130/781]  eta: 0:10:19  lr: 0.000356  min_lr: 0.000356  loss: 1.0727 (1.1020)  class_acc: 0.7188 (0.7312)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0015  max mem: 8477\n",
            "Epoch: [2]  [140/781]  eta: 0:10:08  lr: 0.000355  min_lr: 0.000355  loss: 1.0398 (1.0982)  class_acc: 0.7344 (0.7322)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [150/781]  eta: 0:09:58  lr: 0.000355  min_lr: 0.000355  loss: 1.0185 (1.0977)  class_acc: 0.7500 (0.7317)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0008  max mem: 8477\n",
            "Epoch: [2]  [160/781]  eta: 0:09:48  lr: 0.000354  min_lr: 0.000354  loss: 1.0185 (1.0940)  class_acc: 0.7344 (0.7340)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0006  max mem: 8477\n",
            "Epoch: [2]  [170/781]  eta: 0:09:38  lr: 0.000354  min_lr: 0.000354  loss: 1.0831 (1.0940)  class_acc: 0.7344 (0.7338)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [180/781]  eta: 0:09:28  lr: 0.000353  min_lr: 0.000353  loss: 1.0921 (1.0959)  class_acc: 0.7188 (0.7337)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0014  max mem: 8477\n",
            "Epoch: [2]  [190/781]  eta: 0:09:18  lr: 0.000352  min_lr: 0.000352  loss: 1.0625 (1.0935)  class_acc: 0.7344 (0.7353)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0012  max mem: 8477\n",
            "Epoch: [2]  [200/781]  eta: 0:09:09  lr: 0.000352  min_lr: 0.000352  loss: 1.0586 (1.0909)  class_acc: 0.7656 (0.7366)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0012  max mem: 8477\n",
            "Epoch: [2]  [210/781]  eta: 0:08:59  lr: 0.000351  min_lr: 0.000351  loss: 1.0586 (1.0892)  class_acc: 0.7656 (0.7382)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [220/781]  eta: 0:08:49  lr: 0.000351  min_lr: 0.000351  loss: 1.0643 (1.0878)  class_acc: 0.7500 (0.7383)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [230/781]  eta: 0:08:40  lr: 0.000350  min_lr: 0.000350  loss: 1.0685 (1.0887)  class_acc: 0.7500 (0.7384)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [240/781]  eta: 0:08:30  lr: 0.000350  min_lr: 0.000350  loss: 1.0786 (1.0880)  class_acc: 0.7500 (0.7388)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [250/781]  eta: 0:08:20  lr: 0.000349  min_lr: 0.000349  loss: 1.1024 (1.0885)  class_acc: 0.7344 (0.7387)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [260/781]  eta: 0:08:11  lr: 0.000349  min_lr: 0.000349  loss: 1.1082 (1.0893)  class_acc: 0.7344 (0.7386)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [270/781]  eta: 0:08:01  lr: 0.000348  min_lr: 0.000348  loss: 1.0895 (1.0895)  class_acc: 0.7344 (0.7388)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [280/781]  eta: 0:07:52  lr: 0.000348  min_lr: 0.000348  loss: 1.0912 (1.0919)  class_acc: 0.7344 (0.7378)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0012  max mem: 8477\n",
            "Epoch: [2]  [290/781]  eta: 0:07:42  lr: 0.000347  min_lr: 0.000347  loss: 1.0727 (1.0902)  class_acc: 0.7500 (0.7388)  weight_decay: 0.0500 (0.0500)  time: 0.9362  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [300/781]  eta: 0:07:32  lr: 0.000347  min_lr: 0.000347  loss: 1.0524 (1.0890)  class_acc: 0.7500 (0.7386)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [310/781]  eta: 0:07:23  lr: 0.000346  min_lr: 0.000346  loss: 1.0511 (1.0879)  class_acc: 0.7344 (0.7390)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0006  max mem: 8477\n",
            "Epoch: [2]  [320/781]  eta: 0:07:13  lr: 0.000346  min_lr: 0.000346  loss: 1.0352 (1.0866)  class_acc: 0.7656 (0.7396)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0008  max mem: 8477\n",
            "Epoch: [2]  [330/781]  eta: 0:07:04  lr: 0.000345  min_lr: 0.000345  loss: 1.0522 (1.0864)  class_acc: 0.7656 (0.7399)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [340/781]  eta: 0:06:54  lr: 0.000344  min_lr: 0.000344  loss: 1.0680 (1.0855)  class_acc: 0.7500 (0.7404)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [350/781]  eta: 0:06:45  lr: 0.000344  min_lr: 0.000344  loss: 1.0564 (1.0848)  class_acc: 0.7500 (0.7405)  weight_decay: 0.0500 (0.0500)  time: 0.9355  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [360/781]  eta: 0:06:35  lr: 0.000343  min_lr: 0.000343  loss: 1.0513 (1.0848)  class_acc: 0.7344 (0.7406)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [370/781]  eta: 0:06:26  lr: 0.000343  min_lr: 0.000343  loss: 1.0381 (1.0836)  class_acc: 0.7500 (0.7411)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [380/781]  eta: 0:06:16  lr: 0.000342  min_lr: 0.000342  loss: 1.0749 (1.0842)  class_acc: 0.7344 (0.7407)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [390/781]  eta: 0:06:07  lr: 0.000342  min_lr: 0.000342  loss: 1.0636 (1.0829)  class_acc: 0.7500 (0.7411)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [400/781]  eta: 0:05:58  lr: 0.000341  min_lr: 0.000341  loss: 1.0233 (1.0809)  class_acc: 0.7656 (0.7419)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0008  max mem: 8477\n",
            "Epoch: [2]  [410/781]  eta: 0:05:48  lr: 0.000340  min_lr: 0.000340  loss: 1.0462 (1.0812)  class_acc: 0.7656 (0.7415)  weight_decay: 0.0500 (0.0500)  time: 0.9370  data: 0.0009  max mem: 8477\n",
            "Epoch: [2]  [420/781]  eta: 0:05:39  lr: 0.000340  min_lr: 0.000340  loss: 1.0930 (1.0819)  class_acc: 0.7188 (0.7413)  weight_decay: 0.0500 (0.0500)  time: 0.9364  data: 0.0012  max mem: 8477\n",
            "Epoch: [2]  [430/781]  eta: 0:05:29  lr: 0.000339  min_lr: 0.000339  loss: 1.0728 (1.0808)  class_acc: 0.7500 (0.7416)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0015  max mem: 8477\n",
            "Epoch: [2]  [440/781]  eta: 0:05:20  lr: 0.000339  min_lr: 0.000339  loss: 1.0517 (1.0801)  class_acc: 0.7500 (0.7419)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0016  max mem: 8477\n",
            "Epoch: [2]  [450/781]  eta: 0:05:10  lr: 0.000338  min_lr: 0.000338  loss: 1.0517 (1.0792)  class_acc: 0.7500 (0.7421)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0017  max mem: 8477\n",
            "Epoch: [2]  [460/781]  eta: 0:05:01  lr: 0.000338  min_lr: 0.000338  loss: 1.0365 (1.0794)  class_acc: 0.7500 (0.7422)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0028  max mem: 8477\n",
            "Epoch: [2]  [470/781]  eta: 0:04:52  lr: 0.000337  min_lr: 0.000337  loss: 1.0458 (1.0806)  class_acc: 0.7344 (0.7419)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0023  max mem: 8477\n",
            "Epoch: [2]  [480/781]  eta: 0:04:42  lr: 0.000336  min_lr: 0.000336  loss: 1.0458 (1.0800)  class_acc: 0.7344 (0.7419)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0008  max mem: 8477\n",
            "Epoch: [2]  [490/781]  eta: 0:04:33  lr: 0.000336  min_lr: 0.000336  loss: 1.0769 (1.0808)  class_acc: 0.7344 (0.7412)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0006  max mem: 8477\n",
            "Epoch: [2]  [500/781]  eta: 0:04:23  lr: 0.000335  min_lr: 0.000335  loss: 1.0769 (1.0801)  class_acc: 0.7344 (0.7414)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0006  max mem: 8477\n",
            "Epoch: [2]  [510/781]  eta: 0:04:14  lr: 0.000335  min_lr: 0.000335  loss: 1.0622 (1.0805)  class_acc: 0.7500 (0.7410)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0009  max mem: 8477\n",
            "Epoch: [2]  [520/781]  eta: 0:04:04  lr: 0.000334  min_lr: 0.000334  loss: 1.0610 (1.0804)  class_acc: 0.7500 (0.7408)  weight_decay: 0.0500 (0.0500)  time: 0.9328  data: 0.0013  max mem: 8477\n",
            "Epoch: [2]  [530/781]  eta: 0:03:55  lr: 0.000333  min_lr: 0.000333  loss: 1.0239 (1.0797)  class_acc: 0.7500 (0.7413)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [540/781]  eta: 0:03:46  lr: 0.000333  min_lr: 0.000333  loss: 1.0190 (1.0782)  class_acc: 0.7656 (0.7421)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0012  max mem: 8477\n",
            "Epoch: [2]  [550/781]  eta: 0:03:36  lr: 0.000332  min_lr: 0.000332  loss: 1.0203 (1.0780)  class_acc: 0.7656 (0.7421)  weight_decay: 0.0500 (0.0500)  time: 0.9367  data: 0.0017  max mem: 8477\n",
            "Epoch: [2]  [560/781]  eta: 0:03:27  lr: 0.000332  min_lr: 0.000332  loss: 1.0538 (1.0774)  class_acc: 0.7500 (0.7423)  weight_decay: 0.0500 (0.0500)  time: 0.9369  data: 0.0014  max mem: 8477\n",
            "Epoch: [2]  [570/781]  eta: 0:03:17  lr: 0.000331  min_lr: 0.000331  loss: 1.0629 (1.0775)  class_acc: 0.7500 (0.7424)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0013  max mem: 8477\n",
            "Epoch: [2]  [580/781]  eta: 0:03:08  lr: 0.000330  min_lr: 0.000330  loss: 1.0486 (1.0765)  class_acc: 0.7656 (0.7430)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [590/781]  eta: 0:02:59  lr: 0.000330  min_lr: 0.000330  loss: 1.0176 (1.0758)  class_acc: 0.7812 (0.7435)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [600/781]  eta: 0:02:49  lr: 0.000329  min_lr: 0.000329  loss: 1.0608 (1.0760)  class_acc: 0.7656 (0.7433)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0015  max mem: 8477\n",
            "Epoch: [2]  [610/781]  eta: 0:02:40  lr: 0.000329  min_lr: 0.000329  loss: 1.0856 (1.0765)  class_acc: 0.7344 (0.7429)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0015  max mem: 8477\n",
            "Epoch: [2]  [620/781]  eta: 0:02:31  lr: 0.000328  min_lr: 0.000328  loss: 1.1149 (1.0775)  class_acc: 0.7031 (0.7424)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0008  max mem: 8477\n",
            "Epoch: [2]  [630/781]  eta: 0:02:21  lr: 0.000327  min_lr: 0.000327  loss: 1.0924 (1.0774)  class_acc: 0.7031 (0.7424)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [640/781]  eta: 0:02:12  lr: 0.000327  min_lr: 0.000327  loss: 1.0617 (1.0769)  class_acc: 0.7500 (0.7425)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [650/781]  eta: 0:02:02  lr: 0.000326  min_lr: 0.000326  loss: 1.0617 (1.0769)  class_acc: 0.7500 (0.7424)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0007  max mem: 8477\n",
            "Epoch: [2]  [660/781]  eta: 0:01:53  lr: 0.000325  min_lr: 0.000325  loss: 1.0287 (1.0763)  class_acc: 0.7500 (0.7427)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0008  max mem: 8477\n",
            "Epoch: [2]  [670/781]  eta: 0:01:44  lr: 0.000325  min_lr: 0.000325  loss: 1.0149 (1.0756)  class_acc: 0.7656 (0.7429)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [680/781]  eta: 0:01:34  lr: 0.000324  min_lr: 0.000324  loss: 1.0627 (1.0762)  class_acc: 0.7344 (0.7426)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0009  max mem: 8477\n",
            "Epoch: [2]  [690/781]  eta: 0:01:25  lr: 0.000324  min_lr: 0.000324  loss: 1.0832 (1.0760)  class_acc: 0.7031 (0.7425)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0010  max mem: 8477\n",
            "Epoch: [2]  [700/781]  eta: 0:01:15  lr: 0.000323  min_lr: 0.000323  loss: 1.0832 (1.0766)  class_acc: 0.7188 (0.7422)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0013  max mem: 8477\n",
            "Epoch: [2]  [710/781]  eta: 0:01:06  lr: 0.000322  min_lr: 0.000322  loss: 1.0530 (1.0762)  class_acc: 0.7344 (0.7426)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0016  max mem: 8477\n",
            "Epoch: [2]  [720/781]  eta: 0:00:57  lr: 0.000322  min_lr: 0.000322  loss: 1.0530 (1.0759)  class_acc: 0.7656 (0.7428)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0013  max mem: 8477\n",
            "Epoch: [2]  [730/781]  eta: 0:00:47  lr: 0.000321  min_lr: 0.000321  loss: 1.0588 (1.0755)  class_acc: 0.7656 (0.7431)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0007  max mem: 8477\n",
            "Epoch: [2]  [740/781]  eta: 0:00:38  lr: 0.000320  min_lr: 0.000320  loss: 1.0656 (1.0757)  class_acc: 0.7500 (0.7430)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0009  max mem: 8477\n",
            "Epoch: [2]  [750/781]  eta: 0:00:29  lr: 0.000320  min_lr: 0.000320  loss: 1.0656 (1.0756)  class_acc: 0.7500 (0.7432)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0012  max mem: 8477\n",
            "Epoch: [2]  [760/781]  eta: 0:00:19  lr: 0.000319  min_lr: 0.000319  loss: 1.0521 (1.0751)  class_acc: 0.7500 (0.7434)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0011  max mem: 8477\n",
            "Epoch: [2]  [770/781]  eta: 0:00:10  lr: 0.000318  min_lr: 0.000318  loss: 1.0521 (1.0748)  class_acc: 0.7500 (0.7436)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0009  max mem: 8477\n",
            "Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000318  min_lr: 0.000318  loss: 1.0416 (1.0746)  class_acc: 0.7500 (0.7438)  weight_decay: 0.0500 (0.0500)  time: 0.9355  data: 0.0006  max mem: 8477\n",
            "Epoch: [2] Total time: 0:12:12 (0.9378 s / it)\n",
            "Averaged stats: lr: 0.000318  min_lr: 0.000318  loss: 1.0416 (1.0746)  class_acc: 0.7500 (0.7438)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:05:58  loss: 0.2754 (0.2754)  acc1: 94.7917 (94.7917)  acc5: 100.0000 (100.0000)  time: 3.4140  data: 2.9160  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:08  loss: 0.2419 (0.2264)  acc1: 94.7917 (95.1705)  acc5: 100.0000 (99.8106)  time: 0.7262  data: 0.2679  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:50  loss: 0.1640 (0.1967)  acc1: 96.8750 (96.0317)  acc5: 100.0000 (99.9008)  time: 0.4571  data: 0.0024  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:41  loss: 0.2837 (0.2505)  acc1: 91.6667 (94.2204)  acc5: 100.0000 (99.7984)  time: 0.4580  data: 0.0014  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:34  loss: 0.3546 (0.2799)  acc1: 90.6250 (93.1657)  acc5: 100.0000 (99.8222)  time: 0.4606  data: 0.0010  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:28  loss: 0.2817 (0.2659)  acc1: 92.7083 (93.6479)  acc5: 100.0000 (99.8366)  time: 0.4640  data: 0.0012  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.2817 (0.3033)  acc1: 92.7083 (92.2302)  acc5: 100.0000 (99.7268)  time: 0.4667  data: 0.0016  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.3591 (0.3032)  acc1: 87.5000 (92.2975)  acc5: 100.0000 (99.7359)  time: 0.4658  data: 0.0010  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.2902 (0.2998)  acc1: 93.7500 (92.4897)  acc5: 100.0000 (99.7171)  time: 0.4634  data: 0.0007  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.2142 (0.2895)  acc1: 94.7917 (92.8228)  acc5: 100.0000 (99.7253)  time: 0.4623  data: 0.0010  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.2236 (0.2869)  acc1: 95.8333 (92.9559)  acc5: 100.0000 (99.7422)  time: 0.4616  data: 0.0005  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.2492 (0.2875)  acc1: 94.7917 (92.9500)  acc5: 100.0000 (99.7500)  time: 0.4424  data: 0.0002  max mem: 8477\n",
            "Test: Total time: 0:00:51 (0.4892 s / it)\n",
            "* Acc@1 92.950 Acc@5 99.750 loss 0.287\n",
            "Accuracy of the model on the 10000 test images: 93.0%\n",
            "Max accuracy: 92.95%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [3]  [  0/781]  eta: 0:44:39  lr: 0.000318  min_lr: 0.000318  loss: 1.2266 (1.2266)  class_acc: 0.6562 (0.6562)  weight_decay: 0.0500 (0.0500)  time: 3.4312  data: 2.3863  max mem: 8477\n",
            "Epoch: [3]  [ 10/781]  eta: 0:15:01  lr: 0.000317  min_lr: 0.000317  loss: 1.0579 (1.0743)  class_acc: 0.7344 (0.7557)  weight_decay: 0.0500 (0.0500)  time: 1.1687  data: 0.2185  max mem: 8477\n",
            "Epoch: [3]  [ 20/781]  eta: 0:13:24  lr: 0.000316  min_lr: 0.000316  loss: 1.0414 (1.0404)  class_acc: 0.7656 (0.7656)  weight_decay: 0.0500 (0.0500)  time: 0.9382  data: 0.0015  max mem: 8477\n",
            "Epoch: [3]  [ 30/781]  eta: 0:12:43  lr: 0.000316  min_lr: 0.000316  loss: 1.0195 (1.0422)  class_acc: 0.7812 (0.7641)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [ 40/781]  eta: 0:12:19  lr: 0.000315  min_lr: 0.000315  loss: 1.0218 (1.0468)  class_acc: 0.7656 (0.7626)  weight_decay: 0.0500 (0.0500)  time: 0.9366  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [ 50/781]  eta: 0:12:01  lr: 0.000314  min_lr: 0.000314  loss: 1.0494 (1.0493)  class_acc: 0.7500 (0.7595)  weight_decay: 0.0500 (0.0500)  time: 0.9395  data: 0.0014  max mem: 8477\n",
            "Epoch: [3]  [ 60/781]  eta: 0:11:45  lr: 0.000314  min_lr: 0.000314  loss: 1.0483 (1.0513)  class_acc: 0.7344 (0.7585)  weight_decay: 0.0500 (0.0500)  time: 0.9372  data: 0.0014  max mem: 8477\n",
            "Epoch: [3]  [ 70/781]  eta: 0:11:31  lr: 0.000313  min_lr: 0.000313  loss: 1.0451 (1.0492)  class_acc: 0.7500 (0.7603)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0017  max mem: 8477\n",
            "Epoch: [3]  [ 80/781]  eta: 0:11:18  lr: 0.000313  min_lr: 0.000313  loss: 1.0342 (1.0404)  class_acc: 0.7656 (0.7635)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0014  max mem: 8477\n",
            "Epoch: [3]  [ 90/781]  eta: 0:11:05  lr: 0.000312  min_lr: 0.000312  loss: 0.9777 (1.0343)  class_acc: 0.7656 (0.7646)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0013  max mem: 8477\n",
            "Epoch: [3]  [100/781]  eta: 0:10:54  lr: 0.000311  min_lr: 0.000311  loss: 1.0122 (1.0337)  class_acc: 0.7656 (0.7656)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [110/781]  eta: 0:10:42  lr: 0.000311  min_lr: 0.000311  loss: 1.0248 (1.0379)  class_acc: 0.7344 (0.7638)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0014  max mem: 8477\n",
            "Epoch: [3]  [120/781]  eta: 0:10:32  lr: 0.000310  min_lr: 0.000310  loss: 1.0521 (1.0350)  class_acc: 0.7500 (0.7654)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0017  max mem: 8477\n",
            "Epoch: [3]  [130/781]  eta: 0:10:21  lr: 0.000309  min_lr: 0.000309  loss: 1.0521 (1.0378)  class_acc: 0.7656 (0.7649)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [140/781]  eta: 0:10:10  lr: 0.000308  min_lr: 0.000308  loss: 1.0271 (1.0337)  class_acc: 0.7812 (0.7671)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [150/781]  eta: 0:10:00  lr: 0.000308  min_lr: 0.000308  loss: 1.0099 (1.0344)  class_acc: 0.7656 (0.7665)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0009  max mem: 8477\n",
            "Epoch: [3]  [160/781]  eta: 0:09:50  lr: 0.000307  min_lr: 0.000307  loss: 1.0162 (1.0351)  class_acc: 0.7500 (0.7648)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0013  max mem: 8477\n",
            "Epoch: [3]  [170/781]  eta: 0:09:40  lr: 0.000306  min_lr: 0.000306  loss: 1.0214 (1.0369)  class_acc: 0.7344 (0.7631)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [180/781]  eta: 0:09:30  lr: 0.000306  min_lr: 0.000306  loss: 1.0326 (1.0364)  class_acc: 0.7500 (0.7629)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [190/781]  eta: 0:09:20  lr: 0.000305  min_lr: 0.000305  loss: 1.0397 (1.0362)  class_acc: 0.7500 (0.7628)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0015  max mem: 8477\n",
            "Epoch: [3]  [200/781]  eta: 0:09:10  lr: 0.000304  min_lr: 0.000304  loss: 1.0647 (1.0386)  class_acc: 0.7344 (0.7611)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0018  max mem: 8477\n",
            "Epoch: [3]  [210/781]  eta: 0:09:00  lr: 0.000304  min_lr: 0.000304  loss: 1.0792 (1.0394)  class_acc: 0.7344 (0.7609)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0013  max mem: 8477\n",
            "Epoch: [3]  [220/781]  eta: 0:08:50  lr: 0.000303  min_lr: 0.000303  loss: 1.0259 (1.0395)  class_acc: 0.7656 (0.7611)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0009  max mem: 8477\n",
            "Epoch: [3]  [230/781]  eta: 0:08:41  lr: 0.000302  min_lr: 0.000302  loss: 1.0058 (1.0393)  class_acc: 0.7812 (0.7610)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0010  max mem: 8477\n",
            "Epoch: [3]  [240/781]  eta: 0:08:31  lr: 0.000302  min_lr: 0.000302  loss: 1.0058 (1.0383)  class_acc: 0.7812 (0.7614)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [250/781]  eta: 0:08:21  lr: 0.000301  min_lr: 0.000301  loss: 1.0338 (1.0388)  class_acc: 0.7500 (0.7610)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [260/781]  eta: 0:08:11  lr: 0.000300  min_lr: 0.000300  loss: 1.0410 (1.0388)  class_acc: 0.7500 (0.7613)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [270/781]  eta: 0:08:02  lr: 0.000300  min_lr: 0.000300  loss: 1.0621 (1.0397)  class_acc: 0.7500 (0.7608)  weight_decay: 0.0500 (0.0500)  time: 0.9362  data: 0.0017  max mem: 8477\n",
            "Epoch: [3]  [280/781]  eta: 0:07:52  lr: 0.000299  min_lr: 0.000299  loss: 1.0304 (1.0399)  class_acc: 0.7656 (0.7611)  weight_decay: 0.0500 (0.0500)  time: 0.9374  data: 0.0016  max mem: 8477\n",
            "Epoch: [3]  [290/781]  eta: 0:07:43  lr: 0.000298  min_lr: 0.000298  loss: 1.0304 (1.0401)  class_acc: 0.7656 (0.7611)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0010  max mem: 8477\n",
            "Epoch: [3]  [300/781]  eta: 0:07:33  lr: 0.000297  min_lr: 0.000297  loss: 1.0723 (1.0396)  class_acc: 0.7500 (0.7612)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [310/781]  eta: 0:07:24  lr: 0.000297  min_lr: 0.000297  loss: 1.0435 (1.0394)  class_acc: 0.7500 (0.7614)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0005  max mem: 8477\n",
            "Epoch: [3]  [320/781]  eta: 0:07:14  lr: 0.000296  min_lr: 0.000296  loss: 1.0435 (1.0387)  class_acc: 0.7656 (0.7621)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [330/781]  eta: 0:07:04  lr: 0.000295  min_lr: 0.000295  loss: 1.0310 (1.0398)  class_acc: 0.7500 (0.7616)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [340/781]  eta: 0:06:55  lr: 0.000295  min_lr: 0.000295  loss: 1.0526 (1.0405)  class_acc: 0.7500 (0.7616)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [350/781]  eta: 0:06:45  lr: 0.000294  min_lr: 0.000294  loss: 1.0633 (1.0405)  class_acc: 0.7344 (0.7613)  weight_decay: 0.0500 (0.0500)  time: 0.9355  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [360/781]  eta: 0:06:36  lr: 0.000293  min_lr: 0.000293  loss: 1.0276 (1.0397)  class_acc: 0.7500 (0.7616)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [370/781]  eta: 0:06:26  lr: 0.000293  min_lr: 0.000293  loss: 1.0117 (1.0397)  class_acc: 0.7656 (0.7618)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0010  max mem: 8477\n",
            "Epoch: [3]  [380/781]  eta: 0:06:17  lr: 0.000292  min_lr: 0.000292  loss: 1.0141 (1.0392)  class_acc: 0.7656 (0.7619)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [390/781]  eta: 0:06:08  lr: 0.000291  min_lr: 0.000291  loss: 0.9857 (1.0375)  class_acc: 0.7812 (0.7624)  weight_decay: 0.0500 (0.0500)  time: 0.9359  data: 0.0013  max mem: 8477\n",
            "Epoch: [3]  [400/781]  eta: 0:05:58  lr: 0.000290  min_lr: 0.000290  loss: 0.9444 (1.0361)  class_acc: 0.7969 (0.7628)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0010  max mem: 8477\n",
            "Epoch: [3]  [410/781]  eta: 0:05:49  lr: 0.000290  min_lr: 0.000290  loss: 1.0050 (1.0358)  class_acc: 0.7969 (0.7629)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [420/781]  eta: 0:05:39  lr: 0.000289  min_lr: 0.000289  loss: 1.0119 (1.0359)  class_acc: 0.7656 (0.7628)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0010  max mem: 8477\n",
            "Epoch: [3]  [430/781]  eta: 0:05:30  lr: 0.000288  min_lr: 0.000288  loss: 1.0062 (1.0359)  class_acc: 0.7656 (0.7627)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [440/781]  eta: 0:05:20  lr: 0.000288  min_lr: 0.000288  loss: 1.0292 (1.0360)  class_acc: 0.7500 (0.7626)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0010  max mem: 8477\n",
            "Epoch: [3]  [450/781]  eta: 0:05:11  lr: 0.000287  min_lr: 0.000287  loss: 1.0449 (1.0354)  class_acc: 0.7656 (0.7632)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [460/781]  eta: 0:05:01  lr: 0.000286  min_lr: 0.000286  loss: 0.9893 (1.0341)  class_acc: 0.7812 (0.7638)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0013  max mem: 8477\n",
            "Epoch: [3]  [470/781]  eta: 0:04:52  lr: 0.000285  min_lr: 0.000285  loss: 0.9893 (1.0348)  class_acc: 0.7812 (0.7631)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [480/781]  eta: 0:04:42  lr: 0.000285  min_lr: 0.000285  loss: 1.0034 (1.0340)  class_acc: 0.7656 (0.7634)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0015  max mem: 8477\n",
            "Epoch: [3]  [490/781]  eta: 0:04:33  lr: 0.000284  min_lr: 0.000284  loss: 1.0385 (1.0348)  class_acc: 0.7656 (0.7630)  weight_decay: 0.0500 (0.0500)  time: 0.9362  data: 0.0018  max mem: 8477\n",
            "Epoch: [3]  [500/781]  eta: 0:04:24  lr: 0.000283  min_lr: 0.000283  loss: 1.0374 (1.0347)  class_acc: 0.7656 (0.7628)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [510/781]  eta: 0:04:14  lr: 0.000282  min_lr: 0.000282  loss: 1.0359 (1.0347)  class_acc: 0.7656 (0.7629)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [520/781]  eta: 0:04:05  lr: 0.000282  min_lr: 0.000282  loss: 1.0408 (1.0347)  class_acc: 0.7812 (0.7630)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0013  max mem: 8477\n",
            "Epoch: [3]  [530/781]  eta: 0:03:55  lr: 0.000281  min_lr: 0.000281  loss: 1.0376 (1.0350)  class_acc: 0.7812 (0.7627)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0014  max mem: 8477\n",
            "Epoch: [3]  [540/781]  eta: 0:03:46  lr: 0.000280  min_lr: 0.000280  loss: 1.0201 (1.0349)  class_acc: 0.7500 (0.7629)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [550/781]  eta: 0:03:36  lr: 0.000279  min_lr: 0.000279  loss: 1.0243 (1.0351)  class_acc: 0.7656 (0.7626)  weight_decay: 0.0500 (0.0500)  time: 0.9326  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [560/781]  eta: 0:03:27  lr: 0.000279  min_lr: 0.000279  loss: 1.0298 (1.0349)  class_acc: 0.7656 (0.7626)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [570/781]  eta: 0:03:18  lr: 0.000278  min_lr: 0.000278  loss: 1.0187 (1.0351)  class_acc: 0.7656 (0.7624)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [580/781]  eta: 0:03:08  lr: 0.000277  min_lr: 0.000277  loss: 1.0187 (1.0355)  class_acc: 0.7656 (0.7620)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0017  max mem: 8477\n",
            "Epoch: [3]  [590/781]  eta: 0:02:59  lr: 0.000277  min_lr: 0.000277  loss: 1.0718 (1.0351)  class_acc: 0.7500 (0.7619)  weight_decay: 0.0500 (0.0500)  time: 0.9365  data: 0.0021  max mem: 8477\n",
            "Epoch: [3]  [600/781]  eta: 0:02:49  lr: 0.000276  min_lr: 0.000276  loss: 0.9612 (1.0337)  class_acc: 0.7812 (0.7627)  weight_decay: 0.0500 (0.0500)  time: 0.9362  data: 0.0019  max mem: 8477\n",
            "Epoch: [3]  [610/781]  eta: 0:02:40  lr: 0.000275  min_lr: 0.000275  loss: 0.9699 (1.0333)  class_acc: 0.7812 (0.7630)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0016  max mem: 8477\n",
            "Epoch: [3]  [620/781]  eta: 0:02:31  lr: 0.000274  min_lr: 0.000274  loss: 0.9917 (1.0328)  class_acc: 0.7812 (0.7635)  weight_decay: 0.0500 (0.0500)  time: 0.9367  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [630/781]  eta: 0:02:21  lr: 0.000274  min_lr: 0.000274  loss: 0.9974 (1.0321)  class_acc: 0.7812 (0.7638)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [640/781]  eta: 0:02:12  lr: 0.000273  min_lr: 0.000273  loss: 0.9885 (1.0316)  class_acc: 0.7812 (0.7639)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0007  max mem: 8477\n",
            "Epoch: [3]  [650/781]  eta: 0:02:02  lr: 0.000272  min_lr: 0.000272  loss: 0.9885 (1.0307)  class_acc: 0.7812 (0.7642)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [660/781]  eta: 0:01:53  lr: 0.000271  min_lr: 0.000271  loss: 1.0234 (1.0305)  class_acc: 0.7812 (0.7643)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0014  max mem: 8477\n",
            "Epoch: [3]  [670/781]  eta: 0:01:44  lr: 0.000271  min_lr: 0.000271  loss: 0.9733 (1.0294)  class_acc: 0.7812 (0.7648)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [680/781]  eta: 0:01:34  lr: 0.000270  min_lr: 0.000270  loss: 0.9988 (1.0304)  class_acc: 0.7656 (0.7642)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0013  max mem: 8477\n",
            "Epoch: [3]  [690/781]  eta: 0:01:25  lr: 0.000269  min_lr: 0.000269  loss: 1.0556 (1.0306)  class_acc: 0.7344 (0.7639)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [700/781]  eta: 0:01:16  lr: 0.000268  min_lr: 0.000268  loss: 1.0021 (1.0300)  class_acc: 0.7656 (0.7639)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0009  max mem: 8477\n",
            "Epoch: [3]  [710/781]  eta: 0:01:06  lr: 0.000268  min_lr: 0.000268  loss: 0.9418 (1.0287)  class_acc: 0.7812 (0.7644)  weight_decay: 0.0500 (0.0500)  time: 0.9364  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [720/781]  eta: 0:00:57  lr: 0.000267  min_lr: 0.000267  loss: 0.9392 (1.0280)  class_acc: 0.7969 (0.7649)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0012  max mem: 8477\n",
            "Epoch: [3]  [730/781]  eta: 0:00:47  lr: 0.000266  min_lr: 0.000266  loss: 1.0055 (1.0281)  class_acc: 0.7500 (0.7646)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0009  max mem: 8477\n",
            "Epoch: [3]  [740/781]  eta: 0:00:38  lr: 0.000265  min_lr: 0.000265  loss: 1.0552 (1.0288)  class_acc: 0.7344 (0.7642)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0011  max mem: 8477\n",
            "Epoch: [3]  [750/781]  eta: 0:00:29  lr: 0.000265  min_lr: 0.000265  loss: 1.0245 (1.0284)  class_acc: 0.7500 (0.7644)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0010  max mem: 8477\n",
            "Epoch: [3]  [760/781]  eta: 0:00:19  lr: 0.000264  min_lr: 0.000264  loss: 0.9870 (1.0277)  class_acc: 0.7812 (0.7647)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0007  max mem: 8477\n",
            "Epoch: [3]  [770/781]  eta: 0:00:10  lr: 0.000263  min_lr: 0.000263  loss: 1.0086 (1.0275)  class_acc: 0.7812 (0.7650)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0008  max mem: 8477\n",
            "Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000262  min_lr: 0.000262  loss: 1.0363 (1.0275)  class_acc: 0.7500 (0.7649)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0010  max mem: 8477\n",
            "Epoch: [3] Total time: 0:12:12 (0.9384 s / it)\n",
            "Averaged stats: lr: 0.000262  min_lr: 0.000262  loss: 1.0363 (1.0275)  class_acc: 0.7500 (0.7649)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:04:10  loss: 0.3332 (0.3332)  acc1: 89.5833 (89.5833)  acc5: 100.0000 (100.0000)  time: 2.3836  data: 1.8507  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:01  loss: 0.2090 (0.2274)  acc1: 95.8333 (95.4545)  acc5: 100.0000 (99.9053)  time: 0.6501  data: 0.1793  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:47  loss: 0.1965 (0.2060)  acc1: 95.8333 (96.1310)  acc5: 100.0000 (99.9504)  time: 0.4668  data: 0.0063  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:39  loss: 0.2400 (0.2378)  acc1: 93.7500 (94.8589)  acc5: 100.0000 (99.8656)  time: 0.4580  data: 0.0005  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:33  loss: 0.3016 (0.2633)  acc1: 90.6250 (93.5722)  acc5: 100.0000 (99.8476)  time: 0.4591  data: 0.0005  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:27  loss: 0.3293 (0.2781)  acc1: 89.5833 (93.0556)  acc5: 100.0000 (99.8775)  time: 0.4601  data: 0.0006  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.3649 (0.2997)  acc1: 88.5417 (92.0424)  acc5: 100.0000 (99.8634)  time: 0.4617  data: 0.0006  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.3198 (0.2923)  acc1: 90.6250 (92.4002)  acc5: 100.0000 (99.8239)  time: 0.4619  data: 0.0005  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.2196 (0.2835)  acc1: 95.8333 (92.7726)  acc5: 100.0000 (99.8071)  time: 0.4625  data: 0.0015  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.2130 (0.2747)  acc1: 95.8333 (93.1662)  acc5: 100.0000 (99.7940)  time: 0.4623  data: 0.0016  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.1914 (0.2675)  acc1: 95.8333 (93.4509)  acc5: 100.0000 (99.8144)  time: 0.4609  data: 0.0006  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.1932 (0.2662)  acc1: 95.8333 (93.5000)  acc5: 100.0000 (99.8200)  time: 0.4413  data: 0.0005  max mem: 8477\n",
            "Test: Total time: 0:00:50 (0.4792 s / it)\n",
            "* Acc@1 93.500 Acc@5 99.820 loss 0.266\n",
            "Accuracy of the model on the 10000 test images: 93.5%\n",
            "Max accuracy: 93.50%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [4]  [  0/781]  eta: 0:39:52  lr: 0.000262  min_lr: 0.000262  loss: 1.0027 (1.0027)  class_acc: 0.7656 (0.7656)  weight_decay: 0.0500 (0.0500)  time: 3.0635  data: 2.0538  max mem: 8477\n",
            "Epoch: [4]  [ 10/781]  eta: 0:14:31  lr: 0.000261  min_lr: 0.000261  loss: 1.0027 (1.0084)  class_acc: 0.7656 (0.7713)  weight_decay: 0.0500 (0.0500)  time: 1.1301  data: 0.1888  max mem: 8477\n",
            "Epoch: [4]  [ 20/781]  eta: 0:13:08  lr: 0.000261  min_lr: 0.000261  loss: 0.9355 (0.9657)  class_acc: 0.8125 (0.7961)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0017  max mem: 8477\n",
            "Epoch: [4]  [ 30/781]  eta: 0:12:35  lr: 0.000260  min_lr: 0.000260  loss: 0.9355 (0.9732)  class_acc: 0.8125 (0.7898)  weight_decay: 0.0500 (0.0500)  time: 0.9367  data: 0.0014  max mem: 8477\n",
            "Epoch: [4]  [ 40/781]  eta: 0:12:12  lr: 0.000259  min_lr: 0.000259  loss: 1.0033 (0.9879)  class_acc: 0.7500 (0.7832)  weight_decay: 0.0500 (0.0500)  time: 0.9385  data: 0.0015  max mem: 8477\n",
            "Epoch: [4]  [ 50/781]  eta: 0:11:55  lr: 0.000258  min_lr: 0.000258  loss: 1.0370 (0.9943)  class_acc: 0.7500 (0.7806)  weight_decay: 0.0500 (0.0500)  time: 0.9376  data: 0.0015  max mem: 8477\n",
            "Epoch: [4]  [ 60/781]  eta: 0:11:40  lr: 0.000258  min_lr: 0.000258  loss: 1.0242 (0.9958)  class_acc: 0.7812 (0.7797)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0014  max mem: 8477\n",
            "Epoch: [4]  [ 70/781]  eta: 0:11:26  lr: 0.000257  min_lr: 0.000257  loss: 0.9883 (0.9912)  class_acc: 0.7812 (0.7806)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [4]  [ 80/781]  eta: 0:11:14  lr: 0.000256  min_lr: 0.000256  loss: 0.9481 (0.9896)  class_acc: 0.7812 (0.7816)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0016  max mem: 8477\n",
            "Epoch: [4]  [ 90/781]  eta: 0:11:02  lr: 0.000255  min_lr: 0.000255  loss: 0.9518 (0.9874)  class_acc: 0.7812 (0.7816)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0016  max mem: 8477\n",
            "Epoch: [4]  [100/781]  eta: 0:10:51  lr: 0.000254  min_lr: 0.000254  loss: 0.9827 (0.9897)  class_acc: 0.7812 (0.7814)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0008  max mem: 8477\n",
            "Epoch: [4]  [110/781]  eta: 0:10:40  lr: 0.000254  min_lr: 0.000254  loss: 1.0010 (0.9898)  class_acc: 0.7656 (0.7807)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [120/781]  eta: 0:10:29  lr: 0.000253  min_lr: 0.000253  loss: 1.0010 (0.9888)  class_acc: 0.7656 (0.7814)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0011  max mem: 8477\n",
            "Epoch: [4]  [130/781]  eta: 0:10:19  lr: 0.000252  min_lr: 0.000252  loss: 0.9338 (0.9864)  class_acc: 0.7812 (0.7829)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [140/781]  eta: 0:10:08  lr: 0.000251  min_lr: 0.000251  loss: 0.9500 (0.9879)  class_acc: 0.7812 (0.7824)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0008  max mem: 8477\n",
            "Epoch: [4]  [150/781]  eta: 0:09:58  lr: 0.000251  min_lr: 0.000251  loss: 1.0142 (0.9892)  class_acc: 0.7656 (0.7811)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0006  max mem: 8477\n",
            "Epoch: [4]  [160/781]  eta: 0:09:48  lr: 0.000250  min_lr: 0.000250  loss: 1.0087 (0.9915)  class_acc: 0.7500 (0.7788)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [170/781]  eta: 0:09:38  lr: 0.000249  min_lr: 0.000249  loss: 0.9895 (0.9925)  class_acc: 0.7656 (0.7773)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0010  max mem: 8477\n",
            "Epoch: [4]  [180/781]  eta: 0:09:28  lr: 0.000248  min_lr: 0.000248  loss: 1.0193 (0.9947)  class_acc: 0.7656 (0.7764)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [190/781]  eta: 0:09:19  lr: 0.000247  min_lr: 0.000247  loss: 1.0099 (0.9943)  class_acc: 0.7812 (0.7769)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [200/781]  eta: 0:09:09  lr: 0.000247  min_lr: 0.000247  loss: 0.9868 (0.9922)  class_acc: 0.7969 (0.7787)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0013  max mem: 8477\n",
            "Epoch: [4]  [210/781]  eta: 0:08:59  lr: 0.000246  min_lr: 0.000246  loss: 0.9754 (0.9943)  class_acc: 0.7812 (0.7772)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0018  max mem: 8477\n",
            "Epoch: [4]  [220/781]  eta: 0:08:49  lr: 0.000245  min_lr: 0.000245  loss: 0.9511 (0.9921)  class_acc: 0.7812 (0.7784)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0016  max mem: 8477\n",
            "Epoch: [4]  [230/781]  eta: 0:08:40  lr: 0.000244  min_lr: 0.000244  loss: 0.9918 (0.9941)  class_acc: 0.7812 (0.7775)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0014  max mem: 8477\n",
            "Epoch: [4]  [240/781]  eta: 0:08:30  lr: 0.000244  min_lr: 0.000244  loss: 0.9826 (0.9927)  class_acc: 0.7812 (0.7782)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0017  max mem: 8477\n",
            "Epoch: [4]  [250/781]  eta: 0:08:20  lr: 0.000243  min_lr: 0.000243  loss: 0.9793 (0.9925)  class_acc: 0.7812 (0.7784)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0014  max mem: 8477\n",
            "Epoch: [4]  [260/781]  eta: 0:08:11  lr: 0.000242  min_lr: 0.000242  loss: 1.0026 (0.9936)  class_acc: 0.7812 (0.7781)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0011  max mem: 8477\n",
            "Epoch: [4]  [270/781]  eta: 0:08:01  lr: 0.000241  min_lr: 0.000241  loss: 0.9913 (0.9935)  class_acc: 0.7812 (0.7785)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [280/781]  eta: 0:07:52  lr: 0.000240  min_lr: 0.000240  loss: 0.9894 (0.9955)  class_acc: 0.7656 (0.7777)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0013  max mem: 8477\n",
            "Epoch: [4]  [290/781]  eta: 0:07:42  lr: 0.000240  min_lr: 0.000240  loss: 0.9894 (0.9941)  class_acc: 0.7656 (0.7782)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [300/781]  eta: 0:07:32  lr: 0.000239  min_lr: 0.000239  loss: 1.0029 (0.9956)  class_acc: 0.7812 (0.7778)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0010  max mem: 8477\n",
            "Epoch: [4]  [310/781]  eta: 0:07:23  lr: 0.000238  min_lr: 0.000238  loss: 1.0066 (0.9962)  class_acc: 0.7656 (0.7775)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [320/781]  eta: 0:07:13  lr: 0.000237  min_lr: 0.000237  loss: 0.9754 (0.9961)  class_acc: 0.7812 (0.7775)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0011  max mem: 8477\n",
            "Epoch: [4]  [330/781]  eta: 0:07:04  lr: 0.000236  min_lr: 0.000236  loss: 0.9738 (0.9953)  class_acc: 0.7812 (0.7778)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [340/781]  eta: 0:06:54  lr: 0.000236  min_lr: 0.000236  loss: 0.9913 (0.9961)  class_acc: 0.7812 (0.7775)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [350/781]  eta: 0:06:45  lr: 0.000235  min_lr: 0.000235  loss: 0.9721 (0.9949)  class_acc: 0.7656 (0.7778)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0011  max mem: 8477\n",
            "Epoch: [4]  [360/781]  eta: 0:06:35  lr: 0.000234  min_lr: 0.000234  loss: 0.9342 (0.9942)  class_acc: 0.7812 (0.7785)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [370/781]  eta: 0:06:26  lr: 0.000233  min_lr: 0.000233  loss: 0.9613 (0.9936)  class_acc: 0.7969 (0.7790)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0006  max mem: 8477\n",
            "Epoch: [4]  [380/781]  eta: 0:06:16  lr: 0.000233  min_lr: 0.000233  loss: 0.9613 (0.9925)  class_acc: 0.7969 (0.7796)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [390/781]  eta: 0:06:07  lr: 0.000232  min_lr: 0.000232  loss: 0.9591 (0.9915)  class_acc: 0.7969 (0.7798)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [400/781]  eta: 0:05:58  lr: 0.000231  min_lr: 0.000231  loss: 0.9591 (0.9901)  class_acc: 0.7969 (0.7804)  weight_decay: 0.0500 (0.0500)  time: 0.9376  data: 0.0019  max mem: 8477\n",
            "Epoch: [4]  [410/781]  eta: 0:05:48  lr: 0.000230  min_lr: 0.000230  loss: 0.9714 (0.9904)  class_acc: 0.7969 (0.7801)  weight_decay: 0.0500 (0.0500)  time: 0.9369  data: 0.0018  max mem: 8477\n",
            "Epoch: [4]  [420/781]  eta: 0:05:39  lr: 0.000229  min_lr: 0.000229  loss: 0.9696 (0.9896)  class_acc: 0.7812 (0.7804)  weight_decay: 0.0500 (0.0500)  time: 0.9360  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [430/781]  eta: 0:05:29  lr: 0.000229  min_lr: 0.000229  loss: 0.9472 (0.9891)  class_acc: 0.7969 (0.7808)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0013  max mem: 8477\n",
            "Epoch: [4]  [440/781]  eta: 0:05:20  lr: 0.000228  min_lr: 0.000228  loss: 0.9518 (0.9888)  class_acc: 0.7969 (0.7809)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [450/781]  eta: 0:05:10  lr: 0.000227  min_lr: 0.000227  loss: 0.9354 (0.9877)  class_acc: 0.8125 (0.7817)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [460/781]  eta: 0:05:01  lr: 0.000226  min_lr: 0.000226  loss: 0.9353 (0.9872)  class_acc: 0.8125 (0.7822)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0010  max mem: 8477\n",
            "Epoch: [4]  [470/781]  eta: 0:04:52  lr: 0.000225  min_lr: 0.000225  loss: 0.9632 (0.9874)  class_acc: 0.7812 (0.7818)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0008  max mem: 8477\n",
            "Epoch: [4]  [480/781]  eta: 0:04:42  lr: 0.000225  min_lr: 0.000225  loss: 0.9795 (0.9866)  class_acc: 0.7656 (0.7819)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0011  max mem: 8477\n",
            "Epoch: [4]  [490/781]  eta: 0:04:33  lr: 0.000224  min_lr: 0.000224  loss: 0.9795 (0.9864)  class_acc: 0.7656 (0.7818)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0015  max mem: 8477\n",
            "Epoch: [4]  [500/781]  eta: 0:04:23  lr: 0.000223  min_lr: 0.000223  loss: 0.9479 (0.9854)  class_acc: 0.7812 (0.7826)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0014  max mem: 8477\n",
            "Epoch: [4]  [510/781]  eta: 0:04:14  lr: 0.000222  min_lr: 0.000222  loss: 0.9003 (0.9854)  class_acc: 0.8125 (0.7827)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0013  max mem: 8477\n",
            "Epoch: [4]  [520/781]  eta: 0:04:05  lr: 0.000221  min_lr: 0.000221  loss: 0.9527 (0.9854)  class_acc: 0.7812 (0.7827)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [530/781]  eta: 0:03:55  lr: 0.000221  min_lr: 0.000221  loss: 0.9473 (0.9840)  class_acc: 0.7969 (0.7832)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [540/781]  eta: 0:03:46  lr: 0.000220  min_lr: 0.000220  loss: 0.8909 (0.9833)  class_acc: 0.8125 (0.7835)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0013  max mem: 8477\n",
            "Epoch: [4]  [550/781]  eta: 0:03:36  lr: 0.000219  min_lr: 0.000219  loss: 0.9607 (0.9829)  class_acc: 0.7969 (0.7837)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0011  max mem: 8477\n",
            "Epoch: [4]  [560/781]  eta: 0:03:27  lr: 0.000218  min_lr: 0.000218  loss: 0.9771 (0.9830)  class_acc: 0.7812 (0.7836)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0008  max mem: 8477\n",
            "Epoch: [4]  [570/781]  eta: 0:03:18  lr: 0.000217  min_lr: 0.000217  loss: 0.9808 (0.9839)  class_acc: 0.7812 (0.7832)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [580/781]  eta: 0:03:08  lr: 0.000217  min_lr: 0.000217  loss: 1.0149 (0.9847)  class_acc: 0.7500 (0.7828)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [590/781]  eta: 0:02:59  lr: 0.000216  min_lr: 0.000216  loss: 0.9957 (0.9840)  class_acc: 0.7656 (0.7830)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [600/781]  eta: 0:02:49  lr: 0.000215  min_lr: 0.000215  loss: 0.9132 (0.9830)  class_acc: 0.7969 (0.7834)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0008  max mem: 8477\n",
            "Epoch: [4]  [610/781]  eta: 0:02:40  lr: 0.000214  min_lr: 0.000214  loss: 0.9180 (0.9827)  class_acc: 0.7969 (0.7837)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0007  max mem: 8477\n",
            "Epoch: [4]  [620/781]  eta: 0:02:31  lr: 0.000213  min_lr: 0.000213  loss: 0.9844 (0.9826)  class_acc: 0.7812 (0.7836)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0006  max mem: 8477\n",
            "Epoch: [4]  [630/781]  eta: 0:02:21  lr: 0.000213  min_lr: 0.000213  loss: 0.9773 (0.9821)  class_acc: 0.7812 (0.7839)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0008  max mem: 8477\n",
            "Epoch: [4]  [640/781]  eta: 0:02:12  lr: 0.000212  min_lr: 0.000212  loss: 0.9773 (0.9816)  class_acc: 0.7812 (0.7839)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [650/781]  eta: 0:02:02  lr: 0.000211  min_lr: 0.000211  loss: 0.9277 (0.9808)  class_acc: 0.7969 (0.7841)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [660/781]  eta: 0:01:53  lr: 0.000210  min_lr: 0.000210  loss: 0.9277 (0.9801)  class_acc: 0.7969 (0.7843)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0008  max mem: 8477\n",
            "Epoch: [4]  [670/781]  eta: 0:01:44  lr: 0.000209  min_lr: 0.000209  loss: 0.9554 (0.9795)  class_acc: 0.7969 (0.7847)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0008  max mem: 8477\n",
            "Epoch: [4]  [680/781]  eta: 0:01:34  lr: 0.000209  min_lr: 0.000209  loss: 0.9516 (0.9794)  class_acc: 0.7812 (0.7848)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0011  max mem: 8477\n",
            "Epoch: [4]  [690/781]  eta: 0:01:25  lr: 0.000208  min_lr: 0.000208  loss: 0.9334 (0.9787)  class_acc: 0.7969 (0.7850)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0014  max mem: 8477\n",
            "Epoch: [4]  [700/781]  eta: 0:01:15  lr: 0.000207  min_lr: 0.000207  loss: 0.9155 (0.9779)  class_acc: 0.8125 (0.7854)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0014  max mem: 8477\n",
            "Epoch: [4]  [710/781]  eta: 0:01:06  lr: 0.000206  min_lr: 0.000206  loss: 0.9280 (0.9778)  class_acc: 0.7969 (0.7854)  weight_decay: 0.0500 (0.0500)  time: 0.9366  data: 0.0024  max mem: 8477\n",
            "Epoch: [4]  [720/781]  eta: 0:00:57  lr: 0.000205  min_lr: 0.000205  loss: 0.9427 (0.9769)  class_acc: 0.8125 (0.7857)  weight_decay: 0.0500 (0.0500)  time: 0.9362  data: 0.0021  max mem: 8477\n",
            "Epoch: [4]  [730/781]  eta: 0:00:47  lr: 0.000205  min_lr: 0.000205  loss: 0.9427 (0.9770)  class_acc: 0.7969 (0.7857)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0009  max mem: 8477\n",
            "Epoch: [4]  [740/781]  eta: 0:00:38  lr: 0.000204  min_lr: 0.000204  loss: 0.9783 (0.9773)  class_acc: 0.7969 (0.7858)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0012  max mem: 8477\n",
            "Epoch: [4]  [750/781]  eta: 0:00:29  lr: 0.000203  min_lr: 0.000203  loss: 0.9511 (0.9771)  class_acc: 0.7812 (0.7858)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0014  max mem: 8477\n",
            "Epoch: [4]  [760/781]  eta: 0:00:19  lr: 0.000202  min_lr: 0.000202  loss: 0.9482 (0.9768)  class_acc: 0.7969 (0.7860)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0011  max mem: 8477\n",
            "Epoch: [4]  [770/781]  eta: 0:00:10  lr: 0.000201  min_lr: 0.000201  loss: 0.9482 (0.9763)  class_acc: 0.8125 (0.7861)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0010  max mem: 8477\n",
            "Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000201  min_lr: 0.000201  loss: 0.9581 (0.9761)  class_acc: 0.7969 (0.7863)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0008  max mem: 8477\n",
            "Epoch: [4] Total time: 0:12:12 (0.9378 s / it)\n",
            "Averaged stats: lr: 0.000201  min_lr: 0.000201  loss: 0.9581 (0.9761)  class_acc: 0.7969 (0.7863)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:05:24  loss: 0.2378 (0.2378)  acc1: 94.7917 (94.7917)  acc5: 100.0000 (100.0000)  time: 3.0882  data: 2.5611  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:05  loss: 0.1860 (0.2020)  acc1: 95.8333 (95.8333)  acc5: 100.0000 (99.9053)  time: 0.6944  data: 0.2357  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:49  loss: 0.1845 (0.2044)  acc1: 95.8333 (95.7341)  acc5: 100.0000 (99.9008)  time: 0.4558  data: 0.0024  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:40  loss: 0.3200 (0.2628)  acc1: 92.7083 (93.7500)  acc5: 100.0000 (99.7312)  time: 0.4582  data: 0.0018  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:33  loss: 0.3716 (0.2873)  acc1: 89.5833 (92.5813)  acc5: 100.0000 (99.7713)  time: 0.4608  data: 0.0018  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:28  loss: 0.2994 (0.2804)  acc1: 91.6667 (92.8309)  acc5: 100.0000 (99.7958)  time: 0.4618  data: 0.0011  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.2808 (0.2982)  acc1: 89.5833 (91.8545)  acc5: 100.0000 (99.8122)  time: 0.4625  data: 0.0012  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.3132 (0.2897)  acc1: 89.5833 (92.2535)  acc5: 100.0000 (99.8093)  time: 0.4631  data: 0.0010  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.1938 (0.2773)  acc1: 96.8750 (92.8241)  acc5: 100.0000 (99.7942)  time: 0.4623  data: 0.0003  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.1921 (0.2678)  acc1: 96.8750 (93.2921)  acc5: 100.0000 (99.8054)  time: 0.4618  data: 0.0003  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.1822 (0.2607)  acc1: 96.8750 (93.6366)  acc5: 100.0000 (99.8144)  time: 0.4608  data: 0.0002  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.1782 (0.2573)  acc1: 96.8750 (93.7000)  acc5: 100.0000 (99.8200)  time: 0.4414  data: 0.0002  max mem: 8477\n",
            "Test: Total time: 0:00:50 (0.4845 s / it)\n",
            "* Acc@1 93.700 Acc@5 99.820 loss 0.257\n",
            "Accuracy of the model on the 10000 test images: 93.7%\n",
            "Max accuracy: 93.70%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [5]  [  0/781]  eta: 0:40:48  lr: 0.000200  min_lr: 0.000200  loss: 1.0934 (1.0934)  class_acc: 0.7344 (0.7344)  weight_decay: 0.0500 (0.0500)  time: 3.1347  data: 2.1063  max mem: 8477\n",
            "Epoch: [5]  [ 10/781]  eta: 0:14:34  lr: 0.000200  min_lr: 0.000200  loss: 0.9303 (0.9518)  class_acc: 0.8125 (0.7969)  weight_decay: 0.0500 (0.0500)  time: 1.1343  data: 0.1932  max mem: 8477\n",
            "Epoch: [5]  [ 20/781]  eta: 0:13:10  lr: 0.000199  min_lr: 0.000199  loss: 0.9303 (0.9450)  class_acc: 0.8125 (0.8021)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0013  max mem: 8477\n",
            "Epoch: [5]  [ 30/781]  eta: 0:12:34  lr: 0.000198  min_lr: 0.000198  loss: 0.9410 (0.9422)  class_acc: 0.7969 (0.8019)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [ 40/781]  eta: 0:12:12  lr: 0.000197  min_lr: 0.000197  loss: 0.9629 (0.9585)  class_acc: 0.7969 (0.7984)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [ 50/781]  eta: 0:11:54  lr: 0.000196  min_lr: 0.000196  loss: 0.9788 (0.9579)  class_acc: 0.7969 (0.7963)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [ 60/781]  eta: 0:11:39  lr: 0.000196  min_lr: 0.000196  loss: 0.9378 (0.9542)  class_acc: 0.7969 (0.7984)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0009  max mem: 8477\n",
            "Epoch: [5]  [ 70/781]  eta: 0:11:26  lr: 0.000195  min_lr: 0.000195  loss: 0.9203 (0.9502)  class_acc: 0.8125 (0.7991)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0009  max mem: 8477\n",
            "Epoch: [5]  [ 80/781]  eta: 0:11:14  lr: 0.000194  min_lr: 0.000194  loss: 0.9488 (0.9498)  class_acc: 0.8125 (0.8005)  weight_decay: 0.0500 (0.0500)  time: 0.9357  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [ 90/781]  eta: 0:11:02  lr: 0.000193  min_lr: 0.000193  loss: 0.9243 (0.9474)  class_acc: 0.7969 (0.8005)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [100/781]  eta: 0:10:51  lr: 0.000192  min_lr: 0.000192  loss: 0.9000 (0.9428)  class_acc: 0.8125 (0.8037)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [110/781]  eta: 0:10:40  lr: 0.000192  min_lr: 0.000192  loss: 0.9119 (0.9408)  class_acc: 0.8125 (0.8050)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0014  max mem: 8477\n",
            "Epoch: [5]  [120/781]  eta: 0:10:29  lr: 0.000191  min_lr: 0.000191  loss: 0.9377 (0.9433)  class_acc: 0.7969 (0.8038)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0014  max mem: 8477\n",
            "Epoch: [5]  [130/781]  eta: 0:10:19  lr: 0.000190  min_lr: 0.000190  loss: 0.9661 (0.9469)  class_acc: 0.7812 (0.8015)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [140/781]  eta: 0:10:08  lr: 0.000189  min_lr: 0.000189  loss: 0.9790 (0.9483)  class_acc: 0.7812 (0.8005)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [150/781]  eta: 0:09:58  lr: 0.000188  min_lr: 0.000188  loss: 0.9790 (0.9495)  class_acc: 0.7812 (0.8003)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [160/781]  eta: 0:09:48  lr: 0.000188  min_lr: 0.000188  loss: 0.9441 (0.9504)  class_acc: 0.7812 (0.7998)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [170/781]  eta: 0:09:38  lr: 0.000187  min_lr: 0.000187  loss: 0.9985 (0.9536)  class_acc: 0.7656 (0.7974)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [180/781]  eta: 0:09:28  lr: 0.000186  min_lr: 0.000186  loss: 0.9957 (0.9561)  class_acc: 0.7656 (0.7962)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0014  max mem: 8477\n",
            "Epoch: [5]  [190/781]  eta: 0:09:18  lr: 0.000185  min_lr: 0.000185  loss: 0.9433 (0.9536)  class_acc: 0.7969 (0.7970)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0014  max mem: 8477\n",
            "Epoch: [5]  [200/781]  eta: 0:09:09  lr: 0.000184  min_lr: 0.000184  loss: 0.8893 (0.9508)  class_acc: 0.8281 (0.7990)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [210/781]  eta: 0:08:59  lr: 0.000184  min_lr: 0.000184  loss: 0.8710 (0.9496)  class_acc: 0.8438 (0.7997)  weight_decay: 0.0500 (0.0500)  time: 0.9366  data: 0.0020  max mem: 8477\n",
            "Epoch: [5]  [220/781]  eta: 0:08:49  lr: 0.000183  min_lr: 0.000183  loss: 0.8826 (0.9476)  class_acc: 0.8125 (0.8006)  weight_decay: 0.0500 (0.0500)  time: 0.9371  data: 0.0019  max mem: 8477\n",
            "Epoch: [5]  [230/781]  eta: 0:08:40  lr: 0.000182  min_lr: 0.000182  loss: 0.9106 (0.9478)  class_acc: 0.8281 (0.8013)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [240/781]  eta: 0:08:30  lr: 0.000181  min_lr: 0.000181  loss: 0.9448 (0.9473)  class_acc: 0.8125 (0.8015)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [250/781]  eta: 0:08:20  lr: 0.000180  min_lr: 0.000180  loss: 0.9368 (0.9475)  class_acc: 0.7969 (0.8014)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [260/781]  eta: 0:08:11  lr: 0.000180  min_lr: 0.000180  loss: 0.9368 (0.9488)  class_acc: 0.7812 (0.8008)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [270/781]  eta: 0:08:01  lr: 0.000179  min_lr: 0.000179  loss: 1.0026 (0.9488)  class_acc: 0.7656 (0.8006)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0007  max mem: 8477\n",
            "Epoch: [5]  [280/781]  eta: 0:07:52  lr: 0.000178  min_lr: 0.000178  loss: 0.9868 (0.9492)  class_acc: 0.7812 (0.8005)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [290/781]  eta: 0:07:42  lr: 0.000177  min_lr: 0.000177  loss: 0.9636 (0.9489)  class_acc: 0.7969 (0.8009)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0006  max mem: 8477\n",
            "Epoch: [5]  [300/781]  eta: 0:07:33  lr: 0.000176  min_lr: 0.000176  loss: 0.9235 (0.9481)  class_acc: 0.8125 (0.8014)  weight_decay: 0.0500 (0.0500)  time: 0.9355  data: 0.0006  max mem: 8477\n",
            "Epoch: [5]  [310/781]  eta: 0:07:23  lr: 0.000176  min_lr: 0.000176  loss: 0.8833 (0.9459)  class_acc: 0.8438 (0.8026)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [320/781]  eta: 0:07:14  lr: 0.000175  min_lr: 0.000175  loss: 0.8904 (0.9450)  class_acc: 0.8281 (0.8030)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [330/781]  eta: 0:07:04  lr: 0.000174  min_lr: 0.000174  loss: 0.9440 (0.9455)  class_acc: 0.7969 (0.8024)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [340/781]  eta: 0:06:54  lr: 0.000173  min_lr: 0.000173  loss: 0.9045 (0.9442)  class_acc: 0.8125 (0.8030)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [350/781]  eta: 0:06:45  lr: 0.000173  min_lr: 0.000173  loss: 0.9033 (0.9441)  class_acc: 0.8125 (0.8030)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0013  max mem: 8477\n",
            "Epoch: [5]  [360/781]  eta: 0:06:36  lr: 0.000172  min_lr: 0.000172  loss: 0.9265 (0.9439)  class_acc: 0.8125 (0.8028)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0013  max mem: 8477\n",
            "Epoch: [5]  [370/781]  eta: 0:06:26  lr: 0.000171  min_lr: 0.000171  loss: 0.9527 (0.9444)  class_acc: 0.7812 (0.8028)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [380/781]  eta: 0:06:17  lr: 0.000170  min_lr: 0.000170  loss: 0.9847 (0.9453)  class_acc: 0.7812 (0.8020)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [390/781]  eta: 0:06:07  lr: 0.000169  min_lr: 0.000169  loss: 0.9854 (0.9460)  class_acc: 0.7812 (0.8017)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0009  max mem: 8477\n",
            "Epoch: [5]  [400/781]  eta: 0:05:58  lr: 0.000169  min_lr: 0.000169  loss: 0.9524 (0.9462)  class_acc: 0.7969 (0.8018)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [410/781]  eta: 0:05:48  lr: 0.000168  min_lr: 0.000168  loss: 0.9524 (0.9460)  class_acc: 0.8125 (0.8022)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [420/781]  eta: 0:05:39  lr: 0.000167  min_lr: 0.000167  loss: 0.9216 (0.9454)  class_acc: 0.8281 (0.8026)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0007  max mem: 8477\n",
            "Epoch: [5]  [430/781]  eta: 0:05:29  lr: 0.000166  min_lr: 0.000166  loss: 0.9233 (0.9458)  class_acc: 0.7969 (0.8020)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [440/781]  eta: 0:05:20  lr: 0.000165  min_lr: 0.000165  loss: 0.9233 (0.9455)  class_acc: 0.7969 (0.8019)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0013  max mem: 8477\n",
            "Epoch: [5]  [450/781]  eta: 0:05:10  lr: 0.000165  min_lr: 0.000165  loss: 0.8908 (0.9448)  class_acc: 0.8125 (0.8024)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0013  max mem: 8477\n",
            "Epoch: [5]  [460/781]  eta: 0:05:01  lr: 0.000164  min_lr: 0.000164  loss: 0.8908 (0.9441)  class_acc: 0.8125 (0.8028)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [470/781]  eta: 0:04:52  lr: 0.000163  min_lr: 0.000163  loss: 0.9167 (0.9453)  class_acc: 0.8125 (0.8022)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [480/781]  eta: 0:04:42  lr: 0.000162  min_lr: 0.000162  loss: 0.9519 (0.9448)  class_acc: 0.7812 (0.8024)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [490/781]  eta: 0:04:33  lr: 0.000161  min_lr: 0.000161  loss: 0.9233 (0.9444)  class_acc: 0.7969 (0.8027)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0014  max mem: 8477\n",
            "Epoch: [5]  [500/781]  eta: 0:04:23  lr: 0.000161  min_lr: 0.000161  loss: 0.8946 (0.9432)  class_acc: 0.8125 (0.8033)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0016  max mem: 8477\n",
            "Epoch: [5]  [510/781]  eta: 0:04:14  lr: 0.000160  min_lr: 0.000160  loss: 0.8665 (0.9425)  class_acc: 0.8125 (0.8035)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0013  max mem: 8477\n",
            "Epoch: [5]  [520/781]  eta: 0:04:04  lr: 0.000159  min_lr: 0.000159  loss: 0.8907 (0.9421)  class_acc: 0.8125 (0.8037)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0014  max mem: 8477\n",
            "Epoch: [5]  [530/781]  eta: 0:03:55  lr: 0.000158  min_lr: 0.000158  loss: 0.9242 (0.9423)  class_acc: 0.7812 (0.8036)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0018  max mem: 8477\n",
            "Epoch: [5]  [540/781]  eta: 0:03:46  lr: 0.000158  min_lr: 0.000158  loss: 0.9257 (0.9415)  class_acc: 0.7969 (0.8036)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0015  max mem: 8477\n",
            "Epoch: [5]  [550/781]  eta: 0:03:36  lr: 0.000157  min_lr: 0.000157  loss: 0.9258 (0.9421)  class_acc: 0.7812 (0.8031)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0015  max mem: 8477\n",
            "Epoch: [5]  [560/781]  eta: 0:03:27  lr: 0.000156  min_lr: 0.000156  loss: 0.9103 (0.9412)  class_acc: 0.7812 (0.8033)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [570/781]  eta: 0:03:17  lr: 0.000155  min_lr: 0.000155  loss: 0.8881 (0.9409)  class_acc: 0.8125 (0.8035)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0007  max mem: 8477\n",
            "Epoch: [5]  [580/781]  eta: 0:03:08  lr: 0.000154  min_lr: 0.000154  loss: 0.9094 (0.9403)  class_acc: 0.8125 (0.8036)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [590/781]  eta: 0:02:59  lr: 0.000154  min_lr: 0.000154  loss: 0.8739 (0.9396)  class_acc: 0.8125 (0.8037)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0013  max mem: 8477\n",
            "Epoch: [5]  [600/781]  eta: 0:02:49  lr: 0.000153  min_lr: 0.000153  loss: 0.8603 (0.9387)  class_acc: 0.8281 (0.8043)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [610/781]  eta: 0:02:40  lr: 0.000152  min_lr: 0.000152  loss: 0.9458 (0.9399)  class_acc: 0.8125 (0.8037)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0011  max mem: 8477\n",
            "Epoch: [5]  [620/781]  eta: 0:02:30  lr: 0.000151  min_lr: 0.000151  loss: 0.9754 (0.9400)  class_acc: 0.7656 (0.8036)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0009  max mem: 8477\n",
            "Epoch: [5]  [630/781]  eta: 0:02:21  lr: 0.000150  min_lr: 0.000150  loss: 0.9609 (0.9407)  class_acc: 0.7969 (0.8033)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0007  max mem: 8477\n",
            "Epoch: [5]  [640/781]  eta: 0:02:12  lr: 0.000150  min_lr: 0.000150  loss: 0.9603 (0.9409)  class_acc: 0.7812 (0.8031)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0016  max mem: 8477\n",
            "Epoch: [5]  [650/781]  eta: 0:02:02  lr: 0.000149  min_lr: 0.000149  loss: 0.9188 (0.9404)  class_acc: 0.7969 (0.8034)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0016  max mem: 8477\n",
            "Epoch: [5]  [660/781]  eta: 0:01:53  lr: 0.000148  min_lr: 0.000148  loss: 0.8897 (0.9400)  class_acc: 0.8125 (0.8037)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [670/781]  eta: 0:01:44  lr: 0.000147  min_lr: 0.000147  loss: 0.8627 (0.9390)  class_acc: 0.8281 (0.8041)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [680/781]  eta: 0:01:34  lr: 0.000147  min_lr: 0.000147  loss: 0.8800 (0.9391)  class_acc: 0.8281 (0.8042)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [690/781]  eta: 0:01:25  lr: 0.000146  min_lr: 0.000146  loss: 0.8853 (0.9378)  class_acc: 0.8281 (0.8047)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0013  max mem: 8477\n",
            "Epoch: [5]  [700/781]  eta: 0:01:15  lr: 0.000145  min_lr: 0.000145  loss: 0.8340 (0.9365)  class_acc: 0.8594 (0.8054)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [710/781]  eta: 0:01:06  lr: 0.000144  min_lr: 0.000144  loss: 0.8627 (0.9360)  class_acc: 0.8438 (0.8056)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [720/781]  eta: 0:00:57  lr: 0.000144  min_lr: 0.000144  loss: 0.9024 (0.9351)  class_acc: 0.8125 (0.8059)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [730/781]  eta: 0:00:47  lr: 0.000143  min_lr: 0.000143  loss: 0.8895 (0.9347)  class_acc: 0.7969 (0.8059)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0008  max mem: 8477\n",
            "Epoch: [5]  [740/781]  eta: 0:00:38  lr: 0.000142  min_lr: 0.000142  loss: 0.9247 (0.9348)  class_acc: 0.7969 (0.8061)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [750/781]  eta: 0:00:29  lr: 0.000141  min_lr: 0.000141  loss: 0.9360 (0.9345)  class_acc: 0.8125 (0.8061)  weight_decay: 0.0500 (0.0500)  time: 0.9328  data: 0.0012  max mem: 8477\n",
            "Epoch: [5]  [760/781]  eta: 0:00:19  lr: 0.000140  min_lr: 0.000140  loss: 0.8982 (0.9342)  class_acc: 0.8125 (0.8061)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0010  max mem: 8477\n",
            "Epoch: [5]  [770/781]  eta: 0:00:10  lr: 0.000140  min_lr: 0.000140  loss: 0.9052 (0.9343)  class_acc: 0.8125 (0.8059)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0007  max mem: 8477\n",
            "Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000139  min_lr: 0.000139  loss: 0.9320 (0.9346)  class_acc: 0.7969 (0.8057)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0007  max mem: 8477\n",
            "Epoch: [5] Total time: 0:12:12 (0.9375 s / it)\n",
            "Averaged stats: lr: 0.000139  min_lr: 0.000139  loss: 0.9320 (0.9346)  class_acc: 0.7969 (0.8057)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:04:08  loss: 0.1808 (0.1808)  acc1: 95.8333 (95.8333)  acc5: 100.0000 (100.0000)  time: 2.3673  data: 1.7985  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:02  loss: 0.1532 (0.1581)  acc1: 97.9167 (97.3485)  acc5: 100.0000 (99.9053)  time: 0.6551  data: 0.1907  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:47  loss: 0.1477 (0.1629)  acc1: 97.9167 (97.3214)  acc5: 100.0000 (99.9504)  time: 0.4703  data: 0.0162  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:39  loss: 0.2139 (0.1973)  acc1: 95.8333 (96.3710)  acc5: 100.0000 (99.8320)  time: 0.4576  data: 0.0021  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:33  loss: 0.3342 (0.2609)  acc1: 91.6667 (94.0549)  acc5: 100.0000 (99.7967)  time: 0.4591  data: 0.0010  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:27  loss: 0.3431 (0.2583)  acc1: 90.6250 (94.0360)  acc5: 100.0000 (99.7958)  time: 0.4605  data: 0.0003  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.2675 (0.2671)  acc1: 91.6667 (93.4768)  acc5: 100.0000 (99.7951)  time: 0.4617  data: 0.0005  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.2675 (0.2634)  acc1: 92.7083 (93.7353)  acc5: 100.0000 (99.7799)  time: 0.4625  data: 0.0011  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.2326 (0.2584)  acc1: 95.8333 (93.9815)  acc5: 100.0000 (99.7685)  time: 0.4625  data: 0.0014  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.2160 (0.2533)  acc1: 95.8333 (94.1850)  acc5: 100.0000 (99.7825)  time: 0.4618  data: 0.0008  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.2007 (0.2481)  acc1: 95.8333 (94.3998)  acc5: 100.0000 (99.8040)  time: 0.4611  data: 0.0002  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.1998 (0.2465)  acc1: 95.8333 (94.4100)  acc5: 100.0000 (99.8100)  time: 0.4420  data: 0.0002  max mem: 8477\n",
            "Test: Total time: 0:00:50 (0.4799 s / it)\n",
            "* Acc@1 94.410 Acc@5 99.810 loss 0.247\n",
            "Accuracy of the model on the 10000 test images: 94.4%\n",
            "Max accuracy: 94.41%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [6]  [  0/781]  eta: 0:37:57  lr: 0.000139  min_lr: 0.000139  loss: 0.8487 (0.8487)  class_acc: 0.8750 (0.8750)  weight_decay: 0.0500 (0.0500)  time: 2.9164  data: 1.9066  max mem: 8477\n",
            "Epoch: [6]  [ 10/781]  eta: 0:14:25  lr: 0.000138  min_lr: 0.000138  loss: 0.9721 (0.9879)  class_acc: 0.7969 (0.7884)  weight_decay: 0.0500 (0.0500)  time: 1.1226  data: 0.1776  max mem: 8477\n",
            "Epoch: [6]  [ 20/781]  eta: 0:13:06  lr: 0.000137  min_lr: 0.000137  loss: 0.9138 (0.9537)  class_acc: 0.8125 (0.7984)  weight_decay: 0.0500 (0.0500)  time: 0.9389  data: 0.0031  max mem: 8477\n",
            "Epoch: [6]  [ 30/781]  eta: 0:12:32  lr: 0.000137  min_lr: 0.000137  loss: 0.8649 (0.9219)  class_acc: 0.8281 (0.8155)  weight_decay: 0.0500 (0.0500)  time: 0.9361  data: 0.0015  max mem: 8477\n",
            "Epoch: [6]  [ 40/781]  eta: 0:12:10  lr: 0.000136  min_lr: 0.000136  loss: 0.8969 (0.9283)  class_acc: 0.8125 (0.8129)  weight_decay: 0.0500 (0.0500)  time: 0.9372  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [ 50/781]  eta: 0:11:53  lr: 0.000135  min_lr: 0.000135  loss: 0.9251 (0.9212)  class_acc: 0.8125 (0.8150)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0007  max mem: 8477\n",
            "Epoch: [6]  [ 60/781]  eta: 0:11:38  lr: 0.000134  min_lr: 0.000134  loss: 0.9151 (0.9222)  class_acc: 0.8125 (0.8140)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0005  max mem: 8477\n",
            "Epoch: [6]  [ 70/781]  eta: 0:11:25  lr: 0.000134  min_lr: 0.000134  loss: 0.9139 (0.9215)  class_acc: 0.8125 (0.8138)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0005  max mem: 8477\n",
            "Epoch: [6]  [ 80/781]  eta: 0:11:13  lr: 0.000133  min_lr: 0.000133  loss: 0.8916 (0.9231)  class_acc: 0.8125 (0.8129)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0009  max mem: 8477\n",
            "Epoch: [6]  [ 90/781]  eta: 0:11:01  lr: 0.000132  min_lr: 0.000132  loss: 0.8786 (0.9186)  class_acc: 0.8125 (0.8149)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0015  max mem: 8477\n",
            "Epoch: [6]  [100/781]  eta: 0:10:50  lr: 0.000131  min_lr: 0.000131  loss: 0.8945 (0.9163)  class_acc: 0.8125 (0.8165)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0015  max mem: 8477\n",
            "Epoch: [6]  [110/781]  eta: 0:10:39  lr: 0.000131  min_lr: 0.000131  loss: 0.8633 (0.9188)  class_acc: 0.8125 (0.8140)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [120/781]  eta: 0:10:29  lr: 0.000130  min_lr: 0.000130  loss: 0.9030 (0.9192)  class_acc: 0.8125 (0.8147)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [130/781]  eta: 0:10:18  lr: 0.000129  min_lr: 0.000129  loss: 0.9064 (0.9206)  class_acc: 0.8125 (0.8138)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0006  max mem: 8477\n",
            "Epoch: [6]  [140/781]  eta: 0:10:08  lr: 0.000128  min_lr: 0.000128  loss: 0.9064 (0.9187)  class_acc: 0.7969 (0.8138)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0008  max mem: 8477\n",
            "Epoch: [6]  [150/781]  eta: 0:09:58  lr: 0.000128  min_lr: 0.000128  loss: 0.8875 (0.9186)  class_acc: 0.7969 (0.8134)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0012  max mem: 8477\n",
            "Epoch: [6]  [160/781]  eta: 0:09:48  lr: 0.000127  min_lr: 0.000127  loss: 0.8835 (0.9170)  class_acc: 0.8125 (0.8133)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [170/781]  eta: 0:09:38  lr: 0.000126  min_lr: 0.000126  loss: 0.9057 (0.9189)  class_acc: 0.8125 (0.8126)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [180/781]  eta: 0:09:28  lr: 0.000125  min_lr: 0.000125  loss: 0.9057 (0.9171)  class_acc: 0.8281 (0.8144)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [190/781]  eta: 0:09:18  lr: 0.000125  min_lr: 0.000125  loss: 0.9179 (0.9181)  class_acc: 0.8281 (0.8139)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0009  max mem: 8477\n",
            "Epoch: [6]  [200/781]  eta: 0:09:08  lr: 0.000124  min_lr: 0.000124  loss: 0.9031 (0.9178)  class_acc: 0.8125 (0.8144)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0007  max mem: 8477\n",
            "Epoch: [6]  [210/781]  eta: 0:08:59  lr: 0.000123  min_lr: 0.000123  loss: 0.8960 (0.9182)  class_acc: 0.8125 (0.8139)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0008  max mem: 8477\n",
            "Epoch: [6]  [220/781]  eta: 0:08:49  lr: 0.000122  min_lr: 0.000122  loss: 0.8824 (0.9162)  class_acc: 0.8281 (0.8153)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [230/781]  eta: 0:08:39  lr: 0.000122  min_lr: 0.000122  loss: 0.8824 (0.9155)  class_acc: 0.8125 (0.8151)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0015  max mem: 8477\n",
            "Epoch: [6]  [240/781]  eta: 0:08:30  lr: 0.000121  min_lr: 0.000121  loss: 0.8858 (0.9159)  class_acc: 0.8125 (0.8142)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [250/781]  eta: 0:08:20  lr: 0.000120  min_lr: 0.000120  loss: 0.9162 (0.9162)  class_acc: 0.8125 (0.8136)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [260/781]  eta: 0:08:10  lr: 0.000119  min_lr: 0.000119  loss: 0.9267 (0.9172)  class_acc: 0.7969 (0.8131)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [270/781]  eta: 0:08:01  lr: 0.000119  min_lr: 0.000119  loss: 0.8786 (0.9147)  class_acc: 0.8281 (0.8142)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [280/781]  eta: 0:07:51  lr: 0.000118  min_lr: 0.000118  loss: 0.8457 (0.9159)  class_acc: 0.8125 (0.8138)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [290/781]  eta: 0:07:42  lr: 0.000117  min_lr: 0.000117  loss: 0.9329 (0.9151)  class_acc: 0.7969 (0.8142)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0013  max mem: 8477\n",
            "Epoch: [6]  [300/781]  eta: 0:07:32  lr: 0.000116  min_lr: 0.000116  loss: 0.8998 (0.9151)  class_acc: 0.8125 (0.8141)  weight_decay: 0.0500 (0.0500)  time: 0.9327  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [310/781]  eta: 0:07:23  lr: 0.000116  min_lr: 0.000116  loss: 0.8941 (0.9142)  class_acc: 0.8125 (0.8146)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [320/781]  eta: 0:07:13  lr: 0.000115  min_lr: 0.000115  loss: 0.9039 (0.9151)  class_acc: 0.8125 (0.8140)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0008  max mem: 8477\n",
            "Epoch: [6]  [330/781]  eta: 0:07:04  lr: 0.000114  min_lr: 0.000114  loss: 0.9192 (0.9146)  class_acc: 0.8125 (0.8142)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [340/781]  eta: 0:06:54  lr: 0.000114  min_lr: 0.000114  loss: 0.9039 (0.9140)  class_acc: 0.8125 (0.8144)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0012  max mem: 8477\n",
            "Epoch: [6]  [350/781]  eta: 0:06:45  lr: 0.000113  min_lr: 0.000113  loss: 0.8839 (0.9127)  class_acc: 0.8281 (0.8149)  weight_decay: 0.0500 (0.0500)  time: 0.9328  data: 0.0007  max mem: 8477\n",
            "Epoch: [6]  [360/781]  eta: 0:06:35  lr: 0.000112  min_lr: 0.000112  loss: 0.8591 (0.9124)  class_acc: 0.8281 (0.8150)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [370/781]  eta: 0:06:26  lr: 0.000111  min_lr: 0.000111  loss: 0.8591 (0.9115)  class_acc: 0.8281 (0.8152)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [380/781]  eta: 0:06:16  lr: 0.000111  min_lr: 0.000111  loss: 0.8680 (0.9105)  class_acc: 0.8281 (0.8153)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [390/781]  eta: 0:06:07  lr: 0.000110  min_lr: 0.000110  loss: 0.8572 (0.9091)  class_acc: 0.8281 (0.8159)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0015  max mem: 8477\n",
            "Epoch: [6]  [400/781]  eta: 0:05:57  lr: 0.000109  min_lr: 0.000109  loss: 0.8337 (0.9086)  class_acc: 0.8438 (0.8163)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0015  max mem: 8477\n",
            "Epoch: [6]  [410/781]  eta: 0:05:48  lr: 0.000109  min_lr: 0.000109  loss: 0.8383 (0.9077)  class_acc: 0.8281 (0.8168)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0013  max mem: 8477\n",
            "Epoch: [6]  [420/781]  eta: 0:05:38  lr: 0.000108  min_lr: 0.000108  loss: 0.8740 (0.9072)  class_acc: 0.8125 (0.8170)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0018  max mem: 8477\n",
            "Epoch: [6]  [430/781]  eta: 0:05:29  lr: 0.000107  min_lr: 0.000107  loss: 0.8844 (0.9071)  class_acc: 0.8281 (0.8174)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0018  max mem: 8477\n",
            "Epoch: [6]  [440/781]  eta: 0:05:20  lr: 0.000106  min_lr: 0.000106  loss: 0.8844 (0.9071)  class_acc: 0.8125 (0.8173)  weight_decay: 0.0500 (0.0500)  time: 0.9327  data: 0.0009  max mem: 8477\n",
            "Epoch: [6]  [450/781]  eta: 0:05:10  lr: 0.000106  min_lr: 0.000106  loss: 0.8625 (0.9065)  class_acc: 0.8125 (0.8177)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0006  max mem: 8477\n",
            "Epoch: [6]  [460/781]  eta: 0:05:01  lr: 0.000105  min_lr: 0.000105  loss: 0.8939 (0.9077)  class_acc: 0.8125 (0.8171)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0008  max mem: 8477\n",
            "Epoch: [6]  [470/781]  eta: 0:04:51  lr: 0.000104  min_lr: 0.000104  loss: 0.9012 (0.9073)  class_acc: 0.8125 (0.8174)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [480/781]  eta: 0:04:42  lr: 0.000104  min_lr: 0.000104  loss: 0.8180 (0.9056)  class_acc: 0.8281 (0.8180)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0008  max mem: 8477\n",
            "Epoch: [6]  [490/781]  eta: 0:04:33  lr: 0.000103  min_lr: 0.000103  loss: 0.8277 (0.9050)  class_acc: 0.8438 (0.8185)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [500/781]  eta: 0:04:23  lr: 0.000102  min_lr: 0.000102  loss: 0.8827 (0.9049)  class_acc: 0.8125 (0.8185)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0016  max mem: 8477\n",
            "Epoch: [6]  [510/781]  eta: 0:04:14  lr: 0.000101  min_lr: 0.000101  loss: 0.8750 (0.9047)  class_acc: 0.8125 (0.8187)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [520/781]  eta: 0:04:04  lr: 0.000101  min_lr: 0.000101  loss: 0.8441 (0.9045)  class_acc: 0.8438 (0.8188)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [530/781]  eta: 0:03:55  lr: 0.000100  min_lr: 0.000100  loss: 0.8556 (0.9038)  class_acc: 0.8281 (0.8190)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0008  max mem: 8477\n",
            "Epoch: [6]  [540/781]  eta: 0:03:46  lr: 0.000099  min_lr: 0.000099  loss: 0.8531 (0.9029)  class_acc: 0.8281 (0.8194)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0009  max mem: 8477\n",
            "Epoch: [6]  [550/781]  eta: 0:03:36  lr: 0.000099  min_lr: 0.000099  loss: 0.8437 (0.9022)  class_acc: 0.8281 (0.8197)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [560/781]  eta: 0:03:27  lr: 0.000098  min_lr: 0.000098  loss: 0.8437 (0.9014)  class_acc: 0.8281 (0.8200)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0016  max mem: 8477\n",
            "Epoch: [6]  [570/781]  eta: 0:03:17  lr: 0.000097  min_lr: 0.000097  loss: 0.8680 (0.9011)  class_acc: 0.8438 (0.8201)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0017  max mem: 8477\n",
            "Epoch: [6]  [580/781]  eta: 0:03:08  lr: 0.000097  min_lr: 0.000097  loss: 0.8697 (0.9009)  class_acc: 0.8438 (0.8205)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0014  max mem: 8477\n",
            "Epoch: [6]  [590/781]  eta: 0:02:59  lr: 0.000096  min_lr: 0.000096  loss: 0.8697 (0.9012)  class_acc: 0.8125 (0.8202)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0012  max mem: 8477\n",
            "Epoch: [6]  [600/781]  eta: 0:02:49  lr: 0.000095  min_lr: 0.000095  loss: 0.8925 (0.9017)  class_acc: 0.7969 (0.8199)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0009  max mem: 8477\n",
            "Epoch: [6]  [610/781]  eta: 0:02:40  lr: 0.000095  min_lr: 0.000095  loss: 0.8819 (0.9011)  class_acc: 0.8281 (0.8201)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [620/781]  eta: 0:02:30  lr: 0.000094  min_lr: 0.000094  loss: 0.8438 (0.9004)  class_acc: 0.8281 (0.8203)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0013  max mem: 8477\n",
            "Epoch: [6]  [630/781]  eta: 0:02:21  lr: 0.000093  min_lr: 0.000093  loss: 0.9000 (0.9006)  class_acc: 0.8281 (0.8203)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0013  max mem: 8477\n",
            "Epoch: [6]  [640/781]  eta: 0:02:12  lr: 0.000093  min_lr: 0.000093  loss: 0.9175 (0.9004)  class_acc: 0.8281 (0.8204)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [6]  [650/781]  eta: 0:02:02  lr: 0.000092  min_lr: 0.000092  loss: 0.8599 (0.8999)  class_acc: 0.8438 (0.8207)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0011  max mem: 8477\n",
            "Epoch: [6]  [660/781]  eta: 0:01:53  lr: 0.000091  min_lr: 0.000091  loss: 0.8574 (0.8991)  class_acc: 0.8438 (0.8212)  weight_decay: 0.0500 (0.0500)  time: 0.9366  data: 0.0023  max mem: 8477\n",
            "Epoch: [6]  [670/781]  eta: 0:01:44  lr: 0.000091  min_lr: 0.000091  loss: 0.8574 (0.8989)  class_acc: 0.8438 (0.8214)  weight_decay: 0.0500 (0.0500)  time: 0.9366  data: 0.0022  max mem: 8477\n",
            "Epoch: [6]  [680/781]  eta: 0:01:34  lr: 0.000090  min_lr: 0.000090  loss: 0.8941 (0.8991)  class_acc: 0.8281 (0.8213)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [690/781]  eta: 0:01:25  lr: 0.000089  min_lr: 0.000089  loss: 0.9152 (0.8991)  class_acc: 0.8281 (0.8214)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0015  max mem: 8477\n",
            "Epoch: [6]  [710/781]  eta: 0:01:06  lr: 0.000088  min_lr: 0.000088  loss: 0.8552 (0.8977)  class_acc: 0.8438 (0.8221)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0016  max mem: 8477\n",
            "Epoch: [6]  [720/781]  eta: 0:00:57  lr: 0.000087  min_lr: 0.000087  loss: 0.8552 (0.8972)  class_acc: 0.8594 (0.8223)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0013  max mem: 8477\n",
            "Epoch: [6]  [730/781]  eta: 0:00:47  lr: 0.000087  min_lr: 0.000087  loss: 0.8472 (0.8972)  class_acc: 0.8438 (0.8222)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0012  max mem: 8477\n",
            "Epoch: [6]  [740/781]  eta: 0:00:38  lr: 0.000086  min_lr: 0.000086  loss: 0.8472 (0.8970)  class_acc: 0.8281 (0.8222)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0010  max mem: 8477\n",
            "Epoch: [6]  [750/781]  eta: 0:00:29  lr: 0.000085  min_lr: 0.000085  loss: 0.8925 (0.8976)  class_acc: 0.8281 (0.8221)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0013  max mem: 8477\n",
            "Epoch: [6]  [760/781]  eta: 0:00:19  lr: 0.000085  min_lr: 0.000085  loss: 0.9012 (0.8976)  class_acc: 0.8281 (0.8221)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0013  max mem: 8477\n",
            "Epoch: [6]  [770/781]  eta: 0:00:10  lr: 0.000084  min_lr: 0.000084  loss: 0.8809 (0.8979)  class_acc: 0.8281 (0.8219)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0009  max mem: 8477\n",
            "Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000083  min_lr: 0.000083  loss: 0.8993 (0.8980)  class_acc: 0.8125 (0.8220)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0005  max mem: 8477\n",
            "Epoch: [6] Total time: 0:12:11 (0.9372 s / it)\n",
            "Averaged stats: lr: 0.000083  min_lr: 0.000083  loss: 0.8993 (0.8980)  class_acc: 0.8125 (0.8220)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:04:15  loss: 0.1489 (0.1489)  acc1: 98.9583 (98.9583)  acc5: 100.0000 (100.0000)  time: 2.4296  data: 1.9339  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:01  loss: 0.1467 (0.1589)  acc1: 98.9583 (97.9167)  acc5: 100.0000 (99.9053)  time: 0.6438  data: 0.1788  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:47  loss: 0.1467 (0.1592)  acc1: 96.8750 (97.6687)  acc5: 100.0000 (99.9504)  time: 0.4616  data: 0.0018  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:39  loss: 0.1985 (0.1922)  acc1: 96.8750 (96.5390)  acc5: 100.0000 (99.7984)  time: 0.4583  data: 0.0010  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:33  loss: 0.2940 (0.2240)  acc1: 92.7083 (95.3760)  acc5: 100.0000 (99.8222)  time: 0.4592  data: 0.0010  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:27  loss: 0.2880 (0.2235)  acc1: 93.7500 (95.4248)  acc5: 100.0000 (99.8366)  time: 0.4604  data: 0.0009  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.2491 (0.2425)  acc1: 93.7500 (94.4843)  acc5: 100.0000 (99.8122)  time: 0.4618  data: 0.0009  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.2235 (0.2370)  acc1: 91.6667 (94.7036)  acc5: 100.0000 (99.8093)  time: 0.4621  data: 0.0003  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.1838 (0.2339)  acc1: 96.8750 (94.8560)  acc5: 100.0000 (99.8071)  time: 0.4621  data: 0.0011  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.1805 (0.2289)  acc1: 96.8750 (95.0778)  acc5: 100.0000 (99.8169)  time: 0.4619  data: 0.0012  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.1872 (0.2257)  acc1: 96.8750 (95.1836)  acc5: 100.0000 (99.8350)  time: 0.4611  data: 0.0002  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.1674 (0.2245)  acc1: 95.8333 (95.2100)  acc5: 100.0000 (99.8400)  time: 0.4418  data: 0.0002  max mem: 8477\n",
            "Test: Total time: 0:00:50 (0.4789 s / it)\n",
            "* Acc@1 95.210 Acc@5 99.840 loss 0.225\n",
            "Accuracy of the model on the 10000 test images: 95.2%\n",
            "Max accuracy: 95.21%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [7]  [  0/781]  eta: 0:40:00  lr: 0.000083  min_lr: 0.000083  loss: 0.8488 (0.8488)  class_acc: 0.8594 (0.8594)  weight_decay: 0.0500 (0.0500)  time: 3.0739  data: 2.0308  max mem: 8477\n",
            "Epoch: [7]  [ 10/781]  eta: 0:14:33  lr: 0.000083  min_lr: 0.000083  loss: 0.9604 (0.9463)  class_acc: 0.8125 (0.8082)  weight_decay: 0.0500 (0.0500)  time: 1.1325  data: 0.1857  max mem: 8477\n",
            "Epoch: [7]  [ 20/781]  eta: 0:13:09  lr: 0.000082  min_lr: 0.000082  loss: 0.8724 (0.8867)  class_acc: 0.8281 (0.8304)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [ 30/781]  eta: 0:12:34  lr: 0.000081  min_lr: 0.000081  loss: 0.8403 (0.8823)  class_acc: 0.8281 (0.8281)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [ 40/781]  eta: 0:12:12  lr: 0.000081  min_lr: 0.000081  loss: 0.8602 (0.8779)  class_acc: 0.8281 (0.8308)  weight_decay: 0.0500 (0.0500)  time: 0.9374  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [ 50/781]  eta: 0:11:55  lr: 0.000080  min_lr: 0.000080  loss: 0.8883 (0.8807)  class_acc: 0.8281 (0.8290)  weight_decay: 0.0500 (0.0500)  time: 0.9381  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [ 60/781]  eta: 0:11:40  lr: 0.000079  min_lr: 0.000079  loss: 0.8483 (0.8737)  class_acc: 0.8438 (0.8340)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [ 70/781]  eta: 0:11:26  lr: 0.000079  min_lr: 0.000079  loss: 0.8423 (0.8718)  class_acc: 0.8438 (0.8334)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0012  max mem: 8477\n",
            "Epoch: [7]  [ 80/781]  eta: 0:11:14  lr: 0.000078  min_lr: 0.000078  loss: 0.8649 (0.8763)  class_acc: 0.8281 (0.8312)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [ 90/781]  eta: 0:11:02  lr: 0.000077  min_lr: 0.000077  loss: 0.8511 (0.8723)  class_acc: 0.8438 (0.8328)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [100/781]  eta: 0:10:51  lr: 0.000077  min_lr: 0.000077  loss: 0.8361 (0.8721)  class_acc: 0.8438 (0.8326)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [110/781]  eta: 0:10:40  lr: 0.000076  min_lr: 0.000076  loss: 0.8678 (0.8716)  class_acc: 0.8125 (0.8318)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [120/781]  eta: 0:10:29  lr: 0.000076  min_lr: 0.000076  loss: 0.8703 (0.8724)  class_acc: 0.8125 (0.8312)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0010  max mem: 8477\n",
            "Epoch: [7]  [130/781]  eta: 0:10:19  lr: 0.000075  min_lr: 0.000075  loss: 0.8607 (0.8714)  class_acc: 0.8281 (0.8322)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0012  max mem: 8477\n",
            "Epoch: [7]  [140/781]  eta: 0:10:09  lr: 0.000074  min_lr: 0.000074  loss: 0.8386 (0.8691)  class_acc: 0.8594 (0.8334)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0024  max mem: 8477\n",
            "Epoch: [7]  [150/781]  eta: 0:09:58  lr: 0.000074  min_lr: 0.000074  loss: 0.8673 (0.8699)  class_acc: 0.8438 (0.8334)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0021  max mem: 8477\n",
            "Epoch: [7]  [160/781]  eta: 0:09:48  lr: 0.000073  min_lr: 0.000073  loss: 0.8910 (0.8716)  class_acc: 0.8281 (0.8326)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [170/781]  eta: 0:09:38  lr: 0.000072  min_lr: 0.000072  loss: 0.8873 (0.8726)  class_acc: 0.8125 (0.8328)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [180/781]  eta: 0:09:28  lr: 0.000072  min_lr: 0.000072  loss: 0.8710 (0.8741)  class_acc: 0.8125 (0.8324)  weight_decay: 0.0500 (0.0500)  time: 0.9328  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [190/781]  eta: 0:09:19  lr: 0.000071  min_lr: 0.000071  loss: 0.9052 (0.8749)  class_acc: 0.8125 (0.8321)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [200/781]  eta: 0:09:09  lr: 0.000071  min_lr: 0.000071  loss: 0.9052 (0.8757)  class_acc: 0.8281 (0.8312)  weight_decay: 0.0500 (0.0500)  time: 0.9367  data: 0.0010  max mem: 8477\n",
            "Epoch: [7]  [210/781]  eta: 0:08:59  lr: 0.000070  min_lr: 0.000070  loss: 0.8684 (0.8747)  class_acc: 0.8438 (0.8314)  weight_decay: 0.0500 (0.0500)  time: 0.9370  data: 0.0010  max mem: 8477\n",
            "Epoch: [7]  [220/781]  eta: 0:08:49  lr: 0.000069  min_lr: 0.000069  loss: 0.8377 (0.8736)  class_acc: 0.8281 (0.8314)  weight_decay: 0.0500 (0.0500)  time: 0.9363  data: 0.0010  max mem: 8477\n",
            "Epoch: [7]  [230/781]  eta: 0:08:40  lr: 0.000069  min_lr: 0.000069  loss: 0.8377 (0.8735)  class_acc: 0.8281 (0.8313)  weight_decay: 0.0500 (0.0500)  time: 0.9372  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [240/781]  eta: 0:08:30  lr: 0.000068  min_lr: 0.000068  loss: 0.8543 (0.8730)  class_acc: 0.8281 (0.8312)  weight_decay: 0.0500 (0.0500)  time: 0.9369  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [250/781]  eta: 0:08:21  lr: 0.000068  min_lr: 0.000068  loss: 0.8543 (0.8730)  class_acc: 0.8281 (0.8312)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [260/781]  eta: 0:08:11  lr: 0.000067  min_lr: 0.000067  loss: 0.8631 (0.8728)  class_acc: 0.8281 (0.8314)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [270/781]  eta: 0:08:01  lr: 0.000066  min_lr: 0.000066  loss: 0.8835 (0.8739)  class_acc: 0.8125 (0.8307)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [280/781]  eta: 0:07:52  lr: 0.000066  min_lr: 0.000066  loss: 0.8858 (0.8755)  class_acc: 0.7969 (0.8303)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [290/781]  eta: 0:07:42  lr: 0.000065  min_lr: 0.000065  loss: 0.8714 (0.8736)  class_acc: 0.8281 (0.8311)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [300/781]  eta: 0:07:33  lr: 0.000065  min_lr: 0.000065  loss: 0.8559 (0.8744)  class_acc: 0.8281 (0.8310)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [310/781]  eta: 0:07:23  lr: 0.000064  min_lr: 0.000064  loss: 0.9263 (0.8755)  class_acc: 0.8125 (0.8304)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [320/781]  eta: 0:07:14  lr: 0.000063  min_lr: 0.000063  loss: 0.9040 (0.8758)  class_acc: 0.8125 (0.8304)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [330/781]  eta: 0:07:04  lr: 0.000063  min_lr: 0.000063  loss: 0.8760 (0.8758)  class_acc: 0.8281 (0.8304)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [340/781]  eta: 0:06:55  lr: 0.000062  min_lr: 0.000062  loss: 0.8532 (0.8757)  class_acc: 0.8281 (0.8306)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [350/781]  eta: 0:06:45  lr: 0.000062  min_lr: 0.000062  loss: 0.8546 (0.8757)  class_acc: 0.8281 (0.8310)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [360/781]  eta: 0:06:36  lr: 0.000061  min_lr: 0.000061  loss: 0.8904 (0.8758)  class_acc: 0.8281 (0.8309)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [370/781]  eta: 0:06:26  lr: 0.000061  min_lr: 0.000061  loss: 0.8771 (0.8753)  class_acc: 0.8281 (0.8310)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [380/781]  eta: 0:06:17  lr: 0.000060  min_lr: 0.000060  loss: 0.8310 (0.8743)  class_acc: 0.8281 (0.8308)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [390/781]  eta: 0:06:07  lr: 0.000059  min_lr: 0.000059  loss: 0.8310 (0.8740)  class_acc: 0.8125 (0.8308)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0012  max mem: 8477\n",
            "Epoch: [7]  [400/781]  eta: 0:05:58  lr: 0.000059  min_lr: 0.000059  loss: 0.8420 (0.8735)  class_acc: 0.8438 (0.8315)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [7]  [410/781]  eta: 0:05:48  lr: 0.000058  min_lr: 0.000058  loss: 0.8932 (0.8743)  class_acc: 0.8438 (0.8314)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [420/781]  eta: 0:05:39  lr: 0.000058  min_lr: 0.000058  loss: 0.8523 (0.8732)  class_acc: 0.8438 (0.8319)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0015  max mem: 8477\n",
            "Epoch: [7]  [430/781]  eta: 0:05:29  lr: 0.000057  min_lr: 0.000057  loss: 0.8288 (0.8730)  class_acc: 0.8438 (0.8320)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0015  max mem: 8477\n",
            "Epoch: [7]  [440/781]  eta: 0:05:20  lr: 0.000057  min_lr: 0.000057  loss: 0.8395 (0.8717)  class_acc: 0.8438 (0.8326)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [450/781]  eta: 0:05:10  lr: 0.000056  min_lr: 0.000056  loss: 0.8044 (0.8704)  class_acc: 0.8594 (0.8331)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [460/781]  eta: 0:05:01  lr: 0.000056  min_lr: 0.000056  loss: 0.8123 (0.8700)  class_acc: 0.8594 (0.8334)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [470/781]  eta: 0:04:52  lr: 0.000055  min_lr: 0.000055  loss: 0.8520 (0.8699)  class_acc: 0.8438 (0.8334)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [480/781]  eta: 0:04:42  lr: 0.000054  min_lr: 0.000054  loss: 0.8833 (0.8707)  class_acc: 0.8281 (0.8331)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0012  max mem: 8477\n",
            "Epoch: [7]  [490/781]  eta: 0:04:33  lr: 0.000054  min_lr: 0.000054  loss: 0.8943 (0.8707)  class_acc: 0.8281 (0.8333)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0010  max mem: 8477\n",
            "Epoch: [7]  [500/781]  eta: 0:04:23  lr: 0.000053  min_lr: 0.000053  loss: 0.8606 (0.8703)  class_acc: 0.8438 (0.8338)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [510/781]  eta: 0:04:14  lr: 0.000053  min_lr: 0.000053  loss: 0.8606 (0.8704)  class_acc: 0.8438 (0.8339)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [520/781]  eta: 0:04:05  lr: 0.000052  min_lr: 0.000052  loss: 0.8425 (0.8703)  class_acc: 0.8281 (0.8338)  weight_decay: 0.0500 (0.0500)  time: 0.9326  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [530/781]  eta: 0:03:55  lr: 0.000052  min_lr: 0.000052  loss: 0.8500 (0.8706)  class_acc: 0.8125 (0.8338)  weight_decay: 0.0500 (0.0500)  time: 0.9327  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [540/781]  eta: 0:03:46  lr: 0.000051  min_lr: 0.000051  loss: 0.8514 (0.8701)  class_acc: 0.8281 (0.8340)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [550/781]  eta: 0:03:36  lr: 0.000051  min_lr: 0.000051  loss: 0.8442 (0.8700)  class_acc: 0.8438 (0.8343)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [560/781]  eta: 0:03:27  lr: 0.000050  min_lr: 0.000050  loss: 0.8308 (0.8698)  class_acc: 0.8594 (0.8344)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0007  max mem: 8477\n",
            "Epoch: [7]  [570/781]  eta: 0:03:17  lr: 0.000050  min_lr: 0.000050  loss: 0.8690 (0.8701)  class_acc: 0.8438 (0.8345)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [580/781]  eta: 0:03:08  lr: 0.000049  min_lr: 0.000049  loss: 0.8735 (0.8700)  class_acc: 0.8281 (0.8344)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [590/781]  eta: 0:02:59  lr: 0.000049  min_lr: 0.000049  loss: 0.8553 (0.8696)  class_acc: 0.8438 (0.8345)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [600/781]  eta: 0:02:49  lr: 0.000048  min_lr: 0.000048  loss: 0.8462 (0.8696)  class_acc: 0.8438 (0.8346)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0008  max mem: 8477\n",
            "Epoch: [7]  [610/781]  eta: 0:02:40  lr: 0.000048  min_lr: 0.000048  loss: 0.8462 (0.8694)  class_acc: 0.8438 (0.8347)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [620/781]  eta: 0:02:31  lr: 0.000047  min_lr: 0.000047  loss: 0.8346 (0.8692)  class_acc: 0.8438 (0.8350)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [630/781]  eta: 0:02:21  lr: 0.000047  min_lr: 0.000047  loss: 0.8350 (0.8691)  class_acc: 0.8438 (0.8352)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [640/781]  eta: 0:02:12  lr: 0.000046  min_lr: 0.000046  loss: 0.8350 (0.8688)  class_acc: 0.8594 (0.8354)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0006  max mem: 8477\n",
            "Epoch: [7]  [650/781]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000046  loss: 0.8232 (0.8683)  class_acc: 0.8438 (0.8356)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0006  max mem: 8477\n",
            "Epoch: [7]  [660/781]  eta: 0:01:53  lr: 0.000045  min_lr: 0.000045  loss: 0.8382 (0.8683)  class_acc: 0.8438 (0.8357)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0006  max mem: 8477\n",
            "Epoch: [7]  [670/781]  eta: 0:01:44  lr: 0.000044  min_lr: 0.000044  loss: 0.8349 (0.8675)  class_acc: 0.8438 (0.8361)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [680/781]  eta: 0:01:34  lr: 0.000044  min_lr: 0.000044  loss: 0.8375 (0.8674)  class_acc: 0.8438 (0.8361)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [690/781]  eta: 0:01:25  lr: 0.000044  min_lr: 0.000044  loss: 0.8727 (0.8675)  class_acc: 0.8438 (0.8360)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0007  max mem: 8477\n",
            "Epoch: [7]  [700/781]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000043  loss: 0.8289 (0.8668)  class_acc: 0.8594 (0.8365)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [710/781]  eta: 0:01:06  lr: 0.000043  min_lr: 0.000043  loss: 0.8332 (0.8667)  class_acc: 0.8594 (0.8364)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0014  max mem: 8477\n",
            "Epoch: [7]  [720/781]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000042  loss: 0.8332 (0.8658)  class_acc: 0.8438 (0.8368)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [730/781]  eta: 0:00:47  lr: 0.000042  min_lr: 0.000042  loss: 0.8697 (0.8664)  class_acc: 0.8281 (0.8365)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0006  max mem: 8477\n",
            "Epoch: [7]  [740/781]  eta: 0:00:38  lr: 0.000041  min_lr: 0.000041  loss: 0.9065 (0.8668)  class_acc: 0.8125 (0.8363)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0011  max mem: 8477\n",
            "Epoch: [7]  [750/781]  eta: 0:00:29  lr: 0.000041  min_lr: 0.000041  loss: 0.8878 (0.8671)  class_acc: 0.8125 (0.8360)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0013  max mem: 8477\n",
            "Epoch: [7]  [760/781]  eta: 0:00:19  lr: 0.000040  min_lr: 0.000040  loss: 0.8416 (0.8666)  class_acc: 0.8281 (0.8362)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0012  max mem: 8477\n",
            "Epoch: [7]  [770/781]  eta: 0:00:10  lr: 0.000040  min_lr: 0.000040  loss: 0.8416 (0.8666)  class_acc: 0.8438 (0.8363)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0009  max mem: 8477\n",
            "Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000039  loss: 0.8507 (0.8666)  class_acc: 0.8438 (0.8364)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0006  max mem: 8477\n",
            "Epoch: [7] Total time: 0:12:12 (0.9376 s / it)\n",
            "Averaged stats: lr: 0.000039  min_lr: 0.000039  loss: 0.8507 (0.8666)  class_acc: 0.8438 (0.8364)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:04:36  loss: 0.2487 (0.2487)  acc1: 94.7917 (94.7917)  acc5: 100.0000 (100.0000)  time: 2.6292  data: 2.1033  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:02  loss: 0.1853 (0.1884)  acc1: 96.8750 (96.7803)  acc5: 100.0000 (99.9053)  time: 0.6534  data: 0.1933  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:47  loss: 0.1576 (0.1714)  acc1: 96.8750 (97.3214)  acc5: 100.0000 (99.9504)  time: 0.4564  data: 0.0017  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:39  loss: 0.1861 (0.1855)  acc1: 96.8750 (96.8414)  acc5: 100.0000 (99.8992)  time: 0.4617  data: 0.0021  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:33  loss: 0.2946 (0.2317)  acc1: 91.6667 (95.0203)  acc5: 100.0000 (99.8730)  time: 0.4626  data: 0.0026  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:27  loss: 0.3097 (0.2333)  acc1: 91.6667 (94.9959)  acc5: 100.0000 (99.8775)  time: 0.4596  data: 0.0018  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.2766 (0.2524)  acc1: 92.7083 (94.2111)  acc5: 100.0000 (99.8634)  time: 0.4614  data: 0.0013  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.2543 (0.2425)  acc1: 92.7083 (94.6009)  acc5: 100.0000 (99.8386)  time: 0.4619  data: 0.0012  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.1550 (0.2357)  acc1: 98.9583 (94.9460)  acc5: 100.0000 (99.8457)  time: 0.4617  data: 0.0013  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.1584 (0.2276)  acc1: 96.8750 (95.2152)  acc5: 100.0000 (99.8512)  time: 0.4619  data: 0.0015  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.1642 (0.2232)  acc1: 96.8750 (95.3899)  acc5: 100.0000 (99.8659)  time: 0.4610  data: 0.0009  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.1609 (0.2219)  acc1: 96.8750 (95.4000)  acc5: 100.0000 (99.8700)  time: 0.4417  data: 0.0004  max mem: 8477\n",
            "Test: Total time: 0:00:50 (0.4801 s / it)\n",
            "* Acc@1 95.400 Acc@5 99.870 loss 0.222\n",
            "Accuracy of the model on the 10000 test images: 95.4%\n",
            "Max accuracy: 95.40%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [8]  [  0/781]  eta: 0:41:52  lr: 0.000039  min_lr: 0.000039  loss: 0.9054 (0.9054)  class_acc: 0.8125 (0.8125)  weight_decay: 0.0500 (0.0500)  time: 3.2175  data: 2.1806  max mem: 8477\n",
            "Epoch: [8]  [ 10/781]  eta: 0:14:42  lr: 0.000039  min_lr: 0.000039  loss: 0.9054 (0.9083)  class_acc: 0.8125 (0.8239)  weight_decay: 0.0500 (0.0500)  time: 1.1446  data: 0.1990  max mem: 8477\n",
            "Epoch: [8]  [ 20/781]  eta: 0:13:14  lr: 0.000038  min_lr: 0.000038  loss: 0.8718 (0.8776)  class_acc: 0.8438 (0.8393)  weight_decay: 0.0500 (0.0500)  time: 0.9360  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [ 30/781]  eta: 0:12:38  lr: 0.000038  min_lr: 0.000038  loss: 0.8209 (0.8514)  class_acc: 0.8594 (0.8483)  weight_decay: 0.0500 (0.0500)  time: 0.9358  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [ 40/781]  eta: 0:12:15  lr: 0.000037  min_lr: 0.000037  loss: 0.8801 (0.8627)  class_acc: 0.8438 (0.8441)  weight_decay: 0.0500 (0.0500)  time: 0.9378  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [ 50/781]  eta: 0:11:57  lr: 0.000037  min_lr: 0.000037  loss: 0.8807 (0.8626)  class_acc: 0.8438 (0.8441)  weight_decay: 0.0500 (0.0500)  time: 0.9379  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [ 60/781]  eta: 0:11:42  lr: 0.000036  min_lr: 0.000036  loss: 0.8714 (0.8634)  class_acc: 0.8438 (0.8435)  weight_decay: 0.0500 (0.0500)  time: 0.9371  data: 0.0014  max mem: 8477\n",
            "Epoch: [8]  [ 70/781]  eta: 0:11:28  lr: 0.000036  min_lr: 0.000036  loss: 0.8593 (0.8572)  class_acc: 0.8438 (0.8464)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0012  max mem: 8477\n",
            "Epoch: [8]  [ 80/781]  eta: 0:11:16  lr: 0.000035  min_lr: 0.000035  loss: 0.8593 (0.8604)  class_acc: 0.8594 (0.8451)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [ 90/781]  eta: 0:11:04  lr: 0.000035  min_lr: 0.000035  loss: 0.8499 (0.8577)  class_acc: 0.8438 (0.8453)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [100/781]  eta: 0:10:52  lr: 0.000035  min_lr: 0.000035  loss: 0.8140 (0.8522)  class_acc: 0.8438 (0.8465)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [110/781]  eta: 0:10:41  lr: 0.000034  min_lr: 0.000034  loss: 0.7830 (0.8476)  class_acc: 0.8594 (0.8484)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [120/781]  eta: 0:10:30  lr: 0.000034  min_lr: 0.000034  loss: 0.8063 (0.8485)  class_acc: 0.8594 (0.8478)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0012  max mem: 8477\n",
            "Epoch: [8]  [130/781]  eta: 0:10:20  lr: 0.000033  min_lr: 0.000033  loss: 0.8576 (0.8488)  class_acc: 0.8438 (0.8478)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [140/781]  eta: 0:10:09  lr: 0.000033  min_lr: 0.000033  loss: 0.8380 (0.8523)  class_acc: 0.8594 (0.8467)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0012  max mem: 8477\n",
            "Epoch: [8]  [150/781]  eta: 0:09:59  lr: 0.000032  min_lr: 0.000032  loss: 0.8481 (0.8522)  class_acc: 0.8594 (0.8476)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [160/781]  eta: 0:09:49  lr: 0.000032  min_lr: 0.000032  loss: 0.8658 (0.8535)  class_acc: 0.8438 (0.8468)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [170/781]  eta: 0:09:39  lr: 0.000031  min_lr: 0.000031  loss: 0.8448 (0.8534)  class_acc: 0.8281 (0.8461)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0008  max mem: 8477\n",
            "Epoch: [8]  [180/781]  eta: 0:09:29  lr: 0.000031  min_lr: 0.000031  loss: 0.8432 (0.8532)  class_acc: 0.8438 (0.8466)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [190/781]  eta: 0:09:19  lr: 0.000031  min_lr: 0.000031  loss: 0.8475 (0.8534)  class_acc: 0.8281 (0.8464)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [200/781]  eta: 0:09:09  lr: 0.000030  min_lr: 0.000030  loss: 0.8356 (0.8529)  class_acc: 0.8281 (0.8458)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [210/781]  eta: 0:08:59  lr: 0.000030  min_lr: 0.000030  loss: 0.8302 (0.8533)  class_acc: 0.8438 (0.8453)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [220/781]  eta: 0:08:50  lr: 0.000029  min_lr: 0.000029  loss: 0.8528 (0.8541)  class_acc: 0.8281 (0.8450)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0007  max mem: 8477\n",
            "Epoch: [8]  [230/781]  eta: 0:08:40  lr: 0.000029  min_lr: 0.000029  loss: 0.8698 (0.8550)  class_acc: 0.8281 (0.8442)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [240/781]  eta: 0:08:30  lr: 0.000029  min_lr: 0.000029  loss: 0.8622 (0.8553)  class_acc: 0.8438 (0.8441)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0014  max mem: 8477\n",
            "Epoch: [8]  [250/781]  eta: 0:08:21  lr: 0.000028  min_lr: 0.000028  loss: 0.8440 (0.8549)  class_acc: 0.8438 (0.8444)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0016  max mem: 8477\n",
            "Epoch: [8]  [260/781]  eta: 0:08:11  lr: 0.000028  min_lr: 0.000028  loss: 0.8410 (0.8548)  class_acc: 0.8438 (0.8447)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0016  max mem: 8477\n",
            "Epoch: [8]  [270/781]  eta: 0:08:01  lr: 0.000027  min_lr: 0.000027  loss: 0.8656 (0.8556)  class_acc: 0.8438 (0.8443)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0015  max mem: 8477\n",
            "Epoch: [8]  [280/781]  eta: 0:07:52  lr: 0.000027  min_lr: 0.000027  loss: 0.8526 (0.8559)  class_acc: 0.8281 (0.8441)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [290/781]  eta: 0:07:42  lr: 0.000027  min_lr: 0.000027  loss: 0.8430 (0.8547)  class_acc: 0.8438 (0.8447)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0016  max mem: 8477\n",
            "Epoch: [8]  [300/781]  eta: 0:07:33  lr: 0.000026  min_lr: 0.000026  loss: 0.8498 (0.8554)  class_acc: 0.8438 (0.8442)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0018  max mem: 8477\n",
            "Epoch: [8]  [310/781]  eta: 0:07:23  lr: 0.000026  min_lr: 0.000026  loss: 0.8397 (0.8548)  class_acc: 0.8438 (0.8446)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [320/781]  eta: 0:07:14  lr: 0.000025  min_lr: 0.000025  loss: 0.8134 (0.8540)  class_acc: 0.8594 (0.8449)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [330/781]  eta: 0:07:04  lr: 0.000025  min_lr: 0.000025  loss: 0.8035 (0.8534)  class_acc: 0.8594 (0.8455)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [340/781]  eta: 0:06:55  lr: 0.000025  min_lr: 0.000025  loss: 0.8299 (0.8535)  class_acc: 0.8594 (0.8454)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [350/781]  eta: 0:06:45  lr: 0.000024  min_lr: 0.000024  loss: 0.8392 (0.8530)  class_acc: 0.8438 (0.8455)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [360/781]  eta: 0:06:36  lr: 0.000024  min_lr: 0.000024  loss: 0.8149 (0.8524)  class_acc: 0.8438 (0.8452)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [370/781]  eta: 0:06:26  lr: 0.000023  min_lr: 0.000023  loss: 0.8497 (0.8523)  class_acc: 0.8281 (0.8450)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [380/781]  eta: 0:06:17  lr: 0.000023  min_lr: 0.000023  loss: 0.8396 (0.8524)  class_acc: 0.8438 (0.8446)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [390/781]  eta: 0:06:07  lr: 0.000023  min_lr: 0.000023  loss: 0.8275 (0.8510)  class_acc: 0.8594 (0.8453)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [400/781]  eta: 0:05:58  lr: 0.000022  min_lr: 0.000022  loss: 0.8275 (0.8516)  class_acc: 0.8438 (0.8450)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [410/781]  eta: 0:05:48  lr: 0.000022  min_lr: 0.000022  loss: 0.8488 (0.8521)  class_acc: 0.8281 (0.8445)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [420/781]  eta: 0:05:39  lr: 0.000022  min_lr: 0.000022  loss: 0.8614 (0.8526)  class_acc: 0.8125 (0.8442)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [430/781]  eta: 0:05:29  lr: 0.000021  min_lr: 0.000021  loss: 0.8480 (0.8515)  class_acc: 0.8281 (0.8446)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0006  max mem: 8477\n",
            "Epoch: [8]  [440/781]  eta: 0:05:20  lr: 0.000021  min_lr: 0.000021  loss: 0.8079 (0.8506)  class_acc: 0.8594 (0.8450)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0008  max mem: 8477\n",
            "Epoch: [8]  [450/781]  eta: 0:05:10  lr: 0.000021  min_lr: 0.000021  loss: 0.8083 (0.8501)  class_acc: 0.8750 (0.8454)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0012  max mem: 8477\n",
            "Epoch: [8]  [460/781]  eta: 0:05:01  lr: 0.000020  min_lr: 0.000020  loss: 0.8126 (0.8499)  class_acc: 0.8594 (0.8453)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [470/781]  eta: 0:04:52  lr: 0.000020  min_lr: 0.000020  loss: 0.8493 (0.8514)  class_acc: 0.8125 (0.8444)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [480/781]  eta: 0:04:42  lr: 0.000020  min_lr: 0.000020  loss: 0.8718 (0.8512)  class_acc: 0.8125 (0.8445)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0007  max mem: 8477\n",
            "Epoch: [8]  [490/781]  eta: 0:04:33  lr: 0.000019  min_lr: 0.000019  loss: 0.8311 (0.8511)  class_acc: 0.8438 (0.8443)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0014  max mem: 8477\n",
            "Epoch: [8]  [500/781]  eta: 0:04:23  lr: 0.000019  min_lr: 0.000019  loss: 0.8194 (0.8498)  class_acc: 0.8438 (0.8448)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0015  max mem: 8477\n",
            "Epoch: [8]  [510/781]  eta: 0:04:14  lr: 0.000019  min_lr: 0.000019  loss: 0.7718 (0.8493)  class_acc: 0.8594 (0.8448)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [520/781]  eta: 0:04:05  lr: 0.000018  min_lr: 0.000018  loss: 0.7999 (0.8494)  class_acc: 0.8438 (0.8446)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [530/781]  eta: 0:03:55  lr: 0.000018  min_lr: 0.000018  loss: 0.8474 (0.8497)  class_acc: 0.8438 (0.8445)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [540/781]  eta: 0:03:46  lr: 0.000018  min_lr: 0.000018  loss: 0.8399 (0.8493)  class_acc: 0.8594 (0.8447)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [550/781]  eta: 0:03:36  lr: 0.000017  min_lr: 0.000017  loss: 0.8188 (0.8493)  class_acc: 0.8594 (0.8446)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0015  max mem: 8477\n",
            "Epoch: [8]  [560/781]  eta: 0:03:27  lr: 0.000017  min_lr: 0.000017  loss: 0.8481 (0.8500)  class_acc: 0.8438 (0.8444)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0019  max mem: 8477\n",
            "Epoch: [8]  [570/781]  eta: 0:03:17  lr: 0.000017  min_lr: 0.000017  loss: 0.8465 (0.8498)  class_acc: 0.8281 (0.8445)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0016  max mem: 8477\n",
            "Epoch: [8]  [580/781]  eta: 0:03:08  lr: 0.000016  min_lr: 0.000016  loss: 0.8277 (0.8496)  class_acc: 0.8594 (0.8447)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0012  max mem: 8477\n",
            "Epoch: [8]  [590/781]  eta: 0:02:59  lr: 0.000016  min_lr: 0.000016  loss: 0.8489 (0.8494)  class_acc: 0.8594 (0.8447)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [600/781]  eta: 0:02:49  lr: 0.000016  min_lr: 0.000016  loss: 0.8471 (0.8493)  class_acc: 0.8438 (0.8448)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [610/781]  eta: 0:02:40  lr: 0.000015  min_lr: 0.000015  loss: 0.8471 (0.8489)  class_acc: 0.8438 (0.8451)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [620/781]  eta: 0:02:31  lr: 0.000015  min_lr: 0.000015  loss: 0.8178 (0.8489)  class_acc: 0.8594 (0.8453)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [630/781]  eta: 0:02:21  lr: 0.000015  min_lr: 0.000015  loss: 0.8218 (0.8486)  class_acc: 0.8594 (0.8453)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0016  max mem: 8477\n",
            "Epoch: [8]  [640/781]  eta: 0:02:12  lr: 0.000015  min_lr: 0.000015  loss: 0.8296 (0.8486)  class_acc: 0.8281 (0.8452)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0020  max mem: 8477\n",
            "Epoch: [8]  [650/781]  eta: 0:02:02  lr: 0.000014  min_lr: 0.000014  loss: 0.8544 (0.8487)  class_acc: 0.8281 (0.8450)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0014  max mem: 8477\n",
            "Epoch: [8]  [660/781]  eta: 0:01:53  lr: 0.000014  min_lr: 0.000014  loss: 0.8544 (0.8482)  class_acc: 0.8438 (0.8453)  weight_decay: 0.0500 (0.0500)  time: 0.9339  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [670/781]  eta: 0:01:44  lr: 0.000014  min_lr: 0.000014  loss: 0.8212 (0.8478)  class_acc: 0.8438 (0.8454)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0015  max mem: 8477\n",
            "Epoch: [8]  [680/781]  eta: 0:01:34  lr: 0.000013  min_lr: 0.000013  loss: 0.8321 (0.8481)  class_acc: 0.8438 (0.8454)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0014  max mem: 8477\n",
            "Epoch: [8]  [690/781]  eta: 0:01:25  lr: 0.000013  min_lr: 0.000013  loss: 0.8551 (0.8482)  class_acc: 0.8438 (0.8454)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [700/781]  eta: 0:01:15  lr: 0.000013  min_lr: 0.000013  loss: 0.8436 (0.8481)  class_acc: 0.8438 (0.8454)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [710/781]  eta: 0:01:06  lr: 0.000013  min_lr: 0.000013  loss: 0.8099 (0.8476)  class_acc: 0.8594 (0.8457)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0015  max mem: 8477\n",
            "Epoch: [8]  [720/781]  eta: 0:00:57  lr: 0.000012  min_lr: 0.000012  loss: 0.7898 (0.8471)  class_acc: 0.8750 (0.8460)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0015  max mem: 8477\n",
            "Epoch: [8]  [730/781]  eta: 0:00:47  lr: 0.000012  min_lr: 0.000012  loss: 0.8036 (0.8469)  class_acc: 0.8594 (0.8459)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0010  max mem: 8477\n",
            "Epoch: [8]  [740/781]  eta: 0:00:38  lr: 0.000012  min_lr: 0.000012  loss: 0.8382 (0.8472)  class_acc: 0.8438 (0.8458)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0011  max mem: 8477\n",
            "Epoch: [8]  [750/781]  eta: 0:00:29  lr: 0.000012  min_lr: 0.000012  loss: 0.8382 (0.8471)  class_acc: 0.8281 (0.8457)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0013  max mem: 8477\n",
            "Epoch: [8]  [760/781]  eta: 0:00:19  lr: 0.000011  min_lr: 0.000011  loss: 0.8349 (0.8471)  class_acc: 0.8281 (0.8457)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0014  max mem: 8477\n",
            "Epoch: [8]  [770/781]  eta: 0:00:10  lr: 0.000011  min_lr: 0.000011  loss: 0.8389 (0.8473)  class_acc: 0.8281 (0.8454)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0009  max mem: 8477\n",
            "Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000011  loss: 0.8520 (0.8473)  class_acc: 0.8594 (0.8457)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0006  max mem: 8477\n",
            "Epoch: [8] Total time: 0:12:12 (0.9375 s / it)\n",
            "Averaged stats: lr: 0.000011  min_lr: 0.000011  loss: 0.8520 (0.8473)  class_acc: 0.8594 (0.8457)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:05:58  loss: 0.2316 (0.2316)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 3.4158  data: 2.9058  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:08  loss: 0.1954 (0.1860)  acc1: 96.8750 (96.7803)  acc5: 100.0000 (99.9053)  time: 0.7232  data: 0.2664  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:50  loss: 0.1614 (0.1731)  acc1: 96.8750 (97.2222)  acc5: 100.0000 (99.9504)  time: 0.4544  data: 0.0020  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:41  loss: 0.1850 (0.1907)  acc1: 96.8750 (96.6734)  acc5: 100.0000 (99.8656)  time: 0.4568  data: 0.0022  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:34  loss: 0.3011 (0.2390)  acc1: 92.7083 (94.7409)  acc5: 100.0000 (99.8730)  time: 0.4598  data: 0.0019  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:28  loss: 0.3011 (0.2376)  acc1: 92.7083 (94.8325)  acc5: 100.0000 (99.8775)  time: 0.4628  data: 0.0015  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.2289 (0.2427)  acc1: 93.7500 (94.4672)  acc5: 100.0000 (99.8805)  time: 0.4638  data: 0.0013  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.2114 (0.2320)  acc1: 94.7917 (94.9237)  acc5: 100.0000 (99.8826)  time: 0.4634  data: 0.0008  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.1572 (0.2259)  acc1: 97.9167 (95.2546)  acc5: 100.0000 (99.8843)  time: 0.4639  data: 0.0011  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.1625 (0.2186)  acc1: 97.9167 (95.5586)  acc5: 100.0000 (99.8855)  time: 0.4634  data: 0.0009  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.1625 (0.2143)  acc1: 96.8750 (95.6786)  acc5: 100.0000 (99.8969)  time: 0.4622  data: 0.0004  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.1592 (0.2127)  acc1: 96.8750 (95.7100)  acc5: 100.0000 (99.9000)  time: 0.4430  data: 0.0002  max mem: 8477\n",
            "Test: Total time: 0:00:51 (0.4881 s / it)\n",
            "* Acc@1 95.710 Acc@5 99.900 loss 0.213\n",
            "Accuracy of the model on the 10000 test images: 95.7%\n",
            "Max accuracy: 95.71%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [9]  [  0/781]  eta: 0:38:04  lr: 0.000011  min_lr: 0.000011  loss: 0.9099 (0.9099)  class_acc: 0.8750 (0.8750)  weight_decay: 0.0500 (0.0500)  time: 2.9245  data: 1.8683  max mem: 8477\n",
            "Epoch: [9]  [ 10/781]  eta: 0:14:23  lr: 0.000011  min_lr: 0.000011  loss: 0.9099 (0.8916)  class_acc: 0.8750 (0.8438)  weight_decay: 0.0500 (0.0500)  time: 1.1198  data: 0.1708  max mem: 8477\n",
            "Epoch: [9]  [ 20/781]  eta: 0:13:04  lr: 0.000010  min_lr: 0.000010  loss: 0.8314 (0.8571)  class_acc: 0.8594 (0.8504)  weight_decay: 0.0500 (0.0500)  time: 0.9365  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [ 30/781]  eta: 0:12:30  lr: 0.000010  min_lr: 0.000010  loss: 0.8063 (0.8369)  class_acc: 0.8594 (0.8599)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [ 40/781]  eta: 0:12:09  lr: 0.000010  min_lr: 0.000010  loss: 0.7923 (0.8419)  class_acc: 0.8438 (0.8556)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0013  max mem: 8477\n",
            "Epoch: [9]  [ 50/781]  eta: 0:11:52  lr: 0.000010  min_lr: 0.000010  loss: 0.8115 (0.8375)  class_acc: 0.8438 (0.8560)  weight_decay: 0.0500 (0.0500)  time: 0.9372  data: 0.0010  max mem: 8477\n",
            "Epoch: [9]  [ 60/781]  eta: 0:11:38  lr: 0.000009  min_lr: 0.000009  loss: 0.8072 (0.8344)  class_acc: 0.8594 (0.8553)  weight_decay: 0.0500 (0.0500)  time: 0.9379  data: 0.0010  max mem: 8477\n",
            "Epoch: [9]  [ 70/781]  eta: 0:11:25  lr: 0.000009  min_lr: 0.000009  loss: 0.8276 (0.8311)  class_acc: 0.8594 (0.8574)  weight_decay: 0.0500 (0.0500)  time: 0.9372  data: 0.0014  max mem: 8477\n",
            "Epoch: [9]  [ 80/781]  eta: 0:11:13  lr: 0.000009  min_lr: 0.000009  loss: 0.8087 (0.8295)  class_acc: 0.8750 (0.8571)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [ 90/781]  eta: 0:11:01  lr: 0.000009  min_lr: 0.000009  loss: 0.8087 (0.8300)  class_acc: 0.8438 (0.8568)  weight_decay: 0.0500 (0.0500)  time: 0.9328  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [100/781]  eta: 0:10:50  lr: 0.000008  min_lr: 0.000008  loss: 0.8390 (0.8293)  class_acc: 0.8438 (0.8571)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [110/781]  eta: 0:10:39  lr: 0.000008  min_lr: 0.000008  loss: 0.8109 (0.8296)  class_acc: 0.8438 (0.8554)  weight_decay: 0.0500 (0.0500)  time: 0.9360  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [120/781]  eta: 0:10:29  lr: 0.000008  min_lr: 0.000008  loss: 0.8122 (0.8321)  class_acc: 0.8438 (0.8532)  weight_decay: 0.0500 (0.0500)  time: 0.9356  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [130/781]  eta: 0:10:18  lr: 0.000008  min_lr: 0.000008  loss: 0.8298 (0.8319)  class_acc: 0.8281 (0.8523)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [140/781]  eta: 0:10:08  lr: 0.000008  min_lr: 0.000008  loss: 0.8405 (0.8355)  class_acc: 0.8281 (0.8500)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [150/781]  eta: 0:09:58  lr: 0.000007  min_lr: 0.000007  loss: 0.8420 (0.8360)  class_acc: 0.8438 (0.8499)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [160/781]  eta: 0:09:48  lr: 0.000007  min_lr: 0.000007  loss: 0.8258 (0.8357)  class_acc: 0.8594 (0.8506)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0010  max mem: 8477\n",
            "Epoch: [9]  [170/781]  eta: 0:09:38  lr: 0.000007  min_lr: 0.000007  loss: 0.8535 (0.8389)  class_acc: 0.8438 (0.8490)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [180/781]  eta: 0:09:28  lr: 0.000007  min_lr: 0.000007  loss: 0.8602 (0.8394)  class_acc: 0.8438 (0.8487)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [190/781]  eta: 0:09:18  lr: 0.000007  min_lr: 0.000007  loss: 0.8317 (0.8418)  class_acc: 0.8438 (0.8474)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0013  max mem: 8477\n",
            "Epoch: [9]  [200/781]  eta: 0:09:09  lr: 0.000006  min_lr: 0.000006  loss: 0.8303 (0.8389)  class_acc: 0.8438 (0.8488)  weight_decay: 0.0500 (0.0500)  time: 0.9353  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [210/781]  eta: 0:08:59  lr: 0.000006  min_lr: 0.000006  loss: 0.7815 (0.8366)  class_acc: 0.8594 (0.8498)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [220/781]  eta: 0:08:49  lr: 0.000006  min_lr: 0.000006  loss: 0.8026 (0.8359)  class_acc: 0.8594 (0.8503)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [230/781]  eta: 0:08:39  lr: 0.000006  min_lr: 0.000006  loss: 0.8272 (0.8362)  class_acc: 0.8594 (0.8503)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [240/781]  eta: 0:08:30  lr: 0.000006  min_lr: 0.000006  loss: 0.8107 (0.8358)  class_acc: 0.8594 (0.8506)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0010  max mem: 8477\n",
            "Epoch: [9]  [250/781]  eta: 0:08:20  lr: 0.000006  min_lr: 0.000006  loss: 0.8514 (0.8369)  class_acc: 0.8438 (0.8502)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [260/781]  eta: 0:08:11  lr: 0.000005  min_lr: 0.000005  loss: 0.8579 (0.8379)  class_acc: 0.8281 (0.8502)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [270/781]  eta: 0:08:01  lr: 0.000005  min_lr: 0.000005  loss: 0.8311 (0.8381)  class_acc: 0.8438 (0.8501)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [280/781]  eta: 0:07:51  lr: 0.000005  min_lr: 0.000005  loss: 0.8321 (0.8404)  class_acc: 0.8281 (0.8489)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [290/781]  eta: 0:07:42  lr: 0.000005  min_lr: 0.000005  loss: 0.8263 (0.8397)  class_acc: 0.8438 (0.8492)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [300/781]  eta: 0:07:32  lr: 0.000005  min_lr: 0.000005  loss: 0.8187 (0.8397)  class_acc: 0.8594 (0.8494)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.0006  max mem: 8477\n",
            "Epoch: [9]  [310/781]  eta: 0:07:23  lr: 0.000005  min_lr: 0.000005  loss: 0.8351 (0.8400)  class_acc: 0.8594 (0.8490)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [320/781]  eta: 0:07:13  lr: 0.000004  min_lr: 0.000004  loss: 0.8396 (0.8403)  class_acc: 0.8438 (0.8488)  weight_decay: 0.0500 (0.0500)  time: 0.9334  data: 0.0013  max mem: 8477\n",
            "Epoch: [9]  [330/781]  eta: 0:07:04  lr: 0.000004  min_lr: 0.000004  loss: 0.8158 (0.8395)  class_acc: 0.8438 (0.8488)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [340/781]  eta: 0:06:54  lr: 0.000004  min_lr: 0.000004  loss: 0.8078 (0.8379)  class_acc: 0.8438 (0.8492)  weight_decay: 0.0500 (0.0500)  time: 0.9347  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [350/781]  eta: 0:06:45  lr: 0.000004  min_lr: 0.000004  loss: 0.8106 (0.8383)  class_acc: 0.8438 (0.8492)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0015  max mem: 8477\n",
            "Epoch: [9]  [360/781]  eta: 0:06:35  lr: 0.000004  min_lr: 0.000004  loss: 0.8194 (0.8385)  class_acc: 0.8438 (0.8492)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0015  max mem: 8477\n",
            "Epoch: [9]  [370/781]  eta: 0:06:26  lr: 0.000004  min_lr: 0.000004  loss: 0.8403 (0.8392)  class_acc: 0.8281 (0.8487)  weight_decay: 0.0500 (0.0500)  time: 0.9338  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [380/781]  eta: 0:06:16  lr: 0.000004  min_lr: 0.000004  loss: 0.8067 (0.8380)  class_acc: 0.8438 (0.8495)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [390/781]  eta: 0:06:07  lr: 0.000003  min_lr: 0.000003  loss: 0.8155 (0.8382)  class_acc: 0.8594 (0.8494)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0014  max mem: 8477\n",
            "Epoch: [9]  [400/781]  eta: 0:05:57  lr: 0.000003  min_lr: 0.000003  loss: 0.8207 (0.8371)  class_acc: 0.8594 (0.8499)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0016  max mem: 8477\n",
            "Epoch: [9]  [410/781]  eta: 0:05:48  lr: 0.000003  min_lr: 0.000003  loss: 0.8300 (0.8372)  class_acc: 0.8594 (0.8499)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0013  max mem: 8477\n",
            "Epoch: [9]  [420/781]  eta: 0:05:39  lr: 0.000003  min_lr: 0.000003  loss: 0.8158 (0.8367)  class_acc: 0.8438 (0.8502)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [430/781]  eta: 0:05:29  lr: 0.000003  min_lr: 0.000003  loss: 0.8107 (0.8364)  class_acc: 0.8438 (0.8499)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0010  max mem: 8477\n",
            "Epoch: [9]  [440/781]  eta: 0:05:20  lr: 0.000003  min_lr: 0.000003  loss: 0.8116 (0.8358)  class_acc: 0.8594 (0.8503)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0010  max mem: 8477\n",
            "Epoch: [9]  [450/781]  eta: 0:05:10  lr: 0.000003  min_lr: 0.000003  loss: 0.8037 (0.8350)  class_acc: 0.8750 (0.8509)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [460/781]  eta: 0:05:01  lr: 0.000003  min_lr: 0.000003  loss: 0.7911 (0.8341)  class_acc: 0.8594 (0.8513)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [470/781]  eta: 0:04:51  lr: 0.000003  min_lr: 0.000003  loss: 0.7806 (0.8335)  class_acc: 0.8750 (0.8517)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0014  max mem: 8477\n",
            "Epoch: [9]  [480/781]  eta: 0:04:42  lr: 0.000002  min_lr: 0.000002  loss: 0.8309 (0.8340)  class_acc: 0.8438 (0.8514)  weight_decay: 0.0500 (0.0500)  time: 0.9352  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [490/781]  eta: 0:04:33  lr: 0.000002  min_lr: 0.000002  loss: 0.8461 (0.8343)  class_acc: 0.8438 (0.8513)  weight_decay: 0.0500 (0.0500)  time: 0.9340  data: 0.0014  max mem: 8477\n",
            "Epoch: [9]  [500/781]  eta: 0:04:23  lr: 0.000002  min_lr: 0.000002  loss: 0.8194 (0.8336)  class_acc: 0.8594 (0.8515)  weight_decay: 0.0500 (0.0500)  time: 0.9335  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [510/781]  eta: 0:04:14  lr: 0.000002  min_lr: 0.000002  loss: 0.8033 (0.8338)  class_acc: 0.8594 (0.8513)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [520/781]  eta: 0:04:04  lr: 0.000002  min_lr: 0.000002  loss: 0.8346 (0.8340)  class_acc: 0.8438 (0.8511)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [530/781]  eta: 0:03:55  lr: 0.000002  min_lr: 0.000002  loss: 0.8419 (0.8344)  class_acc: 0.8438 (0.8510)  weight_decay: 0.0500 (0.0500)  time: 0.9327  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [540/781]  eta: 0:03:46  lr: 0.000002  min_lr: 0.000002  loss: 0.8331 (0.8346)  class_acc: 0.8594 (0.8510)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [550/781]  eta: 0:03:36  lr: 0.000002  min_lr: 0.000002  loss: 0.8137 (0.8350)  class_acc: 0.8438 (0.8508)  weight_decay: 0.0500 (0.0500)  time: 0.9349  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [560/781]  eta: 0:03:27  lr: 0.000002  min_lr: 0.000002  loss: 0.8340 (0.8347)  class_acc: 0.8438 (0.8509)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [570/781]  eta: 0:03:17  lr: 0.000002  min_lr: 0.000002  loss: 0.8340 (0.8346)  class_acc: 0.8438 (0.8509)  weight_decay: 0.0500 (0.0500)  time: 0.9337  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [580/781]  eta: 0:03:08  lr: 0.000002  min_lr: 0.000002  loss: 0.8259 (0.8338)  class_acc: 0.8594 (0.8514)  weight_decay: 0.0500 (0.0500)  time: 0.9331  data: 0.0006  max mem: 8477\n",
            "Epoch: [9]  [590/781]  eta: 0:02:59  lr: 0.000002  min_lr: 0.000002  loss: 0.7813 (0.8337)  class_acc: 0.8594 (0.8514)  weight_decay: 0.0500 (0.0500)  time: 0.9333  data: 0.0006  max mem: 8477\n",
            "Epoch: [9]  [600/781]  eta: 0:02:49  lr: 0.000002  min_lr: 0.000002  loss: 0.8305 (0.8340)  class_acc: 0.8438 (0.8512)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [610/781]  eta: 0:02:40  lr: 0.000001  min_lr: 0.000001  loss: 0.8305 (0.8339)  class_acc: 0.8438 (0.8513)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0016  max mem: 8477\n",
            "Epoch: [9]  [620/781]  eta: 0:02:30  lr: 0.000001  min_lr: 0.000001  loss: 0.8033 (0.8334)  class_acc: 0.8594 (0.8515)  weight_decay: 0.0500 (0.0500)  time: 0.9355  data: 0.0014  max mem: 8477\n",
            "Epoch: [9]  [630/781]  eta: 0:02:21  lr: 0.000001  min_lr: 0.000001  loss: 0.8350 (0.8340)  class_acc: 0.8438 (0.8514)  weight_decay: 0.0500 (0.0500)  time: 0.9348  data: 0.0007  max mem: 8477\n",
            "Epoch: [9]  [640/781]  eta: 0:02:12  lr: 0.000001  min_lr: 0.000001  loss: 0.8539 (0.8342)  class_acc: 0.8438 (0.8513)  weight_decay: 0.0500 (0.0500)  time: 0.9351  data: 0.0013  max mem: 8477\n",
            "Epoch: [9]  [650/781]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000001  loss: 0.8267 (0.8337)  class_acc: 0.8438 (0.8515)  weight_decay: 0.0500 (0.0500)  time: 0.9359  data: 0.0018  max mem: 8477\n",
            "Epoch: [9]  [660/781]  eta: 0:01:53  lr: 0.000001  min_lr: 0.000001  loss: 0.8226 (0.8340)  class_acc: 0.8438 (0.8514)  weight_decay: 0.0500 (0.0500)  time: 0.9354  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [670/781]  eta: 0:01:44  lr: 0.000001  min_lr: 0.000001  loss: 0.8112 (0.8337)  class_acc: 0.8594 (0.8516)  weight_decay: 0.0500 (0.0500)  time: 0.9336  data: 0.0007  max mem: 8477\n",
            "Epoch: [9]  [680/781]  eta: 0:01:34  lr: 0.000001  min_lr: 0.000001  loss: 0.8111 (0.8339)  class_acc: 0.8594 (0.8516)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [690/781]  eta: 0:01:25  lr: 0.000001  min_lr: 0.000001  loss: 0.8223 (0.8338)  class_acc: 0.8594 (0.8515)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0007  max mem: 8477\n",
            "Epoch: [9]  [700/781]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000001  loss: 0.8135 (0.8334)  class_acc: 0.8594 (0.8517)  weight_decay: 0.0500 (0.0500)  time: 0.9350  data: 0.0010  max mem: 8477\n",
            "Epoch: [9]  [710/781]  eta: 0:01:06  lr: 0.000001  min_lr: 0.000001  loss: 0.8135 (0.8335)  class_acc: 0.8594 (0.8517)  weight_decay: 0.0500 (0.0500)  time: 0.9341  data: 0.0010  max mem: 8477\n",
            "Epoch: [9]  [720/781]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000001  loss: 0.8191 (0.8333)  class_acc: 0.8438 (0.8518)  weight_decay: 0.0500 (0.0500)  time: 0.9332  data: 0.0008  max mem: 8477\n",
            "Epoch: [9]  [730/781]  eta: 0:00:47  lr: 0.000001  min_lr: 0.000001  loss: 0.8191 (0.8338)  class_acc: 0.8281 (0.8516)  weight_decay: 0.0500 (0.0500)  time: 0.9342  data: 0.0011  max mem: 8477\n",
            "Epoch: [9]  [740/781]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000001  loss: 0.8686 (0.8341)  class_acc: 0.8281 (0.8513)  weight_decay: 0.0500 (0.0500)  time: 0.9344  data: 0.0013  max mem: 8477\n",
            "Epoch: [9]  [750/781]  eta: 0:00:29  lr: 0.000001  min_lr: 0.000001  loss: 0.8396 (0.8338)  class_acc: 0.8438 (0.8514)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0012  max mem: 8477\n",
            "Epoch: [9]  [760/781]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000001  loss: 0.8247 (0.8335)  class_acc: 0.8594 (0.8515)  weight_decay: 0.0500 (0.0500)  time: 0.9346  data: 0.0013  max mem: 8477\n",
            "Epoch: [9]  [770/781]  eta: 0:00:10  lr: 0.000001  min_lr: 0.000001  loss: 0.8245 (0.8335)  class_acc: 0.8594 (0.8517)  weight_decay: 0.0500 (0.0500)  time: 0.9345  data: 0.0009  max mem: 8477\n",
            "Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.8338 (0.8334)  class_acc: 0.8594 (0.8517)  weight_decay: 0.0500 (0.0500)  time: 0.9343  data: 0.0008  max mem: 8477\n",
            "Epoch: [9] Total time: 0:12:12 (0.9375 s / it)\n",
            "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.8338 (0.8334)  class_acc: 0.8594 (0.8517)  weight_decay: 0.0500 (0.0500)\n",
            "Test:  [  0/105]  eta: 0:04:20  loss: 0.2270 (0.2270)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 2.4826  data: 1.9703  max mem: 8477\n",
            "Test:  [ 10/105]  eta: 0:01:00  loss: 0.1882 (0.1802)  acc1: 96.8750 (97.1591)  acc5: 100.0000 (99.9053)  time: 0.6406  data: 0.1809  max mem: 8477\n",
            "Test:  [ 20/105]  eta: 0:00:47  loss: 0.1668 (0.1721)  acc1: 96.8750 (97.3710)  acc5: 100.0000 (99.9504)  time: 0.4571  data: 0.0018  max mem: 8477\n",
            "Test:  [ 30/105]  eta: 0:00:39  loss: 0.1872 (0.1868)  acc1: 96.8750 (96.9086)  acc5: 100.0000 (99.8992)  time: 0.4582  data: 0.0013  max mem: 8477\n",
            "Test:  [ 40/105]  eta: 0:00:33  loss: 0.2981 (0.2336)  acc1: 92.7083 (94.9949)  acc5: 100.0000 (99.8984)  time: 0.4607  data: 0.0009  max mem: 8477\n",
            "Test:  [ 50/105]  eta: 0:00:27  loss: 0.2983 (0.2322)  acc1: 92.7083 (95.0980)  acc5: 100.0000 (99.9183)  time: 0.4618  data: 0.0016  max mem: 8477\n",
            "Test:  [ 60/105]  eta: 0:00:22  loss: 0.2312 (0.2400)  acc1: 93.7500 (94.6209)  acc5: 100.0000 (99.9146)  time: 0.4612  data: 0.0018  max mem: 8477\n",
            "Test:  [ 70/105]  eta: 0:00:17  loss: 0.2160 (0.2301)  acc1: 94.7917 (95.0411)  acc5: 100.0000 (99.8973)  time: 0.4625  data: 0.0020  max mem: 8477\n",
            "Test:  [ 80/105]  eta: 0:00:12  loss: 0.1591 (0.2237)  acc1: 98.9583 (95.3447)  acc5: 100.0000 (99.8971)  time: 0.4624  data: 0.0016  max mem: 8477\n",
            "Test:  [ 90/105]  eta: 0:00:07  loss: 0.1655 (0.2168)  acc1: 97.9167 (95.6044)  acc5: 100.0000 (99.8970)  time: 0.4613  data: 0.0007  max mem: 8477\n",
            "Test:  [100/105]  eta: 0:00:02  loss: 0.1655 (0.2125)  acc1: 96.8750 (95.7405)  acc5: 100.0000 (99.9072)  time: 0.4606  data: 0.0006  max mem: 8477\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.1614 (0.2107)  acc1: 96.8750 (95.7700)  acc5: 100.0000 (99.9100)  time: 0.4416  data: 0.0005  max mem: 8477\n",
            "Test: Total time: 0:00:50 (0.4785 s / it)\n",
            "* Acc@1 95.770 Acc@5 99.910 loss 0.211\n",
            "Accuracy of the model on the 10000 test images: 95.8%\n",
            "Max accuracy: 95.77%\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_ckpt)... Done. 7.7s\n",
            "Training time 2:10:58\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               Global Test/test_acc1 ▁▃▄▅▅▆▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               Global Test/test_acc5 ▁▄▃▅▅▅▆▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               Global Test/test_loss █▇▅▄▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        Global Train/train_class_acc ▁▃▄▅▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             Global Train/train_loss █▆▅▄▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               Global Train/train_lr ██▇▆▅▄▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           Global Train/train_min_lr ██▇▆▅▄▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     Global Train/train_weight_decay ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Rank-0 Batch Wise/global_train_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   Rank-0 Batch Wise/train_class_acc ▁▂▃▅▂▅▃▃▃▅▅▄▅▅▆▆▅▅▅▆▅▇▅▆▆▆▇▆▅▆▅▅▇▆▇▆▅█▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        Rank-0 Batch Wise/train_loss █▆▆▄▆▃▅▅▅▅▄▄▃▃▃▃▃▃▄▃▃▂▄▃▃▃▂▂▃▃▄▃▂▃▂▂▄▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Rank-0 Batch Wise/train_max_lr ███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Rank-0 Batch Wise/train_min_lr ███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               Global Test/test_acc1 95.77\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               Global Test/test_acc5 99.91\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               Global Test/test_loss 0.21067\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        Global Train/train_class_acc 0.85173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             Global Train/train_loss 0.83335\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               Global Train/train_lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           Global Train/train_min_lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     Global Train/train_weight_decay 0.05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Rank-0 Batch Wise/global_train_step 7809\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   Rank-0 Batch Wise/train_class_acc 0.82812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        Rank-0 Batch Wise/train_loss 0.82692\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Rank-0 Batch Wise/train_max_lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Rank-0 Batch Wise/train_min_lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        n_parameters 27827818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtreasured-plant-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/raghvender/convnext/runs/xkrxh4el\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220608_104657-xkrxh4el/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WsF5WNY84YC_"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}