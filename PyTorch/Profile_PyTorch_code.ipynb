{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile PyTorch Code\n",
    "How to use `Tensorboard` or `PyTorch Kineto` plugin for profiling PyTorch code.\n",
    "\n",
    "<img src=\"https://i.imgur.com/fwSc5Z9.png\"/>\n",
    "\n",
    "The work done by `processes`, `threads` and `streams` on the CPU and GPU is displayed along with precise timing information in an interactive viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\ML\\WandbAI\\Examples\\PyTorch\\Profile_PyTorch_code.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML/WandbAI/Examples/PyTorch/Profile_PyTorch_code.ipynb#ch0000001?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ML/WandbAI/Examples/PyTorch/Profile_PyTorch_code.ipynb#ch0000001?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML/WandbAI/Examples/PyTorch/Profile_PyTorch_code.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML/WandbAI/Examples/PyTorch/Profile_PyTorch_code.ipynb#ch0000001?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\pytorch_lightning\\__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/__init__.py?line=26'>27</a>\u001b[0m     _logger\u001b[39m.\u001b[39maddHandler(logging\u001b[39m.\u001b[39mStreamHandler())\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/__init__.py?line=27'>28</a>\u001b[0m     _logger\u001b[39m.\u001b[39mpropagate \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/__init__.py?line=29'>30</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/__init__.py?line=30'>31</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/__init__.py?line=31'>32</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer  \u001b[39m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\pytorch_lightning\\callbacks\\__init__.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/__init__.py?line=0'>1</a>\u001b[0m \u001b[39m# Copyright The PyTorch Lightning team.\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/__init__.py?line=1'>2</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/__init__.py?line=2'>3</a>\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/__init__.py?line=11'>12</a>\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/__init__.py?line=12'>13</a>\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/__init__.py?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/__init__.py?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdevice_stats_monitor\u001b[39;00m \u001b[39mimport\u001b[39;00m DeviceStatsMonitor\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/__init__.py?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mearly_stopping\u001b[39;00m \u001b[39mimport\u001b[39;00m EarlyStopping\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\pytorch_lightning\\callbacks\\base.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/base.py?line=13'>14</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/base.py?line=14'>15</a>\u001b[0m \u001b[39mBase class used to build new callbacks.\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/base.py?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/base.py?line=16'>17</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/base.py?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, List, Optional, Type\n\u001b[1;32m---> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/base.py?line=20'>21</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/base.py?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m Optimizer\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/pytorch_lightning/callbacks/base.py?line=23'>24</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\__init__.py:751\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=743'>744</a>\u001b[0m     __all__\u001b[39m.\u001b[39mappend(name)\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=745'>746</a>\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=746'>747</a>\u001b[0m \u001b[39m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=747'>748</a>\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=748'>749</a>\u001b[0m \n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=749'>750</a>\u001b[0m \u001b[39m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=750'>751</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=753'>754</a>\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=754'>755</a>\u001b[0m \u001b[39m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=755'>756</a>\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/__init__.py?line=757'>758</a>\u001b[0m \u001b[39mdel\u001b[39;00m ByteStorageBase\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\functional.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/functional.py?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/functional.py?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/functional.py?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/functional.py?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lowrank\u001b[39;00m \u001b[39mimport\u001b[39;00m svd_lowrank, pca_lowrank\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/functional.py?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/functional.py?line=10'>11</a>\u001b[0m     has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/functional.py?line=11'>12</a>\u001b[0m     handle_torch_function)\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\nn\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparameter\u001b[39;00m \u001b[39mimport\u001b[39;00m Parameter, UninitializedParameter, UninitializedBuffer\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m DataParallel\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[1;32m----> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlinear\u001b[39;00m \u001b[39mimport\u001b[39;00m Identity, Linear, Bilinear, LazyLinear\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconv\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv1d, Conv2d, Conv3d, \\\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=3'>4</a>\u001b[0m     ConvTranspose1d, ConvTranspose2d, ConvTranspose3d, \\\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=4'>5</a>\u001b[0m     LazyConv1d, LazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d, LazyConvTranspose3d\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mactivation\u001b[39;00m \u001b[39mimport\u001b[39;00m Threshold, ReLU, Hardtanh, ReLU6, Sigmoid, Tanh, \\\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=6'>7</a>\u001b[0m     Softmax, Softmax2d, LogSoftmax, ELU, SELU, CELU, GELU, Hardshrink, LeakyReLU, LogSigmoid, \\\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=7'>8</a>\u001b[0m     Softplus, Softshrink, MultiheadAttention, PReLU, Softsign, Softmin, Tanhshrink, RReLU, GLU, \\\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/__init__.py?line=8'>9</a>\u001b[0m     Hardsigmoid, Hardswish, SiLU, Mish\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\linear.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/linear.py?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/linear.py?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparameter\u001b[39;00m \u001b[39mimport\u001b[39;00m Parameter, UninitializedParameter\n\u001b[1;32m----> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/linear.py?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/linear.py?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m init\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/modules/linear.py?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\nn\\functional.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/functional.py?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/functional.py?line=14'>15</a>\u001b[0m     \u001b[39m# The JIT doesn't understand Union, nor torch.dtype here\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/functional.py?line=15'>16</a>\u001b[0m     DType \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m\n\u001b[1;32m---> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/functional.py?line=17'>18</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_jit_internal\u001b[39;00m \u001b[39mimport\u001b[39;00m boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/functional.py?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/functional.py?line=19'>20</a>\u001b[0m     has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/functional.py?line=20'>21</a>\u001b[0m     handle_torch_function)\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/nn/functional.py?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _reduction \u001b[39mas\u001b[39;00m _Reduction\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\_jit_internal.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/_jit_internal.py?line=25'>26</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_sources\u001b[39;00m \u001b[39mimport\u001b[39;00m get_source_lines_and_file, parse_def, fake_range\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/_jit_internal.py?line=26'>27</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfutures\u001b[39;00m \u001b[39mimport\u001b[39;00m Future\n\u001b[1;32m---> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/_jit_internal.py?line=27'>28</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpackage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_mangling\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpackage_mangling\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/_jit_internal.py?line=28'>29</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Dict, Generic, List, Optional, Tuple, Type, TypeVar, Union  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/_jit_internal.py?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info[:\u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m7\u001b[39m):\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\package\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mglob_group\u001b[39;00m \u001b[39mimport\u001b[39;00m GlobGroup\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/__init__.py?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimporter\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/__init__.py?line=4'>5</a>\u001b[0m     Importer,\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/__init__.py?line=5'>6</a>\u001b[0m     ObjMismatchError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/__init__.py?line=8'>9</a>\u001b[0m     sys_importer,\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/__init__.py?line=9'>10</a>\u001b[0m )\n\u001b[1;32m---> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/__init__.py?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackage_exporter\u001b[39;00m \u001b[39mimport\u001b[39;00m EmptyMatchError, PackageExporter, PackagingError\n\u001b[0;32m     <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/__init__.py?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackage_importer\u001b[39;00m \u001b[39mimport\u001b[39;00m PackageImporter\n",
      "File \u001b[1;32md:\\Development\\Python\\Python3.10\\lib\\site-packages\\torch\\package\\package_exporter.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/package_exporter.py?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mio\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/package_exporter.py?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlinecache\u001b[39;00m\n\u001b[1;32m----> <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/package_exporter.py?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickletools\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/package_exporter.py?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m      <a href='file:///d%3A/Development/Python/Python3.10/lib/site-packages/torch/package/package_exporter.py?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m OrderedDict, defaultdict\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.profiler import tensorboard_trace_handler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: raghvender. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "                                        if not mirror.startswith('http://yann.lecun.com')]\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Profiling Training\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZERS = {\n",
    "    \"Adadelta\": optim.Adadelta,\n",
    "    \"Adagrad\" : optim.Adagrad,\n",
    "    \"SGD\": optim.SGD,\n",
    "}\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "    \"\"\"Very simple LeNet-style DNN, plus DropOut.\"\"\"\n",
    "    def __init__(self, optimizer=\"Adadelta\"):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        self.optimizer = self.set_optimizer(optimizer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        return OPTIMIZERS[optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, idx):\n",
    "  inputs, labels = batch\n",
    "  outputs = self(inputs)\n",
    "  loss =  F.nll_loss(outputs, labels)\n",
    "\n",
    "  return {\"loss\": loss}\n",
    "      \n",
    "def configure_optimizers(self):\n",
    "  return self.optimizer(self.parameters(), lr=0.1)\n",
    "\n",
    "Net.training_step = training_step\n",
    "Net.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiler Callback\n",
    "The profiler operates like a PyTorch optimizer. It has `.step` method that we need to call to demarcate the code for profiling.\n",
    "\n",
    "A single training step (`forward` and `backward prop`) is both the typical target of performance optimizations and already rich enough to more than fill out a profiling `trace`, so we want to call `.step` on each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTensorboardProfilerCallback(pl.Callback):\n",
    "    \"\"\"Quick-and-dirty Callback for invoking TensorboardProfiler during training.\n",
    "    \n",
    "    For greater robustness, extend the pl.profiler.profilers.BaseProfiler. See\n",
    "    https://pytorch-lightning.readthedocs.io/en/stable/advanced/profiler.html\"\"\"\n",
    "\n",
    "    def __init__(self, profiler):\n",
    "        super().__init__()\n",
    "        self.profiler = profiler \n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, *args, **kwargs):\n",
    "        self.profiler.step()\n",
    "        pl_module.log_dict(outputs)  # also logging the loss, while we're here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Profiled Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ML\\WandbAI\\Examples\\PyTorch\\wandb\\run-20220607_041916-1e7f6gfr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/raghvender/trace/runs/1e7f6gfr\" target=\"_blank\">wandering-shadow-1</a></strong> to <a href=\"https://wandb.ai/raghvender/trace\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2702228e64439da01bf55c2be23df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8515f53ef62432fa734a24e9912f3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3598e0a099d4e4a85298bac4e96fa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9340a2f724414bff8cada0b80a990ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\Python\\Python3.10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type    | Params\n",
      "-------------------------------------\n",
      "0 | conv1    | Conv2d  | 320   \n",
      "1 | conv2    | Conv2d  | 18.5 K\n",
      "2 | dropout1 | Dropout | 0     \n",
      "3 | dropout2 | Dropout | 0     \n",
      "4 | fc1      | Linear  | 1.2 M \n",
      "5 | fc2      | Linear  | 1.3 K \n",
      "-------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.800     Total estimated model params size (MB)\n",
      "d:\\Development\\Python\\Python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd081b2d30374962880b8c6e256fb4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# initial values are defaults, for all except batch_size, which has no default\n",
    "config = {\"batch_size\": 32,  # try log-spaced values from 1 to 50,000\n",
    "          \"num_workers\": 0,  # try 0, 1, and 2\n",
    "          \"pin_memory\": False,  # try False and True\n",
    "          \"precision\": 32,  # try 16 and 32\n",
    "          \"optimizer\": \"Adadelta\",  # try optim.Adadelta and optim.SGD\n",
    "          }\n",
    "\n",
    "with wandb.init(project='trace', config=config) as run:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset = datasets.MNIST(\"../data\", train=True, download=True,\n",
    "                            transform=transform)\n",
    "    ## Using a raw DataLoader, rather than LightningDataModule, for greater transparency\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "      dataset,\n",
    "      # Key performance-relevant configuration parameters:\n",
    "      ## batch_size: how many datapoints are passed through the network at once?\n",
    "      batch_size=wandb.config.batch_size,\n",
    "      # larger batch sizes are more compute efficient, up to memory constraints\n",
    "\n",
    "      ##  num_workers: how many side processes to launch for dataloading (should be >0)\n",
    "      num_workers=wandb.config.num_workers,\n",
    "      # needs to be tuned given model/batch size/compute\n",
    "\n",
    "      ## pin_memory: should a fixed \"pinned\" memory block be allocated on the CPU?\n",
    "      pin_memory=wandb.config.pin_memory,\n",
    "      # should nearly always be True for GPU models, see https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n",
    "    )\n",
    "\n",
    "    # Set up model\n",
    "    model = Net(optimizer=wandb.config[\"optimizer\"])\n",
    "\n",
    "    # Set up profiler\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(\n",
    "      wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "      schedule=schedule, on_trace_ready=tensorboard_trace_handler(\"wandb/latest-run/tbprofile\"), with_stack=True)\n",
    "\n",
    "    with profiler:\n",
    "        profiler_callback = TorchTensorboardProfilerCallback(profiler)\n",
    "\n",
    "        trainer = pl.Trainer(gpus=1, max_epochs=1, max_steps=total_steps,\n",
    "                            logger=pl.loggers.WandbLogger(log_model=True, save_code=True),\n",
    "                            callbacks=[profiler_callback], precision=wandb.config.precision)\n",
    "\n",
    "        trainer.fit(model, trainloader)\n",
    "\n",
    "    profile_art = wandb.Artifact(f\"trace-{wandb.run.id}\", type=\"profile\")\n",
    "    profile_art.add_file(glob.glob(\"wandb/latest-run/tbprofile/*.pt.trace.json\")[0], \"trace.pt.trace.json\")\n",
    "    run.log_artifact(profile_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "377e50a5246b311701775dacc641cf519e90716189c79b110a6f1ebdee9e2206"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
